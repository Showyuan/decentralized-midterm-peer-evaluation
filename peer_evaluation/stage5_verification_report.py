#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vancouverç®—æ³•çµæœé©—è­‰å™¨
é©—è­‰Q1~Q5åˆ†æ•¸åŠ ç¸½è¨ˆç®—çš„æ­£ç¢ºæ€§
"""

import json
import pandas as pd
import numpy as np
import os

def verify_score_calculation():
    """é©—è­‰åˆ†æ•¸è¨ˆç®—çš„æ­£ç¢ºæ€§"""
    # è¼‰å…¥çµ±ä¸€é…ç½®
    from stage0_config_unified import PeerEvaluationConfig
    config = PeerEvaluationConfig()
    
    print("ğŸ” é©—è­‰Vancouverç®—æ³•åˆ†æ•¸è¨ˆç®—...")
    print("=" * 60)
    
    # å¾é…ç½®ç²å–è·¯å¾‘
    try:
        from stage0_config_unified import PeerEvaluationConfig
    except ImportError:
        from .stage0_config_unified import PeerEvaluationConfig
    
    config = PeerEvaluationConfig()
    
    # è¼‰å…¥åŸå§‹è©•åˆ†æ•¸æ“š
    evaluation_results_path = config.get_path('stage5_results', 'evaluation_results_json')
    with open(evaluation_results_path, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    # è¼‰å…¥Vancouverç®—æ³•çµæœï¼ˆå‹•æ…‹å°‹æ‰¾æœ€æ–°çµæœæ–‡ä»¶ï¼‰
    vancouver_results_dir = config.ensure_output_dir('stage6_vancouver')
    vancouver_files = [f for f in os.listdir(vancouver_results_dir) if f.startswith("vancouver_results_") and f.endswith(".json")]
    if not vancouver_files:
        print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°Vancouverçµæœæ–‡ä»¶")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„çµæœæ–‡ä»¶
    latest_file = sorted(vancouver_files)[-1]
    vancouver_results_path = os.path.join(vancouver_results_dir, latest_file)
    print(f"ğŸ“‚ ä½¿ç”¨Vancouverçµæœæ–‡ä»¶: {latest_file}")
    
    with open(vancouver_results_path, 'r', encoding='utf-8') as f:
        vancouver_results = json.load(f)
    
    print("ğŸ“Š åŸå§‹æ•¸æ“šçµ±è¨ˆ:")
    print(f"   - è©•åˆ†è€…æ•¸é‡: {original_data['metadata']['total_students']}")
    print(f"   - è©•åˆ†ç¸½æ•¸: {original_data['metadata']['total_evaluations']}")
    if 'overall' in original_data.get('summary', {}):
        print(f"   - åŸå§‹å¹³å‡åˆ†: {original_data['summary']['overall']['mean']:.2f} (å–®é¡Œå¹³å‡)")
    
    # é‡æ–°è¨ˆç®—åŠ ç¸½åˆ†æ•¸ä¾†é©—è­‰
    print("\nğŸ§® é‡æ–°è¨ˆç®—Q1~Q5åŠ ç¸½åˆ†æ•¸...")
    
    total_scores_by_evaluatee = {}
    evaluation_counts = {}
    question_scores_by_student = {}  # {student_id: {Q1: [scores], Q2: [scores], ...}}
    
    # å¾resultsçµæ§‹ä¸­æå–æ•¸æ“š (æ ¼å¼: results[target_id]['all_evaluations'])
    for target_id, target_data in original_data['results'].items():
        if target_id not in total_scores_by_evaluatee:
            total_scores_by_evaluatee[target_id] = []
            evaluation_counts[target_id] = 0
            question_scores_by_student[target_id] = {f'Q{i}': [] for i in range(1, 6)}
        
        # æŒ‰è©•åˆ†è€…èšåˆåˆ†æ•¸
        evaluator_totals = {}
        
        for eval_item in target_data['all_evaluations']:
            evaluator_id = eval_item['evaluator_id']
            question_id = eval_item['question_id']
            score = eval_item['score']
            
            # åªè™•ç†æ¨™æº–å•é¡ŒID (Q1-Q5)ï¼Œè·³étoken IDæ ¼å¼çš„èˆŠæ•¸æ“š
            if question_id.startswith('Q') and question_id in question_scores_by_student[target_id]:
                # è¨˜éŒ„å–®é¡Œåˆ†æ•¸ç”¨æ–¼å¾ŒçºŒåˆ†æ
                question_scores_by_student[target_id][question_id].append(score)
            
            # ç´¯åŠ è©²è©•åˆ†è€…å°è©²å­¸ç”Ÿçš„ç¸½åˆ†ï¼ˆåŒ…å«æ‰€æœ‰é¡å‹çš„è©•åˆ†ï¼‰
            if evaluator_id not in evaluator_totals:
                evaluator_totals[evaluator_id] = 0
            evaluator_totals[evaluator_id] += score
        
        # è¨˜éŒ„æ¯å€‹è©•åˆ†è€…çµ¦é€™å€‹å­¸ç”Ÿçš„ç¸½åˆ†
        for evaluator_id, total_score in evaluator_totals.items():
            total_scores_by_evaluatee[target_id].append(total_score)
            evaluation_counts[target_id] += 1
    
    # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
    print("âœ… åŠ ç¸½åˆ†æ•¸çµ±è¨ˆ:")
    all_total_scores = []
    for scores in total_scores_by_evaluatee.values():
        all_total_scores.extend(scores)
    
    print(f"   - åˆ†æ•¸ç¯„åœ: {min(all_total_scores)} ~ {max(all_total_scores)}")
    print(f"   - å¹³å‡ç¸½åˆ†: {np.mean(all_total_scores):.2f}")
    print(f"   - æ¨™æº–å·®: {np.std(all_total_scores):.2f}")
    
    # æ¯”è¼ƒVancouverç®—æ³•çµæœ
    print("\nğŸ¯ Vancouverç®—æ³•æœ€çµ‚çµæœ:")
    print(f"   - å¹³å‡æœ€çµ‚æˆç¸¾: {vancouver_results['summary_statistics']['avg_final_grade']:.2f}")
    print(f"   - å¹³å‡å…±è­˜åˆ†æ•¸: {vancouver_results['summary_statistics']['avg_consensus_score']:.2f}")
    print(f"   - å¹³å‡è²è­½åˆ†æ•¸: {vancouver_results['summary_statistics']['avg_reputation']:.3f}")
    print(f"   - ä½¿ç”¨ä¿è­·æ©Ÿåˆ¶: {vancouver_results['summary_statistics']['protection_used_count']} äºº")
    
    # å‰µå»ºè©³ç´°æ¯”è¼ƒå ±å‘Š
    print("\nğŸ“‹ è©³ç´°å­¸ç”Ÿæˆç¸¾æ¯”è¼ƒ:")
    print("=" * 120)
    print(f"{'å­¸ç”ŸID':<6} {'åŸå§‹Q1~Q5å‡åˆ†':<15} {'åŠ ç¸½åˆ†æ•¸å‡å€¼':<15} {'Vancouverå…±è­˜':<15} {'æœ€çµ‚æˆç¸¾':<12} {'è²è­½åˆ†æ•¸':<10} {'ä¿è­·æ©Ÿåˆ¶':<8}")
    print("-" * 120)
    
    comparison_data = []
    
    # ä½¿ç”¨å·²ç¶“æ”¶é›†å¥½çš„question_scores_by_student
    for student_id in sorted(total_scores_by_evaluatee.keys()):
        # è¨ˆç®—åŸå§‹å–®é¡Œå¹³å‡åˆ†
        all_question_scores = []
        for q_num in range(1, 6):
            q_key = f'Q{q_num}'
            if q_key in question_scores_by_student[student_id]:
                all_question_scores.extend(question_scores_by_student[student_id][q_key])
        
        original_avg = np.mean(all_question_scores) if all_question_scores else 0
        
        # åŠ ç¸½åˆ†æ•¸çš„å¹³å‡
        total_scores_avg = np.mean(total_scores_by_evaluatee[student_id])
        
        # Vancouverçµæœ
        vancouver_grade = vancouver_results['final_grades'][student_id]
        consensus_score = vancouver_grade['consensus_score']
        final_grade = vancouver_grade['final_grade']
        reputation = vancouver_grade['reputation']
        protection_used = "æ˜¯" if vancouver_grade['protection_used'] else "å¦"
        
        print(f"{student_id:<6} {original_avg:<15.2f} {total_scores_avg:<15.2f} {consensus_score:<15.2f} "
              f"{final_grade:<12.2f} {reputation:<10.3f} {protection_used:<8}")
        
        comparison_data.append({
            'student_id': student_id,
            'original_avg_per_question': original_avg,
            'total_score_avg': total_scores_avg,
            'expected_total': original_avg * 5,  # é æœŸçš„ç¸½åˆ†ï¼ˆå–®é¡Œå‡åˆ† Ã— 5ï¼‰
            'vancouver_consensus': consensus_score,
            'vancouver_final': final_grade,
            'reputation': reputation,
            'protection_used': vancouver_grade['protection_used']
        })
    
    # é©—è­‰è¨ˆç®—æ­£ç¢ºæ€§
    print("\nğŸ”¬ è¨ˆç®—æ­£ç¢ºæ€§é©—è­‰:")
    all_correct = True
    
    for data in comparison_data:
        expected = data['expected_total']
        actual = data['total_score_avg']
        diff = abs(expected - actual)
        
        if diff > 0.01:  # å…è¨±å¾®å°çš„æµ®é»èª¤å·®
            print(f"âš ï¸  {data['student_id']}: é æœŸç¸½åˆ† {expected:.2f}, å¯¦éš›ç¸½åˆ† {actual:.2f}, å·®ç•° {diff:.2f}")
            all_correct = False
    
    if all_correct:
        print("âœ… æ‰€æœ‰å­¸ç”Ÿçš„åˆ†æ•¸è¨ˆç®—éƒ½æ­£ç¢ºï¼")
    
    # åˆ†æVancouverç®—æ³•æ•ˆæœ
    print("\nğŸ“ˆ Vancouverç®—æ³•æ•ˆæœåˆ†æ:")
    
    # è¨ˆç®—åŸå§‹åˆ†æ•¸çš„æ¨™æº–å·®vs Vancouverå…±è­˜åˆ†æ•¸çš„æ¨™æº–å·®
    original_total_avgs = [d['total_score_avg'] for d in comparison_data]
    vancouver_consensus = [d['vancouver_consensus'] for d in comparison_data]
    vancouver_final = [d['vancouver_final'] for d in comparison_data]
    
    print(f"   - åŸå§‹ç¸½åˆ†æ¨™æº–å·®: {np.std(original_total_avgs):.2f}")
    print(f"   - Vancouverå…±è­˜åˆ†æ•¸æ¨™æº–å·®: {np.std(vancouver_consensus):.2f}")
    print(f"   - Vancouveræœ€çµ‚æˆç¸¾æ¨™æº–å·®: {np.std(vancouver_final):.2f}")
    
    # æ’åè®ŠåŒ–åˆ†æ
    original_ranking = sorted(comparison_data, key=lambda x: x['total_score_avg'], reverse=True)
    vancouver_ranking = sorted(comparison_data, key=lambda x: x['vancouver_final'], reverse=True)
    
    print(f"\nğŸ† æ’åè®ŠåŒ–åˆ†æ:")
    print(f"   å‰5å (åŸå§‹ç¸½åˆ†): {', '.join([d['student_id'] for d in original_ranking[:5]])}")
    print(f"   å‰5å (Vancouver): {', '.join([d['student_id'] for d in vancouver_ranking[:5]])}")
    
    # ä½¿ç”¨é…ç½®ä¿å­˜æ¯”è¼ƒå ±å‘Š
    verification_report_path = config.get_path('stage7_reports', 'verification_report')
    config.ensure_output_dir('stage7_reports')  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    df = pd.DataFrame(comparison_data)
    df.to_excel(verification_report_path, index=False)
    print(f"\nğŸ’¾ è©³ç´°æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜åˆ°: {verification_report_path}")

if __name__ == "__main__":
    verify_score_calculation()
