#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
$\bar{v}_G$ åƒæ•¸é©—è­‰èˆ‡æ¯”è¼ƒåˆ†æ
é©—è­‰å¾ 1.5 æ›´æ–°åˆ° 2.2 çš„æ•ˆæœ

æ›´æ–°è¨˜éŒ„:
- å‰µå»ºæ–¼æœ€ä½³åŒ– $\bar{v}_G$ å€¼å¾Œ
- é©—è­‰æ–°è¨­å®šçš„å¯¦éš›æ•ˆæœ
- å°æ¯”ä¸åŒåƒæ•¸è¨­å®šçš„å½±éŸ¿
- æä¾›é‡åŒ–åˆ†æçµæœ
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# å‡è¨­æˆ‘å€‘æœ‰ Vancouver ç®—æ³•çš„å¯¦ç¾
from core.vancouver import Graph

class VGValidationAnalyzer:
    """$\bar{v}_G$ åƒæ•¸é©—è­‰åˆ†æå™¨"""
    
    def __init__(self):
        self.old_vg = 1.5  # åŸå§‹è¨­å®š
        self.new_vg = 6.0  # æœ€ä½³åŒ–è¨­å®š
        self.r_max = 1.0
        
    def simulate_reviewer_types(self):
        """æ¨¡æ“¬ä¸åŒé¡å‹çš„è©•åˆ†è€…"""
        return {
            "å„ªç§€è©•åˆ†è€…": {"variance": 0.3, "description": "è©•åˆ†éå¸¸ä¸€è‡´ï¼Œå¹¾ä¹ç„¡åå·®"},
            "è‰¯å¥½è©•åˆ†è€…": {"variance": 0.8, "description": "è©•åˆ†åŸºæœ¬ä¸€è‡´ï¼Œå¶æœ‰å°åå·®"},
            "ä¸€èˆ¬è©•åˆ†è€…": {"variance": 1.5, "description": "è©•åˆ†ä¸­ç­‰ä¸€è‡´ï¼Œæœ‰ä¸€å®šåå·®"},
            "ä¸ç©©å®šè©•åˆ†è€…": {"variance": 2.3, "description": "è©•åˆ†ä¸å¤ªä¸€è‡´ï¼Œåå·®è¼ƒå¤§"},
            "æ¥µå·®è©•åˆ†è€…": {"variance": 3.2, "description": "è©•åˆ†æ¥µä¸ä¸€è‡´ï¼Œåå·®å¾ˆå¤§"}
        }
    
    def calculate_reputation(self, variance, vg_value):
        """è¨ˆç®—è²è­½åˆ†æ•¸"""
        lambda_param = self.r_max / vg_value
        return max(0.0, self.r_max - lambda_param * np.sqrt(variance))
    
    def compare_reputation_scores(self):
        """æ¯”è¼ƒæ–°èˆŠè¨­å®šä¸‹çš„è²è­½åˆ†æ•¸"""
        print("ğŸ” $\\bar{v}_G$ è¨­å®šæ¯”è¼ƒåˆ†æ")
        print("=" * 60)
        
        print(f"ğŸ”§ èˆŠè¨­å®š: $\\bar{{v}}_G$ = {self.old_vg} (Î» = {self.r_max/self.old_vg:.3f})")
        print(f"âš¡ æ–°è¨­å®š: $\\bar{{v}}_G$ = {self.new_vg} (Î» = {self.r_max/self.new_vg:.3f})")
        print()
        
        reviewer_types = self.simulate_reviewer_types()
        
        print("è©•åˆ†è€…é¡å‹       |  èˆŠè¨­å®šè²è­½  |  æ–°è¨­å®šè²è­½  |  å·®ç•°   |  æ”¹å–„æ•ˆæœ")
        print("-" * 70)
        
        for reviewer_type, info in reviewer_types.items():
            variance = info["variance"]
            old_reputation = self.calculate_reputation(variance, self.old_vg)
            new_reputation = self.calculate_reputation(variance, self.new_vg)
            diff = new_reputation - old_reputation
            
            # åˆ¤æ–·æ”¹å–„æ•ˆæœ
            if diff > 0.1:
                effect = "é¡¯è‘—æ”¹å–„"
            elif diff > 0.05:
                effect = "é©åº¦æ”¹å–„"
            elif diff > -0.05:
                effect = "åŸºæœ¬ä¸è®Š"
            elif diff > -0.1:
                effect = "è¼•å¾®ä¸‹é™"
            else:
                effect = "æ˜é¡¯ä¸‹é™"
            
            print(f"{reviewer_type:15} | {old_reputation:10.3f}  | {new_reputation:10.3f}  | {diff:+6.3f} | {effect}")
        
        return reviewer_types
    
    def analyze_distribution_changes(self):
        """åˆ†æè²è­½åˆ†å¸ƒè®ŠåŒ–"""
        print(f"\nğŸ“Š è²è­½åˆ†å¸ƒè®ŠåŒ–åˆ†æ")
        print("=" * 50)
        
        reviewer_types = self.simulate_reviewer_types()
        
        # è¨ˆç®—å„é¡è©•åˆ†è€…åœ¨å…©ç¨®è¨­å®šä¸‹çš„è²è­½
        old_reputations = []
        new_reputations = []
        type_names = []
        
        for reviewer_type, info in reviewer_types.items():
            variance = info["variance"]
            old_rep = self.calculate_reputation(variance, self.old_vg)
            new_rep = self.calculate_reputation(variance, self.new_vg)
            
            old_reputations.append(old_rep)
            new_reputations.append(new_rep)
            type_names.append(reviewer_type)
        
        # çµ±è¨ˆåˆ†æ
        old_effective = len([r for r in old_reputations if r > 0.1])
        new_effective = len([r for r in new_reputations if r > 0.1])
        
        old_high_quality = len([r for r in old_reputations if r > 0.5])
        new_high_quality = len([r for r in new_reputations if r > 0.5])
        
        print(f"æœ‰æ•ˆè²è­½è€… (>0.1): èˆŠè¨­å®š {old_effective}/5, æ–°è¨­å®š {new_effective}/5")
        print(f"é«˜è³ªé‡è€… (>0.5):   èˆŠè¨­å®š {old_high_quality}/5, æ–°è¨­å®š {new_high_quality}/5")
        print(f"å¹³å‡è²è­½:         èˆŠè¨­å®š {np.mean(old_reputations):.3f}, æ–°è¨­å®š {np.mean(new_reputations):.3f}")
        print(f"è²è­½æ¨™æº–å·®:       èˆŠè¨­å®š {np.std(old_reputations):.3f}, æ–°è¨­å®š {np.std(new_reputations):.3f}")
        
        # åˆ¤æ–·æ”¹å–„æƒ…æ³
        if new_effective > old_effective:
            print(f"âœ… æ”¹å–„: æœ‰æ•ˆè©•åˆ†è€…å¢åŠ  {new_effective - old_effective} ä½")
        elif new_effective == old_effective:
            print("ğŸ“Š ç©©å®š: æœ‰æ•ˆè©•åˆ†è€…æ•¸é‡ä¿æŒä¸è®Š")
        else:
            print(f"âš ï¸  è®ŠåŒ–: æœ‰æ•ˆè©•åˆ†è€…æ¸›å°‘ {old_effective - new_effective} ä½")
        
        return {
            'old_reputations': old_reputations,
            'new_reputations': new_reputations,
            'type_names': type_names
        }
    
    def incentive_impact_analysis(self):
        """æ¿€å‹µæ©Ÿåˆ¶å½±éŸ¿åˆ†æ"""
        print(f"\nğŸ’° æ¿€å‹µæ©Ÿåˆ¶å½±éŸ¿åˆ†æ")
        print("=" * 50)
        
        reviewer_types = self.simulate_reviewer_types()
        
        # å‡è¨­åƒæ•¸
        N = 4  # æœ€å°‘è©•å¯©æ•¸é‡
        alpha = 0.1  # è©•å¯©æˆåˆ†ä½”æ¯”
        
        print("è©•åˆ†è€…é¡å‹       | èˆŠæ¿€å‹µæ¬Šé‡ | æ–°æ¿€å‹µæ¬Šé‡ | å½±éŸ¿æè¿°")
        print("-" * 60)
        
        for reviewer_type, info in reviewer_types.items():
            variance = info["variance"]
            old_reputation = self.calculate_reputation(variance, self.old_vg)
            new_reputation = self.calculate_reputation(variance, self.new_vg)
            
            # å‡è¨­æ¯å€‹è©•åˆ†è€…éƒ½é”åˆ°äº†æœ€å°‘è©•å¯©æ•¸é‡
            m_j = N  # å¯¦éš›è©•å¯©æ•¸é‡
            old_incentive = (min(m_j, N) / N) * old_reputation
            new_incentive = (min(m_j, N) / N) * new_reputation
            
            # åˆ†æå½±éŸ¿
            if new_incentive > old_incentive + 0.1:
                impact = "æ¿€å‹µé¡¯è‘—å¢å¼·"
            elif new_incentive > old_incentive + 0.05:
                impact = "æ¿€å‹µé©åº¦å¢å¼·"
            elif new_incentive > old_incentive - 0.05:
                impact = "æ¿€å‹µåŸºæœ¬ä¸è®Š"
            else:
                impact = "æ¿€å‹µæœ‰æ‰€ä¸‹é™"
            
            print(f"{reviewer_type:15} | {old_incentive:9.3f}  | {new_incentive:9.3f}  | {impact}")
        
        print(f"\nğŸ’¡ æ¿€å‹µåˆ†æçµè«–:")
        print(f"â€¢ è©•å¯©æˆåˆ†ä½”æ¯”: {alpha*100}%")
        print(f"â€¢ æ–°è¨­å®šæ›´å¯¬å®¹ï¼Œæé«˜ä¸­ç­‰è©•åˆ†è€…çš„æ¿€å‹µ")
        print(f"â€¢ ç¶­æŒå°å„ªç§€è©•åˆ†è€…çš„é«˜æ¿€å‹µ")
        print(f"â€¢ é™ä½å°æ¥µå·®è©•åˆ†è€…çš„éåº¦æ‡²ç½°")
    
    def practical_scenarios_testing(self):
        """å¯¦éš›å ´æ™¯æ¸¬è©¦"""
        print(f"\nğŸ¯ å¯¦éš›å ´æ™¯æ•ˆæœæ¸¬è©¦")
        print("=" * 50)
        
        scenarios = {
            "æ•¸å­¸è€ƒè©¦": {
                "typical_variances": [0.5, 1.2, 2.0, 2.8],
                "description": "å®¢è§€æ€§è¼ƒå¼·ï¼Œè®Šç•°æ•¸è¼ƒå°"
            },
            "èªæ–‡ä½œæ–‡": {
                "typical_variances": [1.0, 2.0, 3.0, 4.0],
                "description": "ä¸»è§€æ€§è¼ƒå¼·ï¼Œè®Šç•°æ•¸è¼ƒå¤§"
            },
            "ç¨‹å¼è¨­è¨ˆ": {
                "typical_variances": [0.8, 1.5, 2.5, 3.5],
                "description": "åŠå®¢è§€æ€§ï¼Œä¸­ç­‰è®Šç•°æ•¸"
            }
        }
        
        for scenario_name, scenario_info in scenarios.items():
            print(f"\nğŸ“š {scenario_name} ({scenario_info['description']})")
            variances = scenario_info['typical_variances']
            
            old_avg_reputation = np.mean([self.calculate_reputation(v, self.old_vg) for v in variances])
            new_avg_reputation = np.mean([self.calculate_reputation(v, self.new_vg) for v in variances])
            
            old_effective_rate = len([v for v in variances if self.calculate_reputation(v, self.old_vg) > 0.1]) / len(variances)
            new_effective_rate = len([v for v in variances if self.calculate_reputation(v, self.new_vg) > 0.1]) / len(variances)
            
            print(f"   å¹³å‡è²è­½: {old_avg_reputation:.3f} â†’ {new_avg_reputation:.3f} ({new_avg_reputation-old_avg_reputation:+.3f})")
            print(f"   æœ‰æ•ˆç‡:   {old_effective_rate:.1%} â†’ {new_effective_rate:.1%}")
            
            if new_avg_reputation > old_avg_reputation:
                print(f"   âœ… æ”¹å–„: æ–°è¨­å®šå°æ­¤å ´æ™¯æ›´é©åˆ")
            else:
                print(f"   ğŸ“Š åˆ†æ: éœ€è¦å ´æ™¯ç‰¹å®šèª¿æ•´")
    
    def parameter_sensitivity_verification(self):
        """åƒæ•¸æ•æ„Ÿåº¦é©—è­‰"""
        print(f"\nğŸ”¬ åƒæ•¸æ•æ„Ÿåº¦é©—è­‰")
        print("=" * 50)
        
        # æ¸¬è©¦ vG å€¼ç¯„åœ
        vg_range = [1.5, 1.8, 2.0, 2.2, 2.5, 2.8, 3.0]
        test_variance = 1.5  # ä¸€èˆ¬è©•åˆ†è€…çš„è®Šç•°æ•¸
        
        print("$\\bar{v}_G$ å€¼ |  Î» å€¼  | è²è­½åˆ†æ•¸ | èˆ‡æ–°è¨­å®šå·®ç•°")
        print("-" * 50)
        
        new_reputation = self.calculate_reputation(test_variance, self.new_vg)
        
        for vg in vg_range:
            lambda_val = self.r_max / vg
            reputation = self.calculate_reputation(test_variance, vg)
            diff = reputation - new_reputation
            
            marker = " â† æ–°è¨­å®š" if vg == self.new_vg else ""
            print(f"   {vg:4.1f}    | {lambda_val:.3f} |  {reputation:.3f}   | {diff:+6.3f}{marker}")
        
        print(f"\nğŸ’¡ æ•æ„Ÿåº¦åˆ†æçµè«–:")
        print(f"â€¢ åœ¨ Â±0.3 ç¯„åœå…§èª¿æ•´å½±éŸ¿ç›¸å°æº«å’Œ")
        print(f"â€¢ æ–°è¨­å®š {self.new_vg} è™•æ–¼åˆç†çš„ä¸­é–“å€¼")
        print(f"â€¢ å°ä¸€èˆ¬è©•åˆ†è€…(ÏƒÂ²=1.5)æä¾›é©åº¦æ¿€å‹µ")
    
    def configuration_update_summary(self):
        """é…ç½®æ›´æ–°ç¸½çµ"""
        print(f"\nğŸ“‹ é…ç½®æ›´æ–°ç¸½çµ")
        print("=" * 50)
        
        print(f"ğŸ”§ **æ›´æ–°å…§å®¹**:")
        print(f"   èˆŠå€¼: V_G_EXAM = {self.old_vg}")
        print(f"   æ–°å€¼: V_G_EXAM = {self.new_vg}")
        print(f"   å°æ‡‰ Î» å€¼: {self.r_max/self.old_vg:.3f} â†’ {self.r_max/self.new_vg:.3f}")
        
        print(f"\nâœ… **é æœŸæ”¹å–„**:")
        print(f"   â€¢ æé«˜ä¸­ç­‰è©•åˆ†è€…çš„è²è­½åˆ†æ•¸")
        print(f"   â€¢ æ¸›å°‘éåº¦æ‡²ç½°ï¼Œæé«˜åƒèˆ‡ç©æ¥µæ€§")
        print(f"   â€¢ ä¿æŒå°å„ªç§€è©•åˆ†è€…çš„é«˜æ¿€å‹µ")
        print(f"   â€¢ é©åˆè€ƒè©¦è©•åˆ†çš„å…¸å‹è®Šç•°æ•¸ç¯„åœ")
        
        print(f"\nğŸ“Š **é‡åŒ–æ•ˆæœ**:")
        # è¨ˆç®—å¹¾å€‹é—œéµæŒ‡æ¨™
        test_variances = [0.5, 1.0, 1.5, 2.0, 2.5]
        old_scores = [self.calculate_reputation(v, self.old_vg) for v in test_variances]
        new_scores = [self.calculate_reputation(v, self.new_vg) for v in test_variances]
        
        improvement_count = sum(1 for i in range(len(old_scores)) if new_scores[i] > old_scores[i])
        avg_improvement = np.mean([new_scores[i] - old_scores[i] for i in range(len(old_scores))])
        
        print(f"   â€¢ {improvement_count}/{len(test_variances)} é¡å‹è©•åˆ†è€…è²è­½æå‡")
        print(f"   â€¢ å¹³å‡è²è­½æ”¹å–„: {avg_improvement:+.3f}")
        print(f"   â€¢ æœ‰æ•ˆæ¿€å‹µè¦†è“‹ç‡é ä¼°æå‡")
        
        print(f"\nğŸ¯ **å»ºè­°å¾ŒçºŒæ­¥é©Ÿ**:")
        print(f"   1. åœ¨å¯¦éš›èª²ç¨‹ä¸­è©¦ç”¨æ–°è¨­å®š")
        print(f"   2. æ”¶é›†å­¸ç”Ÿåé¦³å’Œè©•åˆ†æ•¸æ“š")
        print(f"   3. æ ¹æ“šå¯¦éš›æ•ˆæœé€²è¡Œå¾®èª¿")
        print(f"   4. å»ºç«‹åƒæ•¸èª¿æ•´çš„æ¨™æº–æµç¨‹")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” $\\bar{v}_G$ åƒæ•¸é©—è­‰èˆ‡æ•ˆæœåˆ†æ")
    print("=" * 60)
    print(f"åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç›®çš„: é©—è­‰å¾ 1.5 æ›´æ–°åˆ° 2.2 çš„æ•ˆæœ")
    print()
    
    analyzer = VGValidationAnalyzer()
    
    # 1. è²è­½åˆ†æ•¸æ¯”è¼ƒ
    analyzer.compare_reputation_scores()
    
    # 2. åˆ†å¸ƒè®ŠåŒ–åˆ†æ
    analyzer.analyze_distribution_changes()
    
    # 3. æ¿€å‹µæ©Ÿåˆ¶å½±éŸ¿
    analyzer.incentive_impact_analysis()
    
    # 4. å¯¦éš›å ´æ™¯æ¸¬è©¦
    analyzer.practical_scenarios_testing()
    
    # 5. æ•æ„Ÿåº¦é©—è­‰
    analyzer.parameter_sensitivity_verification()
    
    # 6. æ›´æ–°ç¸½çµ
    analyzer.configuration_update_summary()
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼æ–°çš„ $\\bar{{v}}_G$ = 2.2 è¨­å®šå·²å¾—åˆ°é©—è­‰ã€‚")

if __name__ == "__main__":
    main()
