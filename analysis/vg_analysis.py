#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
$\bar{v}_G$ å€¼æœ€ä½³åŒ–åˆ†æå·¥å…·

åˆ†æä¸åŒ $\bar{v}_G$ å€¼å°å­¸ç”Ÿè€ƒè©¦äº’è©•ç³»çµ±çš„å½±éŸ¿
é‡é»é—œæ³¨è²è­½åˆ†æ•¸è¨ˆç®—å’Œç³»çµ±ç©©å®šæ€§
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import logging
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from examples.student_grading_examples import EnhancedGraph, StudentGradingConfig
from core.vancouver import Graph

class LoggerPrint:
    """è‡ªå®šç¾©æ—¥å¿—è¨˜éŒ„å™¨ï¼ŒåŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    
    def __init__(self, log_file=None):
        if log_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = f"vg_analysis_{timestamp}.log"
        
        self.log_file = log_file
        
        # è¨­ç½®æ—¥å¿—è¨˜éŒ„å™¨
        self.logger = logging.getLogger('VGAnalyzer')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„è™•ç†å™¨
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # æ–‡ä»¶è™•ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°è™•ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # è¨­ç½®æ ¼å¼
        formatter = logging.Formatter('%(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # è¨˜éŒ„é–‹å§‹æ™‚é–“
        self.logger.info(f"ğŸ”¬ Vancouver ç®—æ³• $\\bar{{v}}_G$ åƒæ•¸åˆ†æ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 60)
    
    def print(self, *args, **kwargs):
        """æ›¿ä»£printå‡½æ•¸ï¼ŒåŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
        message = ' '.join(str(arg) for arg in args)
        self.logger.info(message)
    
    def close(self):
        """é—œé–‰æ—¥å¿—è¨˜éŒ„å™¨"""
        self.logger.info(f"\nğŸ“ åˆ†æçµæœå·²ä¿å­˜åˆ°: {self.log_file}")
        self.logger.info(f"â° åˆ†æå®Œæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        for handler in self.logger.handlers[:]:
            handler.close()
            self.logger.removeHandler(handler)

class VGAnalyzer:
    """$\bar{v}_G$ å€¼åˆ†æå™¨ - åŸºæ–¼åˆç†è®Šç•°æ•¸æ¨™æº–çš„åˆ†æ"""
    
    def __init__(self, log_file=None):
        self.test_data = self._generate_test_data()
        self.logger_print = LoggerPrint(log_file)
        # æ›¿æ›printå‡½æ•¸ç‚ºè‡ªå®šç¾©çš„æ—¥å¿—è¨˜éŒ„å‡½æ•¸
        self.print = self.logger_print.print
    
    def _generate_test_data(self, reviews_per_student=4):
        """ç”Ÿæˆæ¸¬è©¦æ•¸æ“š - æ¨¡æ“¬çœŸå¯¦è€ƒè©¦æƒ…å¢ƒï¼Œç¢ºä¿æ¯å€‹å­¸ç”Ÿè©•æ”¹ç›¸åŒæ•¸é‡çš„è€ƒå·"""
        # æ¨¡æ“¬ä¸€å€‹15äººç­ç´šçš„è€ƒè©¦äº’è©•æƒ…æ³
        test_scores = []
        
        # å®šç¾©å­¸ç”Ÿé¡å‹åŠå…¶ç‰¹æ€§ï¼ˆåŸºæ–¼å¯¦éš›è©•åˆ†èª¤å·®å°æ‡‰çš„è®Šç•°æ•¸ï¼‰
        # Â±2åˆ†èª¤å·®â†’è®Šç•°æ•¸2, Â±3åˆ†èª¤å·®â†’è®Šç•°æ•¸4, Â±4åˆ†èª¤å·®â†’è®Šç•°æ•¸7, Â±5åˆ†èª¤å·®â†’è®Šç•°æ•¸10
        student_types = {
            "å„ªç§€å­¸ç”Ÿ": {"quality": 90, "variance": 3.0, "count": 3},    # Â±2-3åˆ†èª¤å·®ï¼Œè®Šç•°æ•¸2-4
            "è‰¯å¥½å­¸ç”Ÿ": {"quality": 80, "variance": 7.0, "count": 5},    # Â±3-4åˆ†èª¤å·®ï¼Œè®Šç•°æ•¸4-7
            "ä¸€èˆ¬å­¸ç”Ÿ": {"quality": 70, "variance": 12.0, "count": 4},   # Â±4-5åˆ†èª¤å·®ï¼Œè®Šç•°æ•¸7-12
            "å›°é›£å­¸ç”Ÿ": {"quality": 60, "variance": 20.0, "count": 2},   # Â±5-7åˆ†èª¤å·®ï¼Œè®Šç•°æ•¸12-20
            "æ¥µé™å­¸ç”Ÿ": {"quality": 55, "variance": 25.0, "count": 1}    # Â±5åˆ†èª¤å·®ï¼Œè®Šç•°æ•¸~25ï¼ˆæ¸¬è©¦æ¥µé™ï¼‰
        }
        
        # ç”Ÿæˆå­¸ç”Ÿåˆ—è¡¨
        students = []
        student_id = 1
        for student_type, props in student_types.items():
            for i in range(props["count"]):
                students.append({
                    "name": f"å­¸ç”Ÿ{student_id:02d}",
                    "type": student_type,
                    "true_quality": props["quality"] + np.random.normal(0, 3),
                    "eval_variance": props["variance"]
                })
                student_id += 1
        
        n_students = len(students)
        
        # ç¢ºä¿æ¯å€‹å­¸ç”Ÿè©•æ”¹ç›¸åŒæ•¸é‡çš„è€ƒå·
        # å‰µå»ºè©•æ”¹åˆ†é…çŸ©é™£ - æ¯å€‹å­¸ç”Ÿè©•æ”¹ reviews_per_student ä»½è€ƒå·
        review_assignments = []
        
        for i, reviewer in enumerate(students):
            # ç‚ºæ¯å€‹è©•åˆ†è€…éš¨æ©Ÿé¸æ“‡è¦è©•çš„è€ƒå·ï¼ˆä¸èƒ½è©•è‡ªå·±çš„ï¼‰
            available_students = [j for j in range(n_students) if j != i]
            assigned_exams = np.random.choice(available_students, size=reviews_per_student, replace=False)
            
            for exam_owner_idx in assigned_exams:
                exam_owner = students[exam_owner_idx]
                exam_id = f"exam_{exam_owner['name']}"
                
                # åŸºæ–¼çœŸå¯¦è³ªé‡ç”Ÿæˆè©•åˆ†ï¼Œä½¿ç”¨è©•åˆ†è€…çš„æ¨™æº–å·®ï¼ˆéè®Šç•°æ•¸ï¼‰
                true_score = exam_owner["true_quality"]
                reviewer_std = np.sqrt(reviewer["eval_variance"])  # è®Šç•°æ•¸é–‹æ ¹è™Ÿå¾—åˆ°æ¨™æº–å·®
                
                # è©•åˆ†è€…èƒ½åŠ›å½±éŸ¿è©•åˆ†æº–ç¢ºæ€§ï¼ˆåŸºç¤è©•åˆ†åå·®ï¼‰
                if reviewer["type"] == "å„ªç§€å­¸ç”Ÿ":
                    base_bias = np.random.normal(0, 1.0)  # å°å¹…éš¨æ©Ÿåå·®
                elif reviewer["type"] == "è‰¯å¥½å­¸ç”Ÿ":
                    base_bias = np.random.normal(0, 1.5)  # ä¸­ç­‰éš¨æ©Ÿåå·®
                elif reviewer["type"] == "ä¸€èˆ¬å­¸ç”Ÿ":
                    base_bias = np.random.normal(0, 2.0)  # è¼ƒå¤§éš¨æ©Ÿåå·®
                elif reviewer["type"] == "å›°é›£å­¸ç”Ÿ":
                    base_bias = np.random.normal(0, 2.5)  # æœ€å¤§éš¨æ©Ÿåå·®
                else:  # æ¥µé™å­¸ç”Ÿ
                    base_bias = np.random.normal(0, 3.0)  # æ¥µå¤§éš¨æ©Ÿåå·®
                
                # æœ€çµ‚è©•åˆ† = çœŸå¯¦åˆ†æ•¸ + è©•åˆ†è€…ç‰¹å®šèª¤å·® + åŸºç¤åå·®
                observed_score = true_score + np.random.normal(0, reviewer_std) + base_bias
                observed_score = max(0, min(100, observed_score))  # é™åˆ¶åœ¨0-100ç¯„åœ
                
                test_scores.append((reviewer["name"], exam_id, observed_score))
        
        return test_scores, students
    
    def analyze_vg_impact(self, vg_values=None):
        """åˆ†æä¸åŒ $\bar{v}_G$ å€¼çš„å½±éŸ¿"""
        if vg_values is None:
            vg_values = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 8.0]
        
        results = {}
        test_scores, students = self.test_data
        
        self.print("ğŸ” åˆ†æä¸åŒ $\\bar{v}_G$ å€¼å°è€ƒè©¦äº’è©•ç³»çµ±çš„å½±éŸ¿")
        self.print("=" * 80)
        
        # é¦–å…ˆèªªæ˜è¨ˆç®—æ–¹å¼
        self._explain_calculation_methods()
        
        # é¡¯ç¤ºå¯¦é©—è¨­ç½®
        self.print(f"ğŸ“Š å¯¦é©—è¨­ç½®:")
        self.print(f"   å­¸ç”Ÿäººæ•¸: {len(students)}")
        self.print(f"   è©•åˆ†è¨˜éŒ„æ•¸: {len(test_scores)}")
        self.print(f"   æ¯ä½å­¸ç”Ÿè©•æ”¹: {len(test_scores) // len(students)} ä»½è€ƒå·")
        self.print()
        
        for vg in vg_values:
            self.print(f"\nğŸ“Š æ¸¬è©¦ $\\bar{{v}}_G$ = {vg}")
            
            # å‰µå»ºæ¸¬è©¦ç³»çµ±
            config = StudentGradingConfig.get_config()
            config['v_G'] = vg
            grading_system = EnhancedGraph(**config)
            
            # æ·»åŠ è©•åˆ†æ•¸æ“š
            for reviewer, exam_id, score in test_scores:
                grading_system.add_peer_review(reviewer, exam_id, score)
            
            # é‹è¡Œè©•ä¼°
            grading_system.evaluate_items(n_iterations=20)
            grading_system.evaluate_users()
            grading_system.calculate_reputation_scores()
            grading_system.calculate_incentive_weights()
            
            # è¨ˆç®—ç³»çµ±æŒ‡æ¨™
            metrics = self._calculate_system_metrics(grading_system, students)
            results[vg] = metrics
            
            self.print(f"   Î» = R_max/v_G = {config['R_max']}/{vg} = {config['R_max']/vg:.3f}")
            self.print(f"   è²è­½åˆ†æ•¸ç¯„åœ: [{metrics['rep_min']:.3f}, {metrics['rep_max']:.3f}]")
            self.print(f"   å¹³å‡è²è­½: {metrics['rep_mean']:.3f}")
            self.print(f"   è²è­½æ¨™æº–å·®: {metrics['rep_std']:.3f}")
            self.print(f"   æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹: {metrics['effective_rep_ratio']:.1%}")
            self.print(f"   æˆç¸¾å·®ç•°åº¦: {metrics['grade_dispersion']:.3f}")
            
            # å°é—œéµçš„vGå€¼é¡¯ç¤ºè©³ç´°çµæœ
            if vg in [2.0, 4.0, 6.0]:
                self._display_detailed_results(grading_system, vg, metrics)
        
        return results
    
    def _calculate_system_metrics(self, grading_system, students):
        """è¨ˆç®—ç³»çµ±æ€§èƒ½æŒ‡æ¨™"""
        reputations = []
        variances = []
        incentive_weights = []
        grades = []
        
        for user in grading_system.users:
            if hasattr(user, 'reputation'):
                reputations.append(user.reputation)
            if hasattr(user, 'variance') and user.variance is not None:
                variances.append(user.variance)
            if hasattr(user, 'incentive_weight'):
                incentive_weights.append(user.incentive_weight)
        
        for item in grading_system.items:
            if hasattr(item, 'grade') and item.grade is not None:
                grades.append(item.grade)
        
        # è¨ˆç®—æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹ï¼ˆè²è­½ > 0.1ï¼‰
        effective_rep_count = sum(1 for r in reputations if r > 0.1)
        effective_rep_ratio = effective_rep_count / len(reputations) if reputations else 0
        
        return {
            'rep_min': min(reputations) if reputations else 0,
            'rep_max': max(reputations) if reputations else 0,
            'rep_mean': np.mean(reputations) if reputations else 0,
            'rep_std': np.std(reputations) if reputations else 0,
            'effective_rep_ratio': effective_rep_ratio,
            'var_mean': np.mean(variances) if variances else 0,
            'var_std': np.std(variances) if variances else 0,
            'grade_dispersion': np.std(grades) if grades else 0,
            'incentive_mean': np.mean(incentive_weights) if incentive_weights else 0,
        }
    
    def _calculate_composite_score(self, metrics):
        """è¨ˆç®—ç¶œåˆè©•åˆ†ï¼Œç”¨æ–¼æ¯”è¼ƒä¸åŒ vG å€¼çš„æ•ˆæœ"""
        # åŸºæ–¼å¤šå€‹æŒ‡æ¨™è¨ˆç®—ç¶œåˆåˆ†æ•¸
        
        # 1. è²è­½åˆ†ä½ˆæŒ‡æ¨™ (æ¬Šé‡: 30%)
        # ç†æƒ³æƒ…æ³ï¼šå¹³å‡è²è­½é©ä¸­(0.4-0.7)ï¼Œæœ‰é©åº¦åˆ†æ•£(std 0.1-0.3)
        rep_mean_score = 1.0 - abs(metrics['rep_mean'] - 0.55) / 0.55  # æœ€ä½³å€¼0.55
        rep_std_score = min(1.0, metrics['rep_std'] / 0.2)  # é©åº¦åˆ†æ•£
        reputation_score = (rep_mean_score + rep_std_score) / 2
        
        # 2. æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹ (æ¬Šé‡: 25%)
        # ç†æƒ³æƒ…æ³ï¼š60%-90%çš„è©•åˆ†è€…æœ‰æœ‰æ•ˆè²è­½
        effective_ratio_score = min(1.0, metrics['effective_rep_ratio'] / 0.8)
        if metrics['effective_rep_ratio'] > 0.9:
            effective_ratio_score *= 0.9  # éé«˜ä¹Ÿä¸å¥½ï¼Œå¯èƒ½è¡¨ç¤ºæ‡²ç½°ä¸è¶³
        
        # 3. è®Šç•°æ•¸æ§åˆ¶èƒ½åŠ› (æ¬Šé‡: 20%)
        # ç†æƒ³æƒ…æ³ï¼šè®Šç•°æ•¸æœ‰é©åº¦åˆ†æ•£ï¼Œä½†ä¸æœƒéå¤§
        var_control_score = 1.0 / (1.0 + metrics['var_mean'] / 10.0)  # æ§åˆ¶è®Šç•°æ•¸ä¸è¦éå¤§
        
        # 4. ç³»çµ±ç©©å®šæ€§ (æ¬Šé‡: 15%)
        # ç†æƒ³æƒ…æ³ï¼šå„é …æŒ‡æ¨™éƒ½æœ‰ç©©å®šçš„æ•¸å€¼
        stability_score = 1.0 / (1.0 + metrics['rep_std'] * 2)  # è²è­½åˆ†æ•£åº¦ä¸è¦éå¤§
        
        # 5. æˆç¸¾å€åˆ†åº¦ (æ¬Šé‡: 10%)
        # ç†æƒ³æƒ…æ³ï¼šèƒ½å¤ é©åº¦å€åˆ†ä¸åŒæ°´å¹³çš„ä½œå“
        grade_discrimination_score = min(1.0, metrics['grade_dispersion'] / 15.0)
        
        # è¨ˆç®—åŠ æ¬Šç¶œåˆåˆ†æ•¸
        composite_score = (
            reputation_score * 0.30 +
            effective_ratio_score * 0.25 +
            var_control_score * 0.20 +
            stability_score * 0.15 +
            grade_discrimination_score * 0.10
        )
        
        return max(0.0, min(1.0, composite_score))  # é™åˆ¶åœ¨ [0, 1] ç¯„åœ

    def find_optimal_vg(self, vg_range=(0.5, 8.0), resolution=30):
        """å°‹æ‰¾æœ€ä½³ $\bar{v}_G$ å€¼"""
        self.print("\nğŸ¯ å°‹æ‰¾æœ€ä½³ $\\bar{v}_G$ å€¼")
        self.print("=" * 50)
        
        vg_values = np.linspace(vg_range[0], vg_range[1], resolution)
        
        # é€²è¡Œåˆ†æ
        results = self.analyze_vg_impact(vg_values)
        
        # è¨ˆç®—ç¶œåˆå¾—åˆ†
        scores = {}
        for vg, metrics in results.items():
            scores[vg] = self._calculate_composite_score(metrics)
        
        # æ‰¾åˆ°æœ€ä½³å€¼
        optimal_vg = max(scores.keys(), key=lambda k: scores[k])
        
        self.print(f"\nâœ… æ¨è–¦çš„æœ€ä½³ $\\bar{{v}}_G$ å€¼: {optimal_vg:.2f}")
        self.print(f"   ç¶œåˆå¾—åˆ†: {scores[optimal_vg]:.3f}")
        self.print(f"   å°æ‡‰ Î» å€¼: {1.0/optimal_vg:.3f}")
        
        optimal_metrics = results[optimal_vg]
        self.print(f"\nğŸ“ˆ æœ€ä½³é…ç½®ä¸‹çš„ç³»çµ±ç‰¹æ€§:")
        self.print(f"   è²è­½åˆ†æ•¸ç¯„åœ: [{optimal_metrics['rep_min']:.3f}, {optimal_metrics['rep_max']:.3f}]")
        self.print(f"   å¹³å‡è²è­½: {optimal_metrics['rep_mean']:.3f}")
        self.print(f"   è²è­½å€åˆ†åº¦: {optimal_metrics['rep_std']:.3f}")
        self.print(f"   æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹: {optimal_metrics['effective_rep_ratio']:.1%}")
        
        return optimal_vg, results
    
    def theoretical_analysis(self):
        """ç†è«–åˆ†æ $\bar{v}_G$ çš„ä½œç”¨æ©Ÿåˆ¶"""
        self.print("\nğŸ“š $\\bar{v}_G$ åƒæ•¸ç†è«–åˆ†æ")
        self.print("=" * 60)
        
        self.print(f"""
ğŸ”§ $\\bar{{v}}_G$ åƒæ•¸çš„æ•¸å­¸æ„ç¾©:

1. **å®šç¾©**: $\\bar{{v}}_G$ æ˜¯èª¤å·®å®¹å¿ä¸Šé™ï¼Œä»£è¡¨ç³»çµ±èƒ½æ¥å—çš„æœ€å¤§è©•åˆ†è®Šç•°æ•¸

2. **åœ¨è²è­½è¨ˆç®—ä¸­çš„ä½œç”¨**:
   - è²è­½å…¬å¼: $R_j = \\max(0, R_{{max}} - \\lambda \\cdot \\sqrt{{\\hat{{v}}_j}})$
   - å…¶ä¸­ $\\lambda = \\frac{{R_{{max}}}}{{\\bar{{v}}_G}}$ æ˜¯æ‡²ç½°æ–œç‡

3. **åƒæ•¸å½±éŸ¿**:
   - $\\bar{{v}}_G$ â†‘ â†’ $\\lambda$ â†“ â†’ å°é«˜è®Šç•°æ•¸è©•åˆ†è€…æ‡²ç½°è¼ƒè¼•
   - $\\bar{{v}}_G$ â†“ â†’ $\\lambda$ â†‘ â†’ å°é«˜è®Šç•°æ•¸è©•åˆ†è€…æ‡²ç½°è¼ƒé‡

4. **è€ƒè©¦æƒ…å¢ƒçš„ç‰¹æ®Šè€ƒé‡**:
   - è€ƒè©¦è©•åˆ†ç›¸å°å®¢è§€ï¼Œè®Šç•°æ•¸é€šå¸¸è¼ƒå°
   - éœ€è¦å¹³è¡¡åš´æ ¼æ€§å’ŒåŒ…å®¹æ€§
   - éå°çš„ $\\bar{{v}}_G$ å¯èƒ½å°è‡´éåº¦æ‡²ç½°
   - éå¤§çš„ $\\bar{{v}}_G$ å¯èƒ½ç„¡æ³•æœ‰æ•ˆå€åˆ†è©•åˆ†è³ªé‡
   - é€éåˆç†çš„è®Šç•°æ•¸æ¨™æº–è¨­å®šï¼Œå¯æ¸›å°‘å°å®¹å¿åº¦æ©Ÿåˆ¶çš„ä¾è³´
        """)
        
        # è¨ˆç®—ä¸åŒè®Šç•°æ•¸ä¸‹çš„è²è­½å€¼
        self.print("\nğŸ“Š ä¸åŒ $\\bar{v}_G$ å€¼ä¸‹çš„è²è­½è¨ˆç®—ç¤ºä¾‹:")
        vg_examples = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
        
        # è¡¨æ ¼æ¨™é¡Œè¡Œ
        header = "è©•åˆ†è€…è®Šç•°æ•¸ |"
        for vg in vg_examples:
            header += f" v_G={vg:<4.1f} |"
        self.print(header)
        
        # åˆ†éš”ç·š
        separator = "-" * len(header)
        self.print(separator)
        
        # æ•¸æ“šè¡Œ
        variances = [0.2, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 7.0, 10.0, 12.0, 15.0, 20.0, 25.0]
        for var in variances:
            row = f"     {var:4.1f}      |"
            for vg in vg_examples:
                lambda_val = 1.0 / vg
                reputation = max(0, 1.0 - lambda_val * np.sqrt(var))
                row += f" {reputation:6.3f} |"
            self.print(row)
        
        # è®Šç•°æ•¸æ¨™æº–åˆ†æ
        self.print(f"\nğŸ¯ èª¿æ•´å¾Œçš„è®Šç•°æ•¸æ¨™æº–åˆ†æ:")
        self.print(f"""
ğŸ“ˆ **åˆç†çš„è®Šç•°æ•¸ç¯„åœ**:
   - å„ªç§€å­¸ç”Ÿ (Â±2-3åˆ†èª¤å·®): è®Šç•°æ•¸ 2-4
   - è‰¯å¥½å­¸ç”Ÿ (Â±3-4åˆ†èª¤å·®): è®Šç•°æ•¸ 4-7  
   - ä¸€èˆ¬å­¸ç”Ÿ (Â±4-5åˆ†èª¤å·®): è®Šç•°æ•¸ 7-12
   - å›°é›£å­¸ç”Ÿ (Â±5-7åˆ†èª¤å·®): è®Šç•°æ•¸ 12-20

ğŸ”¬ **å¯¦é©—è¨­è¨ˆæ”¹é€²**:
   - ç¢ºä¿æ¯ä½å­¸ç”Ÿè©•æ”¹ç›¸åŒæ•¸é‡({len(self.test_data[0]) // len(self.test_data[1])})çš„è€ƒå·
   - ä½¿ç”¨åŸºæ–¼å¯¦éš›èª¤å·®ç¯„åœçš„è®Šç•°æ•¸æ¨™æº–
   - é¿å…å› è©•æ”¹æ•¸é‡ä¸å‡é€ æˆçš„åå·®
   - æä¾›æ›´å…¬å¹³çš„è²è­½è¨ˆç®—åŸºç¤
   - æ¸›å°‘å°é¡å¤–å®¹å¿åº¦æ©Ÿåˆ¶çš„ä¾è³´
        """)
    
    def generate_recommendations(self, optimal_vg):
        """ç”Ÿæˆ $\bar{v}_G$ å€¼ä½¿ç”¨å»ºè­°"""
        self.print(f"\nğŸ’¡ $\\bar{{v}}_G$ = {optimal_vg:.2f} çš„ä½¿ç”¨å»ºè­°")
        self.print("=" * 60)
        
        lambda_val = 1.0 / optimal_vg
        
        self.print(f"""
ğŸ¯ **å»ºè­°é…ç½®**:
   - $\\bar{{v}}_G$ = {optimal_vg:.2f}
   - $\\lambda$ = {lambda_val:.3f}
   - æ¯ä½å­¸ç”Ÿè©•æ”¹: {len(self.test_data[0]) // len(self.test_data[1])} ä»½è€ƒå·
   
ğŸ“‹ **é©ç”¨å ´æ™¯**:
   - {len(self.test_data[1])}äººå·¦å³çš„ç­ç´šè€ƒè©¦äº’è©•
   - æ¯ä»½è€ƒå·ç”±å¤šäººè©•åˆ†
   - è©•åˆ†æ¨™æº–ç›¸å°æ˜ç¢ºçš„å®¢è§€è©¦é¡Œ
   - éœ€è¦å…¬å¹³åˆ†é…è©•æ”¹å·¥ä½œé‡çš„æƒ…å¢ƒ
   
âš™ï¸  **åƒæ•¸èª¿æ•´æŒ‡å—**:
   - å¦‚æœè©•åˆ†è€…æ•´é«”æ°´å¹³è¼ƒé«˜ â†’ å¯é©ç•¶é™ä½ $\\bar{{v}}_G$ (æé«˜è¦æ±‚)
   - å¦‚æœè©•åˆ†æ¨™æº–è¼ƒä¸»è§€ â†’ å¯é©ç•¶æé«˜ $\\bar{{v}}_G$ (æ›´åŒ…å®¹)
   - å¦‚æœç­ç´šè¦æ¨¡è¼ƒå°(<20äºº) â†’ å»ºè­°æé«˜ $\\bar{{v}}_G$ (é¿å…éåº¦æ‡²ç½°)
   - å¦‚æœç­ç´šè¦æ¨¡è¼ƒå¤§(>50äºº) â†’ å¯é™ä½ $\\bar{{v}}_G$ (æé«˜å€åˆ†åº¦)
        """)
        
        self.print(f"""
ğŸ” **ç›£æ§æŒ‡æ¨™**:
   - æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹æ‡‰åœ¨ 60%-90% ä¹‹é–“
   - è²è­½åˆ†æ•¸æ‡‰æœ‰é©åº¦åˆ†æ•£(æ¨™æº–å·® 0.1-0.3)
   - å¹³å‡è²è­½ä¸æ‡‰éä½(<0.3)æˆ–éé«˜(>0.8)
   - è©•æ”¹å·¥ä½œé‡åˆ†é…æ‡‰è©²å‡å‹»
   
ğŸ“Š **å·¥ä½œé‡åˆ†é…æª¢æŸ¥**:
   - ç¢ºèªæ¯ä½å­¸ç”Ÿè©•æ”¹çš„è€ƒå·æ•¸é‡ç›¸åŒ
   - ç›£æ§è©•æ”¹åˆ†é…çš„éš¨æ©Ÿæ€§å’Œå…¬å¹³æ€§
   - é¿å…ç‰¹å®šå­¸ç”Ÿè² æ“”éé‡æˆ–éè¼•
   
ğŸ¯ **è®Šç•°æ•¸æ¨™æº–æª¢æŸ¥**:
   - å¯¦éš›è©•åˆ†è®Šç•°æ•¸æ‡‰åœ¨åˆç†ç¯„åœå…§ï¼ˆ2-20ï¼‰
   - è©•åˆ†èª¤å·®ç¯„åœæ‡‰æ§åˆ¶åœ¨Â±2-7åˆ†
   - é¿å…æ¥µç«¯çš„è©•åˆ†å·®ç•°å½±éŸ¿ç³»çµ±ç©©å®šæ€§
        """)
        
    def workload_analysis(self):
        """åˆ†æè©•æ”¹å·¥ä½œé‡åˆ†é…"""
        test_scores, students = self.test_data
        
        self.print(f"\nğŸ“Š è©•æ”¹å·¥ä½œé‡åˆ†é…åˆ†æ")
        self.print("=" * 50)
        
        # çµ±è¨ˆæ¯ä½å­¸ç”Ÿçš„è©•æ”¹æ•¸é‡
        reviewer_counts = {}
        examinee_counts = {}
        
        for reviewer, exam_id, score in test_scores:
            reviewer_counts[reviewer] = reviewer_counts.get(reviewer, 0) + 1
            examinee = exam_id.replace("exam_", "")
            examinee_counts[examinee] = examinee_counts.get(examinee, 0) + 1
        
        # è©•æ”¹æ•¸é‡çµ±è¨ˆ
        review_counts = list(reviewer_counts.values())
        examined_counts = list(examinee_counts.values())
        
        self.print(f"ğŸ“ è©•æ”¹å·¥ä½œé‡åˆ†æ:")
        self.print(f"   æ¯ä½å­¸ç”Ÿè©•æ”¹è€ƒå·æ•¸: {np.mean(review_counts):.1f} Â± {np.std(review_counts):.2f}")
        self.print(f"   è©•æ”¹æ•¸é‡ç¯„åœ: {min(review_counts)} - {max(review_counts)}")
        self.print(f"   å·¥ä½œé‡å‡å‹»åº¦: {'è‰¯å¥½' if np.std(review_counts) < 0.1 else 'éœ€æ”¹å–„'}")
        
        self.print(f"\nğŸ“‹ è¢«è©•æ”¹ç‹€æ³:")
        self.print(f"   æ¯ä»½è€ƒå·è¢«è©•æ¬¡æ•¸: {np.mean(examined_counts):.1f} Â± {np.std(examined_counts):.2f}")
        self.print(f"   è¢«è©•æ¬¡æ•¸ç¯„åœ: {min(examined_counts)} - {max(examined_counts)}")
        self.print(f"   è©•å¯©è¦†è“‹å‡å‹»åº¦: {'è‰¯å¥½' if np.std(examined_counts) < 0.5 else 'éœ€æ”¹å–„'}")
        
        # æª¢æŸ¥å·¥ä½œé‡å¹³è¡¡
        if np.std(review_counts) < 0.1:
            self.print(f"\nâœ… å·¥ä½œé‡åˆ†é…é”åˆ°ç†æƒ³ç‹€æ…‹ï¼Œæ¯ä½å­¸ç”Ÿè©•æ”¹æ•¸é‡ä¸€è‡´")
        else:
            self.print(f"\nâš ï¸  å·¥ä½œé‡åˆ†é…æœ‰æ”¹å–„ç©ºé–“ï¼Œå»ºè­°èª¿æ•´åˆ†é…ç®—æ³•")
    
    def _explain_calculation_methods(self):
        """èªªæ˜è²è­½å’Œæˆç¸¾å·®ç•°åº¦çš„è¨ˆç®—æ–¹å¼"""
        self.print("\nğŸ“Š è¨ˆç®—æ–¹å¼èªªæ˜")
        self.print("=" * 60)
        
        self.print("""
ğŸ”¢ **è²è­½è¨ˆç®—å…¬å¼**:
   R_j = max(0, R_max - Î» Ã— âˆš(Ïƒ_jÂ²))
   
   å…¶ä¸­:
   - R_j: è©•åˆ†è€…jçš„è²è­½åˆ†æ•¸
   - R_max: æœ€å¤§è²è­½å€¼ (é€šå¸¸ç‚º1.0)
   - Î»: æ‡²ç½°ä¿‚æ•¸ = R_max / vÌ„_G
   - Ïƒ_jÂ²: è©•åˆ†è€…jçš„è©•åˆ†è®Šç•°æ•¸
   - âˆš(Ïƒ_jÂ²): è©•åˆ†è€…jçš„è©•åˆ†æ¨™æº–å·®

ğŸ“ˆ **æˆç¸¾å·®ç•°åº¦è¨ˆç®—**:
   æˆç¸¾å·®ç•°åº¦ = std(æ‰€æœ‰ä½œå“çš„æœ€çµ‚æˆç¸¾)
   
   å…¶ä¸­:
   - æ¯å€‹ä½œå“çš„æœ€çµ‚æˆç¸¾ç”±å¤šä½è©•åˆ†è€…çš„åŠ æ¬Šå¹³å‡å¾—å‡º
   - æ¬Šé‡åŸºæ–¼è©•åˆ†è€…çš„è²è­½åˆ†æ•¸
   - è¼ƒé«˜å·®ç•°åº¦è¡¨ç¤ºç³»çµ±èƒ½æ›´å¥½å€åˆ†ä½œå“è³ªé‡

ğŸ¯ **æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹**:
   æœ‰æ•ˆè²è­½è€…æ¯”ä¾‹ = (è²è­½ > 0.1çš„è©•åˆ†è€…æ•¸é‡) / ç¸½è©•åˆ†è€…æ•¸é‡
   
   - è²è­½ > 0.1 è¦–ç‚ºã€Œæœ‰æ•ˆã€è©•åˆ†è€…
   - ç†æƒ³æ¯”ä¾‹ç‚º60%-90%ï¼Œéä½è¡¨ç¤ºéåº¦æ‡²ç½°ï¼Œéé«˜è¡¨ç¤ºæ‡²ç½°ä¸è¶³
        """)

    def _display_detailed_results(self, grading_system, vg, metrics):
        """é¡¯ç¤ºè©³ç´°çš„å€‹åˆ¥å­¸ç”Ÿçµæœ"""
        self.print(f"\nğŸ“‹ vG = {vg:.2f} æ™‚å„å­¸ç”Ÿçš„è©³ç´°è¡¨ç¾:")
        self.print("-" * 80)
        
        # é¡¯ç¤ºå­¸ç”Ÿè²è­½è©³æƒ…
        self.print("ğŸ† å­¸ç”Ÿè²è­½æ’è¡Œ:")
        user_details = []
        for user in grading_system.users:
            if hasattr(user, 'reputation') and hasattr(user, 'variance'):
                user_details.append({
                    'name': user.name,
                    'reputation': user.reputation,
                    'variance': user.variance if user.variance is not None else 0,
                    'std_dev': np.sqrt(user.variance) if user.variance is not None else 0
                })
        
        # æŒ‰è²è­½æ’åº
        user_details.sort(key=lambda x: x['reputation'], reverse=True)
        
        self.print("   æ’å  å§“å     è²è­½    è®Šç•°æ•¸   æ¨™æº–å·®   è¨ˆç®—éç¨‹")
        self.print("   ----  ----     ----    -----   -----   --------")
        
        for i, user in enumerate(user_details[:10]):  # åªé¡¯ç¤ºå‰10å
            lambda_val = 1.0 / vg
            penalty = lambda_val * user['std_dev']
            reputation_calc = max(0, 1.0 - penalty)
            
            self.print(f"   {i+1:2d}.  {user['name']}   {user['reputation']:.3f}   {user['variance']:6.1f}   {user['std_dev']:5.2f}   "
                      f"max(0, 1.0 - {lambda_val:.3f}Ã—{user['std_dev']:.2f}) = {reputation_calc:.3f}")
        
        if len(user_details) > 10:
            self.print(f"   ... (é‚„æœ‰ {len(user_details)-10} ä½å­¸ç”Ÿ)")
        
        # é¡¯ç¤ºä½œå“æˆç¸¾è©³æƒ…
        self.print(f"\nğŸ“ ä½œå“æˆç¸¾åˆ†ä½ˆ:")
        item_grades = []
        for item in grading_system.items:
            if hasattr(item, 'grade') and item.grade is not None:
                item_grades.append({
                    'name': item.name,
                    'grade': item.grade
                })
        
        # æŒ‰æˆç¸¾æ’åº
        item_grades.sort(key=lambda x: x['grade'], reverse=True)
        
        self.print("   æ’å  ä½œå“åç¨±    æœ€çµ‚æˆç¸¾")
        self.print("   ----  --------    --------")
        
        for i, item in enumerate(item_grades[:10]):  # åªé¡¯ç¤ºå‰10å
            self.print(f"   {i+1:2d}.  {item['name']}      {item['grade']:6.2f}")
        
        if len(item_grades) > 10:
            self.print(f"   ... (é‚„æœ‰ {len(item_grades)-10} å€‹ä½œå“)")
        
        # è¨ˆç®—ä¸¦é¡¯ç¤ºæˆç¸¾å·®ç•°åº¦
        grades = [item['grade'] for item in item_grades]
        if grades:
            grade_mean = np.mean(grades)
            grade_std = np.std(grades)
            self.print(f"\n   æˆç¸¾çµ±è¨ˆ: å¹³å‡ = {grade_mean:.2f}, æ¨™æº–å·® = {grade_std:.2f}")
            grade_sample = [f'{g:.1f}' for g in grades[:5]]
            self.print(f"   æˆç¸¾å·®ç•°åº¦è¨ˆç®—: std([{', '.join(grade_sample)}...]) = {grade_std:.3f}")
    
    def plot_analysis_results(self, results, optimal_vg):
        """Create comprehensive visualization of vG analysis results in English"""
        # Import seaborn safely with fallback
        try:
            import seaborn as sns
            sns.set_style("whitegrid")
            plt.style.use('default')
        except ImportError:
            plt.style.use('default')
        
        # Configure matplotlib for better rendering
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.labelsize'] = 12
        plt.rcParams['axes.titlesize'] = 14
        plt.rcParams['legend.fontsize'] = 10
        plt.rcParams['figure.dpi'] = 100
        
        # Extract data from results
        vg_values = sorted(results.keys())
        metrics_data = {
            'rep_mean': [results[vg]['rep_mean'] for vg in vg_values],
            'rep_std': [results[vg]['rep_std'] for vg in vg_values],
            'effective_rep_ratio': [results[vg]['effective_rep_ratio'] for vg in vg_values],
            'grade_dispersion': [results[vg]['grade_dispersion'] for vg in vg_values],
            'rep_min': [results[vg]['rep_min'] for vg in vg_values],
            'rep_max': [results[vg]['rep_max'] for vg in vg_values]
        }
        
        # Calculate composite scores
        composite_scores = []
        for vg in vg_values:
            score = self._calculate_composite_score(results[vg])
            composite_scores.append(score)
        
        # Create the main figure with subplots
        fig = plt.figure(figsize=(16, 12))
        fig.suptitle('Vancouver Algorithm vG Parameter Optimization Analysis (15-Student Class Setup)', 
                     fontsize=14, fontweight='bold', y=0.98)
        
        # 1. Composite Score vs vG
        ax1 = plt.subplot(2, 3, 1)
        plt.plot(vg_values, composite_scores, 'b-', linewidth=2, marker='o', markersize=4)
        plt.axvline(x=optimal_vg, color='red', linestyle='--', alpha=0.7, 
                   label=f'Optimal vG = {optimal_vg:.2f}')
        optimal_score = self._calculate_composite_score(results[optimal_vg])
        plt.axhline(y=optimal_score, color='red', linestyle='--', alpha=0.7)
        plt.scatter([optimal_vg], [optimal_score], color='red', s=100, zorder=5)
        plt.xlabel('vG Value')
        plt.ylabel('Composite Score')
        plt.title('Overall System Performance')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # 2. Reputation Statistics
        ax2 = plt.subplot(2, 3, 2)
        plt.plot(vg_values, metrics_data['rep_mean'], 'g-', linewidth=2, marker='s', 
                markersize=4, label='Mean Reputation')
        plt.plot(vg_values, metrics_data['rep_std'], 'orange', linewidth=2, marker='^', 
                markersize=4, label='Reputation Std Dev')
        plt.axvline(x=optimal_vg, color='red', linestyle='--', alpha=0.7)
        plt.axhline(y=0.55, color='gray', linestyle=':', alpha=0.5, label='Ideal Mean (0.55)')
        plt.axhline(y=0.25, color='gray', linestyle=':', alpha=0.5, label='Ideal Std (0.25)')
        plt.xlabel('vG Value')
        plt.ylabel('Reputation Score')
        plt.title('Reputation Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. Effective Reputation Ratio
        ax3 = plt.subplot(2, 3, 3)
        effective_percentages = [ratio * 100 for ratio in metrics_data['effective_rep_ratio']]
        plt.plot(vg_values, effective_percentages, 'purple', linewidth=2, marker='D', markersize=4)
        plt.axvline(x=optimal_vg, color='red', linestyle='--', alpha=0.7)
        plt.axhspan(60, 90, alpha=0.2, color='green', label='Ideal Range (60%-90%)')
        plt.xlabel('vG Value')
        plt.ylabel('Effective Reputation Ratio (%)')
        plt.title('System Coverage')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 4. Lambda (Penalty Coefficient) vs vG
        ax4 = plt.subplot(2, 3, 4)
        lambda_values = [1.0/vg for vg in vg_values]
        plt.plot(vg_values, lambda_values, 'brown', linewidth=2, marker='x', markersize=6)
        plt.axvline(x=optimal_vg, color='red', linestyle='--', alpha=0.7)
        optimal_lambda = 1.0/optimal_vg
        plt.scatter([optimal_vg], [optimal_lambda], color='red', s=100, zorder=5)
        plt.xlabel('vG Value')
        plt.ylabel('Lambda (Penalty Coefficient)')
        plt.title('Penalty Mechanism Strength')
        plt.grid(True, alpha=0.3)
        
        # 5. Grade Dispersion
        ax5 = plt.subplot(2, 3, 5)
        plt.plot(vg_values, metrics_data['grade_dispersion'], 'teal', linewidth=2, marker='h', markersize=4)
        plt.axvline(x=optimal_vg, color='red', linestyle='--', alpha=0.7)
        plt.xlabel('vG Value')
        plt.ylabel('Grade Dispersion (Std Dev)')
        plt.title('Grade Discrimination Ability')
        plt.grid(True, alpha=0.3)
        
        # 6. Reputation Range
        ax6 = plt.subplot(2, 3, 6)
        plt.fill_between(vg_values, metrics_data['rep_min'], metrics_data['rep_max'], 
                        alpha=0.3, color='skyblue', label='Reputation Range')
        plt.plot(vg_values, metrics_data['rep_max'], 'b-', linewidth=1.5, label='Max Reputation')
        plt.plot(vg_values, metrics_data['rep_min'], 'b--', linewidth=1.5, label='Min Reputation')
        plt.axvline(x=optimal_vg, color='red', linestyle='--', alpha=0.7)
        plt.xlabel('vG Value')
        plt.ylabel('Reputation Score')
        plt.title('Reputation Score Range')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Save the plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"vg_analysis_visualization_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        
        self.print(f"\nğŸ“Š Visualization saved as: {filename}")
        plt.show()
        
        return filename
    
    def plot_detailed_comparison(self, results):
        """Create detailed comparison of key vG values"""
        # Key vG values to compare
        key_vg_values = [2.0, 4.0, 6.0, 8.0]
        available_keys = [vg for vg in key_vg_values if vg in results]
        
        if len(available_keys) < 2:
            self.print("Not enough data points for detailed comparison")
            return None
        
        # Extract metrics for comparison
        comparison_data = {}
        for vg in available_keys:
            comparison_data[vg] = results[vg]
        
        # Create comparison chart
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Detailed Comparison of Key vG Values', fontsize=14, fontweight='bold')
        
        # 1. Reputation Statistics Comparison
        vg_labels = [f'vG={vg}' for vg in available_keys]
        rep_means = [comparison_data[vg]['rep_mean'] for vg in available_keys]
        rep_stds = [comparison_data[vg]['rep_std'] for vg in available_keys]
        effective_ratios = [comparison_data[vg]['effective_rep_ratio'] * 100 for vg in available_keys]
        
        x_pos = np.arange(len(available_keys))
        width = 0.35
        
        ax1.bar(x_pos - width/2, rep_means, width, label='Mean Reputation', alpha=0.8)
        ax1.bar(x_pos + width/2, rep_stds, width, label='Reputation Std Dev', alpha=0.8)
        ax1.set_xlabel('vG Value')
        ax1.set_ylabel('Reputation Score')
        ax1.set_title('Reputation Statistics')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(vg_labels)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Effective Reputation Ratio
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
        bars = ax2.bar(vg_labels, effective_ratios, color=colors[:len(available_keys)], alpha=0.8)
        ax2.axhline(y=60, color='green', linestyle='--', alpha=0.7, label='Min Ideal (60%)')
        ax2.axhline(y=90, color='green', linestyle='--', alpha=0.7, label='Max Ideal (90%)')
        ax2.set_xlabel('vG Value')
        ax2.set_ylabel('Effective Reputation Ratio (%)')
        ax2.set_title('System Coverage Comparison')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, ratio in zip(bars, effective_ratios):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{ratio:.1f}%', ha='center', va='bottom')
        
        # 3. Penalty Coefficient (Lambda)
        lambda_values = [1.0/vg for vg in available_keys]
        ax3.plot(available_keys, lambda_values, 'ro-', linewidth=2, markersize=8)
        ax3.set_xlabel('vG Value')
        ax3.set_ylabel('Lambda (Penalty Coefficient)')
        ax3.set_title('Penalty Mechanism Strength')
        ax3.grid(True, alpha=0.3)
        
        # Add value labels
        for vg, lambda_val in zip(available_keys, lambda_values):
            ax3.annotate(f'{lambda_val:.3f}', (vg, lambda_val), 
                        textcoords="offset points", xytext=(0,10), ha='center')
        
        # 4. Composite Score Comparison
        composite_scores = [self._calculate_composite_score(comparison_data[vg]) for vg in available_keys]
        bars = ax4.bar(vg_labels, composite_scores, color=colors[:len(available_keys)], alpha=0.8)
        ax4.set_xlabel('vG Value')
        ax4.set_ylabel('Composite Score')
        ax4.set_title('Overall Performance Score')
        ax4.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, score in zip(bars, composite_scores):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # Save the plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"vg_detailed_comparison_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        
        self.print(f"ğŸ“Š Detailed comparison saved as: {filename}")
        plt.show()
        
        return filename

    def generate_comprehensive_analysis(self, include_plots=True):
        """Run complete analysis with optional plotting"""
        self.print("\nğŸš€ Starting Comprehensive vG Parameter Analysis")
        self.print("=" * 60)
        
        # First, check workload distribution
        self.workload_analysis()
        
        # Run theoretical analysis
        self.theoretical_analysis()
        
        # Find optimal vG with plotting
        optimal_vg, results = self.find_optimal_vg()
        
        if include_plots:
            self.print("\nğŸ“Š Generating Visualizations...")
            try:
                # Create main analysis plots
                main_plot = self.plot_analysis_results(results, optimal_vg)
                
                # Create detailed comparison
                detail_plot = self.plot_detailed_comparison(results)
                
                self.print(f"âœ… Analysis complete with visualizations")
                
            except Exception as e:
                self.print(f"âš ï¸ Plotting failed: {e}")
                self.print("Analysis data is still available in results")
        
        return optimal_vg, results

def main():
    """ä¸»å‡½æ•¸ - åŸºæ–¼èª¿æ•´å¾Œè®Šç•°æ•¸æ¨™æº–çš„vGåƒæ•¸åˆ†æ"""
    analyzer = VGAnalyzer()
    
    # é‹è¡Œå®Œæ•´çš„ç¶œåˆåˆ†æï¼ˆåŒ…å«è‹±æ–‡ç•«åœ–åŠŸèƒ½ï¼‰
    optimal_vg, results = analyzer.generate_comprehensive_analysis(include_plots=True)
    
    # ç”Ÿæˆå»ºè­°
    analyzer.generate_recommendations(optimal_vg)
    
    # æ›´æ–°é…ç½®å»ºè­°
    analyzer.print(f"\nğŸ”§ å»ºè­°æ›´æ–° StudentGradingConfig ä¸­çš„é…ç½®:")
    analyzer.print(f"   V_G_EXAM = {optimal_vg:.2f}  # æœ€ä½³åŒ–å¾Œçš„è€ƒè©¦èª¤å·®å®¹å¿ä¸Šé™")
    analyzer.print(f"   æ¯ä½å­¸ç”Ÿè©•æ”¹è€ƒå·æ•¸: {len(analyzer.test_data[0]) // len(analyzer.test_data[1])}  # ç¢ºä¿å·¥ä½œé‡å‡ç­‰")
    
    # å¯¦é©—ç¸½çµ
    analyzer.print(f"\nğŸ“‹ å¯¦é©—ç¸½çµ:")
    analyzer.print(f"   â€¢ åˆ†æäº†{len(analyzer.test_data[1])}ä½å­¸ç”Ÿçš„è©•åˆ†è¡Œç‚º")
    analyzer.print(f"   â€¢ æ¯ä½å­¸ç”Ÿè©•æ”¹{len(analyzer.test_data[0]) // len(analyzer.test_data[1])}ä»½è€ƒå·ï¼Œå·¥ä½œé‡å‡ç­‰")
    analyzer.print(f"   â€¢ ä½¿ç”¨èª¿æ•´å¾Œçš„è®Šç•°æ•¸æ¨™æº–ï¼Œç„¡éœ€é¡å¤–å®¹å¿åº¦æ©Ÿåˆ¶")
    analyzer.print(f"   â€¢ æ¨è–¦ä½¿ç”¨ vG = {optimal_vg:.2f}")
    analyzer.print(f"   â€¢ è®Šç•°æ•¸æ¨™æº–æ›´è²¼è¿‘å¯¦éš›è©•åˆ†æƒ…æ³")
    
    # é—œé–‰æ—¥å¿—è¨˜éŒ„å™¨
    analyzer.logger_print.close()

if __name__ == "__main__":
    main()
