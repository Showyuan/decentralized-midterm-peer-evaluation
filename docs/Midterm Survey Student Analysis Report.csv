name,id,sis_id,section,section_id,section_sis_id,submitted,attempt,"335725: For each of the following statements, answer Yes if it is TRUE or brief why it is wrong in one sentence.



(2 points) The less threading, the less performance.
(2 points) The less frames, the more page fault rates.
(2 points) The smaller time quantum for scheduling, the longer average turnaround time.
(2 points) Firmware can be executed faster in ROM than in RAM.
(2 points) Working set model has to be supported by MMU.
(2 points) A safe state will not go to a deadlocked state.
(2 points) Context switch is usually supported in hardware instruction for performance.
(2 points) Session Semantics: Once the file is closed, the changes are visible only to new sessions.
(2 points) A real-time scheduler schedules tasks according to their real-time priorities.
(2 points) The two-phase commit protocol guarantees serializability and prevents deadlock.


",20,"335726: For homework 2, the real-time RM/EDF scheduler simulation, it is not O(1) as the uC/OS-II, the ID is the priority. Since Linux has change from O(1) to Completely Fair Scheduler and then EEVDF, why not use the rea-time scheduler such as in uC/OS-II for Linux on real-time scheduling policy? Hints: how about many tasks with the same priority, a scheduler needs to run forever...",20,"335727: For homework 3, the chair competing problem, N Persons Competing M Chairs in a round, each person must win at least W times in R consecutive rounds. Please describe your solution, not necessarily with codes. The pseudo code is welcome. Hints: what would the output be for justice and fairness, close to reality: the chairs might be renewed (like concert tickets) in each round, people run differently in speed and might not need to wait for all others to start for next round, N and M might be very big...",20,"335728: OSes are emerging. They go diversified and embedded or specialized. For a distributed real-time OS supports trust as a blockchain does but would like to make the most out of system utilization by concurrency such as sharding, meaning parallelizing with data dependency, to solve efficiency problems such as many blockchain nodes (processors) generate only one block (task) in a period of time. To implement solutions of homework 3, the 大風吹 problem, for resource reservation in a blockchain platform supporting trust, synchronization monitor might be an easy mechanism. What difficulty is there in optimizing for performance using a synchronization monitor? Hints: serializability, linearizability, deadlock, scalability, explain with a sample system.",20,"335729: What system software or component can you design for your term project to optimize the resource utilization (hardware and software) in the project system? What value can your term project create? For whom? When and where?Write your proposal inLecture Noteslink if you have not done so and provide a date to present your idea in class. Hints: Define your most valuable system design, or resource utilization even it is a survey.",20,n correct,n incorrect,score
A,A01,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 15:59:59 UTC,1,"1.

No. 一個反例是：當 Thread 越多，多到超過 CPU Core 的數量，平均起來每個 CPU Core 會分到一個以上的 Thread，這會造成額外的 Context Switch 的壓力。即使程式能完全 100% 平行化，也不建議 Thread 數量超過 Physical CPU Count。

2.

No. 一個反例是 Belady's anomaly：當越多 frame 時反而會讓 page-fault rate 上升，在這種情況下較少的 frame 反而能減少 page-fault rate。

3

No. 雖然較小的 time quantum 需要額外付出 Context Switch Overhead，但若是 task 大多數是 IO-Bound 時，較小的 time quantum 反而可以透過更頻繁的 Context Switch 減少 average turnaround time。

4.

No. ROM 的速度比 RAM 慢，即使有 firmware 在 ROM 上執行，這也不是因為速度問題，而且絕大多數微控制器還是有 SRAM，firmware 不會直接在 ROM 上執行。

5.

Yes

6.

Yes

7.

No. Context switch 過程中，會需要軟體介入，包括將暫存器資料存回記憶體，以及讓 OS Scheduler 得知目前這隻程式運行狀態，讓 Scheduler 挑選下一個 Job 執行。

8.

Yes

9.

Yes

10.

Yes

 ",0,"首先，在 uC/OS-II 的 Scheduler 中，Task ID 被當作 Priority 使用，並且為了快速查表，Task ID 有數量上限。

O(1) Scheduler 一定會有一些限制存在，這裡我先以 uC/OS-II 的 Scheduler 舉例。

 

因為 Task ID 有數量上限，所以在 Linux 這種 general-purpose OS 上面，我們無法像是 RTOS 一樣預先知道 Task 有多少組，這就導致 uC/OS-II 的 Scheduler 無法使用。

這個會限制 Task ID 的 Scalability。

 

另外，因為 Task ID 必須是 Unique ID，所以不能有多個 Task share same priority。

它的影響會造成：當 process fork 的時候，我應該給他怎麼樣的 TaskID/Priority？Fork 出來的 Process 會比原本的 Process Priority 高或是低。

同時，這也帶來另一個問題：Task ID 不能任意被改變／新建，OS 不能無腦的 incremental task ID，要不然 Task ID 可能會超過 Priority 的值域，因此我們必須找一個當下沒有人在使用的 TaskID/Priority。

另外，為了避免 Starvation，我們通常會讓 Priority 隨著時間遞增，但是這在 uC/OS-II 的 Scheduler 中無法被實現：因為這意味著需要修改 TaskID。

 

另外，在 Server 等有 multi-user 的場景 (e.g., compute farm)，通常作業系統能夠協助實作 Resource Quota for each user，確保每個 user 可以公平的分配 Server 的 resource。

但是，在 uC/OS-II 的架構中，因為 Priority 是固定的，所以只要搶到 high-priority TaskID，就能總是優先搶贏其他 user。",0,"對於這一題，我會希望用更有 debug 也更簡單安全的方式實作：

題目的要求是每個人都必須要搶到 W 次權限，所以可以替每個人創造一個 priority 的屬性，值域為 [0~W]，代表這個人還有幾次權限可以搶。
這裡的 W 次數可以說是每個人的低消，因為存在 Round * Chair 遠大於 W * N 的可能性，所以有可能有些人可以搶到 W+1 次甚至更多次。

 

當椅子（資源）被釋放時，挑下一個人讓他坐椅子的邏輯則是：總是挑目前 waiting-queue 中 priority 最高的人。
但如果目前尚未到 N*W/M round，或是目前分發過的椅子數量仍小於 N*W 次，卻挑出 priority = 0 的人，我們不能將資源讓出來給 priority = 0 的人用，這樣子就可以保證「直到全部人都搶到 W 次椅子後，大家才會一起競爭多餘的椅子」，也就是justice。
不能有人先達到低消後，不管其他人還沒達到低消，就開始競爭椅子。

因為椅子在每一輪都有可能發生變動，且題目假設我們可以不用等全部椅子都結束才進入下一輪，所以我們可以賦予椅子一組 ID，ID 的範圍在 [0~M)。
並且，我們還需要記錄每一張椅子目前數到第幾輪了，以及椅子 ID 背後對應到的 ticket (以演唱會為例)。

High-level 軟體架構圖：

[image.png] (/users/231538/files/7411659/preview?verifier=75mYOlVrYu8D72DW5aWnGQq85IRx5j8tZgDnn2ch)

 

 

C++/C#-like Pseudo-code (部分指令假設程式語言支援 atomic 指令，由硬體保證 ordering，簡化 pseudo code 複雜度；Chair 相關的 function 只列出名字，並無實作細節。)

void Person_Handler(int personID)
{
    while (true)
    {
        auto chairID = Acquire_Chair(personID);
        DoSomething(chairID);
        ReleaseChair(personID, chairID);
    }    
}

// ===== Control Part =====
void Initialization(int nPerson, int mChair, int wMinRequest, int rRound)
{
    create mutex array with length = nPerson
    create priority array with length = nPerson, initValue = wMinRequest
    create priority queue as waiting queue
    create chair array with length = mChair
    create chair round counter array with length = chairRoundCounter
    availableChairCount = mChair
    signal(mutex_maintainer);
}

object Acquire_Chair(int personID)
{
    atomic_add_person_in_priority_queue(personID, priority[personID]);
    Maintainer();
    wait(mutex[personID]);
    atomic_add(availableChairCount, -1);
    auto chairID = atomic_get_available_chair_id();
    occupy Chair = chairID
    return chairID;
}

void ReleaseChair(personID, chairID)
{
    release Chair = chairID
    priority[personID] = max(0, priority[personID] - 1);
    if (++chairRoundCounter[chairID] < rRound)
        atomic_add(availableChairCount, 1);   
    Maintainer();
}

void Maintainer()
{
    wait(mutex_maintainer);
    if (chairRoundCounter[chairID].AllEqualTo(rRound))
        return CompetitionDone();
    while (atomic_compare_equal(availableChairCount, 0))
    {
        int max_priority = get_max_priority_from_priority_queue();
        bool isAllowPriorityZero = chairRoundCounter.Sum() >= N*W;
        if (!isAllowPriorityZero && max_priority == 0)
            break;
        int personID = dequeue_max_priority_from_priority_queue();
        signal(mutex[personID]);
    }    
    signal(mutex_maintainer);
}

 

這個做法的缺點在於：如果 Sum(chairs' Round) < N*W 時，只要有人還沒打完低消 W，其他打完低消的人都必須排隊等待。

在公平性上，這個做法的確保證每個人都先搶完 W 張椅子後，多出來的椅子再來公平競爭，但其實這樣做在效能上並不是很好。因為有可能有人搶椅子的速度比別人慢很多，這樣 bottleneck 會在他身上。

以 Performance 角度來說，應該只要替還沒打完低消的人預留椅子後，其他打完低消的人就可以提前競爭椅子，但是這樣公平性有待商榷：舉例來說，訂票系統中，有人不幸的被排到慢速的 Server (person)，而這個 Server (person) 只有 W 張票 (chair) 分給所有搶票的人，但可能有個特別強的 Server 搶到 W  + 100 張票 (chair) 分給所有搶票的人。",0,"首先，我先定義題目為：透過 sharding 的方式提升資料的併行性，並且透過 monitor 去確保資源的互斥性。

我們以搶票系統為例。透過 sharding 技巧，我們把票券切成很多個 block，Monitor 內有大量節點，而 monitor 就是要確保每個 block 不會被分配第二次，也就是雙重支付問題。

每個節點會向 monitor 要一個 block，block 內有很多的票券，當這個節點把 block 都分發完以後，就會透過重新到 monitor 要票。

透過分散式系統的概念，將搶票民眾分散在不同節點處理。

在這個場景底下，會有兩個比較難解決的問題。

一個是使用 monitor 的情況下，Monitor 本身的 Conditional variable 會決定讓哪些節點 access 資源，但是因為這是一個分散式系統，有可能這個節點在過程中故障，這樣資源會無法被釋放，導致整個搶票系統故障。

另一個問題是 scalability，搶票系統的節點可能因為人流增加，導致新的節點參與，但 monitor 內已經 pre-define 好節點的數量，所以沒有辦法做到 scalability。",0,"我的 Project [CPU Cache Optimization by OS Scheduling] (https://hackmd.io/YlX9XsG2QnuxqpCtD9cguA) 是為了提升 Cache 的 Utilization。

在 Multi-core CPU 架構中，通常會有一層 Last-level Cache shared among cores，這些 cache 會被這些 CPU core 瓜分。

但是，若是讓這些 task 之間能夠公平競爭 cache，並不會得到最佳的系統效能，因為系統上執行的程式對 Cache 的需求都不同。

 

簡單化分的話，IO-Bound 跟 Compute-Bound 的程式中，IO-Bound 因為是卡在 IO 上，所以對 cache 的需求不高，但是因為它會頻繁讀寫 memory，所以 IO 的資料是有可能造成 cache 污染的。

至於 Compute-bound，則要分為是 memory-intensive 還是 compute-intensive，如果是需要頻繁 access memory 的程式，它對 cache 有更高的需求，但如果是需要對少量資料做大量 operation 的程式則不需要大量的 cache。

因此，我希望可以提出一個方式，根據 task 的 cache-performance sensitivity，給每隻 process 不同的 Cache Priority，透過 CPU 的 Hardware 去做到 Cache 的 QoS。

 

For whom? => Personal mobile / laptop / desktop / workstation / Server ，只要是支援 Multi-tasking 的平台都可以適用。即使只有 Single-core 也能適用：能夠避免 context switch 後，新進的程式把前一隻程式的資料全部刷出 cache

When & Where => 當 Context switch 時做 sampling，分析程式的 cache performance 並指定 cache priority。這個是一個 General solution，可以讓全場景使用的機制，如果後續 apply 到 linux kernel 上，就是全部人都能適用的機制。",0,5,0,0
B,B02,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 15:59:59 UTC,2,"1. 不是。要看程式的平行程度，如果程式沒辦法做平行的話，效能應該會相當。

2. 是。

3. 不是。因為context switch是有成本的。

4. 不是。ROM的設計的主要目標是儲存資料。

5. 是。

6. 是。

7. 是。

8. 如果他為了避免race condition，又考量效能所以寫到buffer的話，是。

9. 會考量priority，但不是唯一考量，應該要讓所有task都meet deadline。

10. 不是。還是有可能兩個process彼此等待。",0,"* uC/OS-II 的 real-time scheduler 雖然 O(1) 很快，但限制也很多（固定優先權數、無法有同優先權衝突、無法處理大量變動 task）

* Linux 要處理太複雜的場景了，不能只靠 O(1) 的方式

* 如果你給 100 個 task 相同優先權，scheduler 選誰就要開始排隊、比 FIFO，變成 O(n)，甚至會餓死其他程序

* 所以 Linux 為了平衡「即時性、公平性、可擴展性」，選擇比較泛用的策略（像 CFS 和 EEVDF）

* 但如果真的要做 hard real-time 的任務，Linux 也提供 SCHED_FIFO / SCHED_RR 類似 uC/OS 的 real-time policy，只是要自己保證priority排列妥當、避免 starvation",0," 

初始化

win_count: 記錄每個人贏幾次的陣列

Priority: 陣列紀錄優先級

roundNum: 記錄當前回合數

 

迴圈

For each round in R:

    Randomly generate M chairs

    All eligible persons try to grab a chair

        - Faster persons have higher chance

        - Getting chair based on probability: P = 1 / piority

 

win_count[winners] += 1

priorities[winners] += 1",0,"假設我們有一個區塊鏈系統，它結合資源保留與共識機制。我們來模擬一個「大風吹」資源競爭場景：

* 系統中有 N 個節點（可以想成區塊鏈的參與者或 processor）。

* 每一輪會有 M 張椅子（表示可保留的區塊、任務、資源）。

* 每個節點想搶到一張椅子 → 這就像是「大風吹」遊戲。

* 一個 monitor 被用來保護對椅子的存取（用來配對資源）。

 

使用 Synchronization Monitor 解法

Monitor結構如下：

monitor ChairReservation {
Mutex lock;
Condition available;
Queue chairs;

 

function reserveChair(nodeID):

    lock.acquire()

    if chairs.isEmpty():

        available.wait()

    chair = chairs.dequeue()

    lock.release()

    return chair

 

function releaseChair(chairID):

    lock.acquire()

    chairs.enqueue(chairID)

    available.notify()

    lock.release()

}

每個節點執行 reserveChair() 嘗試搶資源。

 

 

Serializability：所有節點都要經過 monitor → 嚴重序列化瓶頸

* 即使 M 個椅子同時可用，Reserve 函數只能一個一個來（因為 lock）。

* 所有節點會在 Mutex 的 lock 上等待，造成排隊。

* 即使系統有 16 核處理器，最多還是一次一個節點進入 → 資源利用率低。

 

Linearizability：嚴格的一致性會限制分散式執行

* 如果我們要每一次領取椅子的順序被看到為一致（即使跨節點），那就需要 monitor 同步所有節點狀態。

* → 就需要 global lock 或網路共識（很慢）。

 

 Scalability：所有節點都集中使用這個 monitor → 延展性差

在區塊鏈上若有成百上千個節點同時參加搶椅子：

* 每一次資源保留都要進入相同 monitor → 中央瓶頸。

* Mutex 無法橫跨分散式節點 → 需用分散式鎖（如 Raft, Paxos，極慢）。

* 成千上萬的節點時，吞吐效能持續下降。

 

Deadlock:

設想：

* Node A 試圖同時搶兩張椅子 (椅子 L 和 R)。

* A 搶到 L，等 R；B 搶到 R，等 L。

 • → 兩個 monitor 都進入 wait → deadlock",0,,0,5,0,0
C,C03,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 15:59:59 UTC,1,"1. (2 points) The less threading, the less performance. Threading refers to creating
software threads.
No. There is a trade-off between the number of threads and the perceived
computing performance. If we use too few threads (e.g. less than the number of
logical threads), the computational resources are underutilized. But if we keep
increasing the number of threads, we may face race conditions, have very large
task queues, or system call blocking (many-to-one model).
2. (2 points) The less frames, the more page fault rates.
Yes. If we have few “free” or available frames in the main memory, it means that
only a few pages of a process are indeed in the main memory. Thus, the less
available frames the more likely is to have a page fault (try to access a page that is
not in memory but in secondary storage).
3. (2 points) The smaller time quantum for scheduling, the longer average turnaround
time.
No. It depends on the processes’ CPU burst times. Reducing the length of the time
quantum leads to a longer average turnaround time only when most processes
have CPU burst times shorter than the given time quantum.
4. (2 points) Firmware can be executed faster in ROM than in RAM.
No. Firmware is stored and executed using an EEPROM (slower than RAM), and
it is only used to handle low-level hardware tasks (booting). The main memory (RAM) is not
used to run firmware’s routines.
5. (2 points) Working set model has to be supported by MMU.
Yes. The working-set model aims to reduce page fault rate by assuming the
existence of process memory localities. It needs to get information from MMU
about which pages are active and how many frames are currently allocated to a
process.
6. (2 points) A safe state will not go to a deadlocked state.
Yes. By definition if a resource-allocation system stays in a safe state it avoids
deadlocks. Deadlocks may only occur if the system is in an unsafe state.
7. (2 points) Context switch is usually supported in hardware instruction for
performance.
Yes. Depending on the architecture, some instruction sets allow to store/load
multiple registers to/from the main memory in a single CPU instruction. This
speeds up context switch.
8. (2 points) Session Semantics: Once the file is closed, the changes are visible only
to new sessions.
Yes. According to the consistency semantics of OpenAFS.
9. (2 points) A real-time scheduler schedules tasks according to their real-time
priorities.
No. A real-time scheduler regards tasks’ deadlines or rate requirements (period) to
assign priorities. The assigned priorities are used in a priority-based scheduling algorithm.
10.(2 points) The two-phase commit protocol guarantees serializability and prevents
deadlock.
No. It just ensures that a transaction between processes is fully completed or not
(atomicity). It may generate blocking (similar to deadlock) if the coordinator
process crashes while the participant processes are waiting for the transaction
commit.",0,"The scheduler for Uc/os-II has up to 64 priorities (dynamically assigned). It was designed for
embedded systems with less than 64 tasks running concurrently. Accordingly, using uC/OS-II
limits the number of schedulable tasks and therefore other scheduling approach should be used
for RT systems running over 64 tasks concurrently. The hint is ambiguous, according to Uc/os-II
documentation, two running tasks cannot have the same priority. However, the same priority can
be assigned (reused) to tasks over time (creation, deletion) and possibly cause starvation to
older tasks that were assigned a lower priority.",0,"My rephrased problem:
• N: total number of people
• M: number of winners per round (chairs).
• R: total number of rounds
• W: Minimum number of wins per person.
• M winners are selected at each round selects
• Each winner must wait floor(M/N) rounds before competing again
Pseudocode:
spacing ← floor(N / M)
waiting ← array of length N initialized to 0 (it defines how many round a person has to wait
before competing)
win_count ← array of length N initialized to 0
for round in 0 to R-1:
eligible_people ← []
for person in 0 to N-1:
if waiting[person] == 0:
eligible_people.append(person)
selected_winners ← pick M people from eligible_people
for person in 0 to N-1:
if waiting[person] > 0:
waiting[person] ← waiting[person] - 1
for winner in selected_winners:
waiting[winner] ← spacing
win_count[winner] ← win_count[winner] + 1
// At the end of the algorithm, check that all the entries in win_count are at least W. If all the
entries are higher than W the algorithm succeeds.
By enforcing a spacing between wins of floor(M/N) rounds we ensure that at each person will
win at least W in R rounds. At the end. The win counter (win_count) is verified to see if the
algorithm succeed or failed.
Input: N M W R
Output: Succeed/Failed , Number of used rounds",0,,0,"Project: Emulation of an ARM-based Industrial Edge Controller Network
Components: QEMU Virtual Controller Instances, I/O Simulation Module (SPI, I2C), Resource
Orchestrator (host PC)
Monitoring Dashboard
Data Flow:
QEMU nodes send I/O traffic → I/O Module
Orchestrator reads node states → pauses/limits/resumes nodes
Dashboard collects CPU/memory/network usage → displays to user",0,5,0,0
D,D04,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 15:14:39 UTC,1,"* 太多 thread 會造成 context-switching overhead。如果 threads 之間爭奪共享資源，則會獲得瓶頸，像是 lock contention.

* Belady's anomaly. 使用 FIFO algorithm 作為缺頁置換策略時，分配的 frame 增多，但page fault ratio 反而提高

* 大部分時候，較小的 time quantum 會導致更長的 turnaround time，因為頻繁的 context-switching。

* Firmware 在 RAM 的執行時間比 ROM 快。像 ROM 裡常用的 Flash 記憶體（NAND/NOR），寫入時是透過 改變電晶體中的電荷狀態。RAM 只是「控制電流通不通」，不牽涉到物理材料變化

* 是。Working set 要求 MMU 跟踪和管理 page，以確定在使用哪些 page。

* 是。save state 保證該系統可以在某種程度上分配資源以避免 deadlock。

* context switch 通常由 software (OS) 處理，一旦用硬體處理 context switch，整個機制變得難以維護或調整。例如不同作業系統或版本之間，需求不一樣，而硬體是一旦設計就很難改。

* 是。在 session semantics 中，當檔案關閉後再次開啟時，才看得到上次寫入的東西。

* 是。real-time 根據任務的 priority 分配 CPU 時間，以滿足 timing constraints。

* two-phase commit protocol 可確保 distributed transactions 中的 atomic，但不能保證 serialization 或防止 deadlock。",0,"RTOS scheduler（例如UC/OS-II）：
Rate-Monotonic(RM) algorithm：RM 會以程序的 period 作為標準，固定程序的順序，時間越短的越優先處理。
Earliest-Deadline-First(EDF) algorithm：以目前的狀態為基準，選擇最接近 deadline 的程式先執行。
具有較高優先級（或較早截止日期）的 task 始終搶占低優先級的任務，以確保 deterministic latency。缺點是對 general computing 缺乏 fairness 和 scalability，例如 desktop，server。

Linux schedulers :
最佳化各種工作 interactive, batch, real-time 的 fairness 和 throughput。
CFS 使用 “virtual runtime” 分發 CPU 時間 (齊頭式平等)，EEVDF 則是 deadline-aware scheduling 來保持公平 (立足點平等)

對於 real-time 如果兩個任務共享優先級，則 scheduler 無法解決衝突，導致 nondeterministic behaviors 或 starvation

假設 1000 個任務全部具有相同的 priority

O(1) 查找變為 O(n)，因為

* 需要掃具有相同級 priority 的 task queu

* 每個任務必須獲得 time slice

* scheduling 開銷隨著更多 task 而增加

priority inversion 或 starvation 將會很容易發生

Linux 的設計強調 flexibility, scalability 和 fairness 而避免使用 RM/EDF 之類的 RTOS-style schedulers。對於 real-time 的需求，則有 PREEMPT_RT 或 dual-kernel。",0,"條件：

* N 個人.

* 每回合 M 張椅子

* R 個回合.

* 每個人至少需要在 R 回合中至少拿到 W 次椅子

每個人有一個 vtime[i]，表示累積「使用資源」的程度，即為反映成功過幾次。
使用一個 priority queue（按 vtime 排序），來選擇誰下一輪有機會先「搶」椅子。
優先分配椅子給 vtime[i] 最小的人（代表他之前搶得比較少）。

 

```

initialize N persons
initialize each person with:
    vtime[i] = 0            // 虛擬時間
    wins[i] = 0             // 搶到的椅子次數
    speed[i] = random or defined // 代表跑得快慢（priority）

for round in 1 to R:
    build min-heap queue sorted by vtime[i]
    
    for j in 1 to M:  // M 張椅子
        person = queue.pop()
        wins[person] += 1
        
        // update vtime
        delta = BASE_TIME / speed[person]
        vtime[person] += delta
        
        queue.push(person)

if any wins[i] < W:
    report unfair outcome
```",0,,0,"我們設計了一個分散式的共享使用者空間的檔案系統。該系統的儲存資源主要部署於伺服器端，為用戶端機器提供一個高速存取的共享檔案系統架構。此檔案系統以伺服器/客戶端模式運作，並在每台機器上部署相應的元件。我們進一步善用具備 RDMA 功能的網路介面卡，並採用 RDMA 協議進行資料與檔案請求的傳輸，克服傳統 TCP/IP 在頻寬、延遲及多層網路堆疊處理上所帶來的限制與負擔。

案實作了一套分散式檔案系統，將 lock 管理與檔案狀態追蹤集中在伺服器端進行。伺服器負責統一管理lock 與狀態資訊，客戶端僅需遵循協定操作，降低應用層的開發複雜度。

此解決方案適用的場景，例如：

需要高頻寬 I/O 與快速檔案存取的大數據分析；
追求低延遲以提升串流品質的影音串流與多媒體編碼；
仰賴高速存取大量資料集的 AI/ML 訓練工作負載；

本系統具備高度擴展性，支援在 RDMA 架構下運行多個伺服器與客戶端，能滿足大規模儲存與高吞吐量的需求。只要網路頻寬與 RDMA 網路介面卡資源充足，即可支援大型叢集或資料中心。",0,5,0,0
E,E05,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 14:54:32 UTC,1,"1. 不是。要看程式的平行程度，如果程式沒辦法做平行的話，效能應該會相當。

2. 是。

3. 不是。因為context switch是有成本的。

4. 不是。ROM的設計的主要目標是儲存資料。

5. 是。

6. 是。

7. 是。

8. 如果他為了避免race condition，又考量效能所以寫到buffer的話，是。

9. 會考量priority，但不是唯一考量，應該要讓所有task都meet deadline。

10. 不是。還是有可能彼此等待產生deadlock。",0,uC/OS-II是用在排程順序明確、task較少的環境。又因為Linux必須在不同硬體平台上運行，需要考量通用性的關係，所以在決定一樣priority的task的執行順序必須有效率且確保沒有starving的發生，而如果只使用RR的方式會造成一直做切換沒有效率，task有可能一直無法有效率地被運行。,0,"初始化

win_count: 記錄每個人贏幾次的陣列

Priority: 陣列紀錄優先級

roundNum: 記錄當前回合數

 

迴圈

For each round in R:

    Randomly generate M chairs

    All eligible persons try to grab a chair

        - Faster persons have higher chance

        - Getting chair based on probability: P = 1 / piority

 

win_count[winners] += 1

priorities[winners] += 1",0,"假設我們有一個區塊鏈系統，它結合資源保留與共識機制。我們來模擬一個「大風吹」資源競爭場景：

* 系統中有 N 個節點（可以想成區塊鏈的參與者或 processor）。

* 每一輪會有 M 張椅子（表示可保留的區塊、任務、資源）。

* 每個節點想搶到一張椅子 → 這就像是「大風吹」遊戲。

* 一個 monitor 被用來保護對椅子的存取（用來配對資源）。

 

使用 Synchronization Monitor 解法

Monitor結構如下：

monitor ChairReservation {
Mutex lock;
Condition available;
Queue chairs;

 

function reserveChair(nodeID):

    lock.acquire()

    if chairs.isEmpty():

        available.wait()

    chair = chairs.dequeue()

    lock.release()

    return chair

 

function releaseChair(chairID):

    lock.acquire()

    chairs.enqueue(chairID)

    available.notify()

    lock.release()

}

每個節點執行 reserveChair() 嘗試搶資源。

 

 

Serializability：所有節點都要經過 monitor → 嚴重序列化瓶頸

* 即使 M 個椅子同時可用，Reserve 函數只能一個一個來（因為 lock）。

* 所有節點會在 Mutex 的 lock 上等待，造成排隊。

* 即使系統有 16 核處理器，最多還是一次一個節點進入 → 資源利用率低。

 

Linearizability：嚴格的一致性會限制分散式執行

* 如果我們要每一次領取椅子的順序被看到為一致（即使跨節點），那就需要 monitor 同步所有節點狀態。

* → 就需要 global lock 或網路共識（很慢）。

 

 Scalability：所有節點都集中使用這個 monitor → 延展性差

在區塊鏈上若有成百上千個節點同時參加搶椅子：

* 每一次資源保留都要進入相同 monitor → 中央瓶頸。

* Mutex 無法橫跨分散式節點 → 需用分散式鎖（如 Raft, Paxos，極慢）。

* 成千上萬的節點時，吞吐效能持續下降。

 

Deadlock:

設想：

* Node A 試圖同時搶兩張椅子 (椅子 L 和 R)。

* A 搶到 L，等 R；B 搶到 R，等 L。

• • → 兩個 monitor 都進入 wait → deadlock！",0,我的project主旨是做職務代理人的分配及推薦系統，也就是在公司內負責業務的同仁暫時離開座位、休假等時，職務代理人可以獨立完成業務內容，不會因為負責人不在而業務停擺。這個project最重要的component是量化工作內容以及個人工作能力，因為需要衡量有無多的人力可以做這些業務內容，也需要衡量業務內容的多寡。再來是排程的方法，因為需要讓大家在休假的時間可以錯開，休假的時候職務代理人可以在場。這個系統預期的效益是讓公司分配人力安排提早做風險評估（員工離職、身體狀況不佳等）、員工休假時工作不會停擺等，可以讓公司的運作更有效率。 詳細project已放在網站中。,0,5,0,0
F,F06,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 14:37:36 UTC,1,"1. No, 有時候 thread 數量過多(e.g, > core 數目) 反倒造成對 core 競爭程度越大, context switch 次數增加, 還有 memory bus contention 增加
2. Yes, 一般而言，若分配給程式的 frame 數較少，其 working set 超出 frame 數目, page fault 越高, 還可能造成 thrashing
3. Yes, 較小的 time quantum 會導致更多 context switch，增加 overhead，從而延長 avg turnaround time
4. No, In most systems, firmware is stored in nonvolatile memory (e.g., ROM or Flash) on the device; however, for performance reasons, the firmware is typically copied into RAM for execution because RAM has a much faster access speed.
5. No, working set model 是 OS 的管理策略，可藉由 software 追蹤 page reference state 實作，不必完全依賴 MMU 的硬體支援。
6. Yes, safe state 意味著存在一個能讓所有 processes 依照順序完成的資源分配順序，因此不會導致 deadlock, 而不在 safe state 不代表一定會 deadlock
7. Yes, hardware instruction 可減少 Context switch overhead 像是使用快速的 load store instruction 來 push/pop stack from/to registers
8. Yes, Under session semantics, once a file is closed, the changes become visible only to sessions that open the file afterward.
9. Yes, A real-time scheduler prioritizes tasks based on their real-time priorities, ensuring that timing requirements are met.
10. No, The two-phase commit protocol guarantees atomic commitment but does not ensure serializability nor prevent deadlock.",0,"uC/OS-II（以及類似的小型 RTOS）裡常用位元遮罩（bitmask）來管理每個可能的優先權（由任務 ID 來代表），使得「找出最高優先權任務」是個 O(1) 的動作。 前提是：整個系統有明確、固定數量的優先權，且每個任務都有獨特優先權。uC/OS-II 基於「每個任務一個唯一優先權」的假設，通常很少考慮「多個任務同樣優先權」的情況。如果真的有同樣優先權（相同 ID），必須開啟 Round Robin 等機制，但預設情況下並不鼓勵多任務共用優先權。而且，uC/OS-II 典型應用在小型 MCU、工控裝置、車用系統等，其任務數量相對不大，而且通常已在設計階段定義好優先權，不太會動態新增/移除大量任務。
Linux 為什麼不直接用這種做法？

General Purpose OS 的需求複雜度更、高使用者多、任務龐雜：Linux 不是只跑幾個週期性工作，而是要同時執行各種使用者程式、系統服務、網路應用等等，數量層級可能遠超過一百隻、甚至幾千上萬隻任務。
需要兼顧公平性 (Fairness)：除了即時性，Linux 還要考慮不同使用者與行程間的公平分享 CPU。uC/OS-II 那種「最高優先權獨大到跑完」的概念，會造成一般使用者程式 starvation。
時間複雜度的權衡：uC/OS-II 倚賴固定長度的 bitmask 才能 O(1)。Linux 面對大量行程，就算可以維持 O(1) 也可能需要複雜的多層級資料結構，實務上 CFS 與 EEVDF 更能動態控管 CPU 分配與負載平衡。
同優先權任務衝突： 在 Linux 中，同一個排程類別（如 SCHED_OTHER、SCHED_FIFO、SCHED_RR），可以有多個行程擁有相同優先權。uC/OS-II 的預設做法是「每個任務給一個唯一優先權」，但這在 Linux 常不切實際，因為用戶可能想同時跑多個高優先權 RT 任務，大家優先權相同。如果用 uC/OS-II 的模式，等於要為每個任務都分配「不同優先權 ID」，但當任務數量大於優先權域（通常 RTOS 限制 256 甚至更少）就沒辦法了。
Scheduler 需要長期穩定運行： uC/OS-II 的設計環境裡，排程情況比較「可預期」，也可能不需要像桌面 / 伺服器一樣長時間運作且不停動態生成/銷毀行程。Linux 需要面對連續數天、數月甚至數年的長期運行，同時有大量行程動態上線、下線，需要更彈性的排程法。

總之，uC/OS-II 或小型 RTOS 追求的是「嚴格即時性」與「小巧」；Linux 作為通用型作業系統，需要平衡許多面向，包括公平性、可擴充性、大量行程管理。",0,"[aos.png] (/users/180373/files/7411115/preview?verifier=aiZ8GtXCDZY3qVpWyFvPPFqZldyRoAjQRvGLkGrH)滑動視窗(Sliding Window): 由於需要在「任何連續 R 輪」中保證每個人至少贏 W 次，我們可以對每個人維護一個長度為 R 的「勝負紀錄」，每輪比賽結束，就把這輪的結果（贏或輸）更新到紀錄裡，並把最舊的一筆移除 (或用環狀陣列進行覆蓋)，透過紀錄來判定這個人最近 R 輪的勝利次數。
優先分配(Priority Allocation): 如果某人在最近 R 輪中獲勝次數不足 W，表示他正面臨「低於最低門檻」的窘境，應該在下一輪比賽時得到更高的「優先權」，讓他有更大機率搶到椅子。反之，如果某人在最近 R 輪內已遠超 W 次勝利，則在當輪分配時可以稍微讓出機會給別人，確保整體的公平度。
非同步(Asynchronous): 並不是等到「所有人都準備好」才一同更新，也就是說並不是一次同時更新（往上滑動一格）所有 column，而是一次只將某幾個 column 往上滑動一格，也就是說，只要某些人結束他自己的第 k 輪，就立刻更新其滑動視窗；當他要開始第 (k+1) 輪時，系統就根據他的最新狀態（近期贏的次數）來調整優先權，這樣做可以保證「誰先完成就先進入下一輪，誰慢就稍晚進入」，因為若 N、M 非常大，或者有人比較快、有人比較慢，等到所有人都準備好的話雖然可以做較精準的決策，但是等待時間比較長，造成效率下降。
大規模 N、M: 當 N、M 非常大，關鍵就在於我們分配機制的演算法效率，所以以下提供一個以紅黑樹（Red-Black Tree）實現選拔機制，利用紅黑樹快速挑選出優先權最高的競爭者，每位競爭者的優先權可以依據其最近 R 輪的勝場數和勝場數要求 W 動態計算，定義一個數值：priority = W − win_count，這樣 win_count 較少者（也就是需要補票的候選人）會得到較高的 priority 值，進而在選拔時能夠較早被挑中。紅黑樹具有維持 O(log N) 的插入、刪除與搜尋時間的優點。",0,"Difficulties of Using a Synchronization Monitor in a Distributed Real-Time OS for Blockchain:
1. Serializability and Linearizability:
In a distributed environment, a monitor usually provides mutual exclusion at the local level. However, to guarantee serializability or linearizability across multiple nodes (or shards), you must coordinate state updates globally. Relying solely on local monitors may force you into using a single “global lock,” which severely limits concurrency and can lead to performance degradation.
2. Deadlock Risk:
When multiple monitors are used to protect different parts of the system (e.g., one monitor per shard), the potential for deadlocks increases. For example, if a transaction needs to lock resources in both Shard A and Shard B, improper lock ordering or contention among distributed nodes may easily cause circular wait conditions, resulting in deadlock.
3. Scalability Constraints:
Synchronization monitors can become a bottleneck in high-concurrency systems. If every resource reservation or state update is serialized through a monitor, the system cannot fully leverage parallelism. This is especially problematic in large-scale blockchain platforms where many processors must generate tasks concurrently—using monitors may drastically reduce overall system utilization.
4. Example Scenario:
Consider a multi-shard blockchain system: if each shard uses its own monitor but a cross-shard transaction requires acquiring monitors on multiple shards, you are at risk of either deadlock or significant performance penalties. The monitor might force operations to be executed in a serialized manner rather than exploiting the available parallelism.
Approaches to Overcoming the Performance Challenges with Synchronization Monitors:
1. Fine-Grained Locking:
Instead of using a single global monitor, partition the system into smaller, fine-grained locks (or monitors) assigned per shard or even per resource. Enforce a strict lock ordering to minimize the risk of deadlock while allowing more concurrent operations across independent components.
2. Optimistic Concurrency Control / MVCC:
Implement techniques like Multi-Version Concurrency Control (MVCC) or optimistic concurrency control. These methods allow most operations to proceed without blocking, resolving conflicts only at commit time. This can reduce the reliance on locks provided by monitors, thereby enhancing parallelism.
3. Distributed Coordination Primitives:
Use advanced distributed coordination mechanisms such as distributed locks, consensus protocols (e.g., Paxos, Raft), or even blockchain-specific consensus algorithms to ensure that state changes across nodes remain consistent without relying on a single centralized monitor.
4. Timeouts and Deadlock Detection:
Integrate timeout mechanisms and deadlock detection/recovery strategies into your monitor implementation. By detecting potential deadlock situations early, the system can roll back or abort transactions, thus maintaining system liveness and ensuring real-time responsiveness.
5. Event-Driven or Asynchronous Designs:
Where possible, adopt asynchronous or event-driven programming models. This reduces the blocking nature of monitors and allows the system to continue processing other tasks while waiting for a resource reservation to complete.",0,"[image.png] (/users/180373/files/7411647/preview?verifier=rqOjgsjo7g0Lr14Fwyrt6o7vZG39MLHpJjPsMDFE)

1. What system software or component can you design for your term project to optimize the resource utilization (hardware and software) in the project system?
We have designed a distributed and high-performance shared user-space file system. The file system's storage primarily resides on server machines, providing client machines with a fast-access shared file system architecture. The file system daemon we implement follows a server/client model, with components running on each machine. Additionally, we leverage existing RDMA-enabled network interface cards and use the RDMA protocol for data and file request transmissions, overcoming the limitations of traditional TCP/IP such as low bandwidth, higher latency, and the overhead introduced by traversing multiple layers of the network stack.
2. What value can your term project create? For whom?
Leveraging the high bandwidth capabilities of RDMA and a distributed file system architecture, our design ensures excellent data transfer speed and low latency even when multiple processes or machines access the system concurrently. This project provides a complete distributed file system protocol, where lock management and file state tracking are centralized on the server side. The server is responsible for managing locks and states, while clients simply follow the protocol, significantly reducing the development complexity at the application layer. This solution is ideal for high-performance scenarios such as big data analytics, which require high-bandwidth I/O and rapid file access; video streaming and multimedia encoding, where reduced latency improves stream quality; AI/ML training workloads that demand fast access to large datasets; and financial trading systems, where microsecond-level latency can make a crucial difference. The system is highly scalable, supporting multiple servers and clients under the RDMA architecture to meet the needs of large storage and high throughput. As long as sufficient network bandwidth and RDMA NICs are available, it can support large-scale clusters or enterprise-grade data centers. Additionally, being a user-space file system, it offers greater flexibility, ease of development, and maintainability compared to kernel-space implementations, allowing quicker updates and reduced development overhead. Centralized file management through one or more servers enables clients to access the same files and directories concurrently, providing high-speed, real-time sharing while minimizing data inconsistency risks and supporting centralized backup, fault tolerance, and other administrative features.

3. When and where?
In environments involving large-scale data access or high concurrency, our RDMA-based distributed file system excels in maintaining performance and efficiency. For AI and machine learning training, where rapid access to massive datasets is essential, this architecture significantly reduces data preprocessing and loading times, thereby accelerating the overall model training pipeline. In data centers and big data platforms, where multiple nodes frequently access shared databases simultaneously, the system must handle high levels of concurrency and intense I/O workloads. Traditional file systems often become bottlenecks under such conditions, whereas RDMA-enabled systems sustain stable performance even in high-load scenarios, making them ideal for modern, data-intensive applications.",0,5,0,0
G,G07,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 14:00:49 UTC,1,"* 否。過多的執行緒可能造成資源競爭，從而導致效能降低。

* 否。需要視程式實際的存取模式而定。

* 是。

* 否。RAM的存取速度更快，因此執行韌體也更快。

* 是。

* 是。

* 否。Context Switch主要是夠過軟體來實作。

* 否。新Session和既有的Session都能看到Session Semantics。

* 是。

* 否。2PC無法保證Serializability和防止Deadlock。",0,μC/OS-II 是一款常見於嵌入式系統的即時作業系統，具有輕量、可預測的排程特性。在這類系統中，任務數量通常較少，且多數任務具有固定且不同的優先權，因此較少出現相同優先權導致排程衝突的情形。相較之下，Linux 作業系統必須同時處理大量即時與非即時任務，情境更為複雜。若僅依據靜態優先權進行排程，當有多個執行緒擁有相同優先權時，系統可能必須不斷掃描整個執行緒佇列（run queue），導致排程器延遲增加，甚至影響整體系統效能。為了解決這個問題，Linux 採用了如 CFS（Completely Fair Scheduler） 與近期導入的 EEVDF（Earliest Eligible Virtual Deadline First） 等更通用且動態的排程演算法。這些 scheduler 透過考量公平性、延遲控制與可擴展性，能夠更有效地管理各類型任務，確保系統在高負載或多任務環境下仍維持良好的效能與穩定性。,0,"假設每個人有不同的搶椅子速度，我們在每回合根據玩家的速度來決定誰先搶到椅子，並優先讓最快的玩家中選。若有玩家距離目標次數差距較大，就暫時提升他的速度，讓他有更高機會搶到椅子。

// 初始化系統參數

N = 總玩家數

M = 每輪椅子的數量

R = 總回合數

W = 每位玩家至少需要搶到的椅子次數

for i in 1 to N:

    speed[i] = 隨機設定或根據真實情況決定

    wins[i] = 0  // 初始化搶椅子次數

 

// 開始進行 R 回合模擬

for round in 1 to R:

    // 找出尚未達標的玩家

    active_players = []

    for i in 1 to N:

        if wins[i] < W:

            active_players.append(i)

    // 模擬搶椅子動作：依個人速度生成反應時間

    waitlist = []  // 儲存(玩家ID, 反應時間)

    for person in active_players:

        reaction_time = rand() / speed[person]  // 速度越快的人反應可能越快

        waitlist.append((person, reaction_time))

    // 依反應時間排序，決定搶到椅子的前 M 名

    sort waitlist by reaction_time ascending

    for i in 0 to min(M-1, len(waitlist)-1):

        winner_id = waitlist[i].person

        wins[winner_id] += 1  // 該玩家成功搶到椅子

    // 公平性補償：判斷是否有玩家來不及達標

    for i in 1 to N:

        if R - round < W - wins[i]:  // 當回合剩餘數不足以達標時

            speed[i] *= 1.5  // 增加其速度以提升中選機率",0,"Monitor是一種結合 條件變數（Condition Variable） 與 互斥鎖（Mutex） 的同步機制，透過封裝對共享資源的存取流程，確保在任一時刻只有一個執行緒（thread）能夠操作該資源。在區塊鏈系統中，若需實作資源保留機制，Monitor 是一種相對容易的解決方案。然而在實際應用上，仍會面臨以下問題：

* 可序列化（Serializability）：Monitor 雖然能有效避免競爭條件（race condition），但也會抑制執行緒的並行程度（concurrency），造成效能瓶頸。例如，在某區塊鏈節點中，若所有交易狀態更新都必須透過同一個 Monitor 處理，就會限制系統的處理吞吐量。

* 線性一致性（Linearizability）：當共享資源分別由不同 Monitor 管理時，難以保證操作事件的順序符合線性一致性的要求，可能造成不同節點觀察到不一致的狀態。例如某筆交易需同時更新帳戶 A 與帳戶 B，各自受不同 Monitor 控制，若節點甲先鎖 A 再鎖 B，節點乙反過來操作，就可能導致狀態順序不一致，進而影響共識結果。

* 死結（Deadlock）：若執行緒在執行複合操作時依序鎖定多個資源，容易出現環狀等待（circular wait），導致系統陷入死結。例如從資金池 A 提領資產再轉入資金池 B，若多個節點同時執行相似操作但鎖定順序不同，便可能發生互鎖，造成交易卡住無法完成。

* 可擴展性（Scalability）：隨著節點數與資源數快速增加，Monitor 造成的鎖競爭與等待時間將成為限制系統效能的主要瓶頸。例如有 1000 個節點同時參與 NFT 鑄造，但鑄造程序受限於單一 Monitor 控制，導致每次存取都需排隊，難以滿足情境下的即時處理需求。",0,在大專院校的設計學群科系中，專案導向的課程設計相當常見。為了支援同學們的日常學習與製作，系所會建置一定數量的硬體設備與軟體；然而仍時常會見到軟硬體資源使用的競爭與浪費問題。本專案設計一套跨領域協作資源管理平台，目的在於優化教室中有限的硬體（如電腦、VR設備、3D列印機）和專業軟體資源使用效率。透過智慧排程、即時設備狀態監控，以及專案任務分工管理，促進系上不同課程的師生進行溝通。其價值在於提升學生資源使用效率、改善課程品質，適用於課程期間及期末專案製作階段，受益對象為修習課程的學生及授課教師。本專案已於Lecture Notes建立連結，預計於5月21日進行課堂簡報發表。,0,5,0,0
H,H08,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 13:40:57 UTC,4,"* No. If there are lots of critical sections, more threading may lead to lower performance due to the synchronization.

* No. If the address sequence is 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 and FIFO algorithm is applied, when frame sizes are set to 3 and 4, the page faults become 9 (the last 1, 2, 5 are hit) and 10 (only the second 1 and 2 are hit), respectively.

* No. In the case where both process A and process B require 4 time units, setting quantum times to 2 and 3 gives the average turnaround times (6+8)/2=7 (AABBAABB) and (7+8)/2=7.5 (AAABBBAB), respectively.

* No. A firmware is copied to and run in RAM to gain a better performance.

* Yes.

* Yes.

* No. As hardware-supported context switches lack flexibility, software-supported ones are used in Windows and Linux.

* No. A growing file can be read with some commands (e.g., tail -f) without changing the file descriptor.

* Yes.

* Yes.",0,"Unlike completely fair schedulers (CFS) and EEVDF schedulers, an O(1) scheduler doesn't maintain virtual run time and has to determine the priority in a stricter way. Linux, as a general-purpose OS, allows many interactive processes to be executed on it. For an interactive process P awaiting user's input, if the priority of P is set to be high, chances are the other background processes obtain few resources; if the priority of P is set to be low, the user may experience lagging.",0,"Assume N > M and NW <= MR. At first glance the problem seems to be solved by simply initializing Q[1..N] to be all W and then decrement Q[i] by 1 when person i gets seated, and person i is allowed to get her chair if Q[i]>0 or Q[j]<=0 for all j = 1, 2, ..., N. But it doesn't work when 3 persons compete for 2 chairs in 3 rounds and each person has to be seated in at least 2 rounds, since person 1 and person 2 are allowed to occupy the 2 chairs in round 1 and round 2. But the algorithm can be slightly modified to work if we consider Q[i] the priority of person i getting seated in the next round. This is guaranteed to work since the maximum of |Q[i]-Q[j]| doesn't exceed 1 during the procedure.

Suppose N, M, W, and R are global variables or constants. As Q[1..N] may be too large to be stored at the center, we let person i herself keep Q[i] so as to simplify the center. To demonstrate how the algorithm works, we first define struct Center as follows:
struct Center{
   int r=1, v=M, p=W, c=N;
   int request(int qi, int ki);
};
Here

* Member r represents the current round number.

* Member v represents the number of vacant chairs in the current round.

* Member p represents the maximum of Q[1..N].

* Member c represents the number of i satisfying Q[i]=p.

* Method request(int qi, int ki) is fed with Q[i] and K[i], where K[i] is initially 0. If the request succeeds, the round number (1~R) is returned, and K[i] should be replaced by the return value; otherwise, 0 is returned.

The center, center, is a global instance of Center, and each person has access to it. A lawful (i.e., no fake Q[i] or K[i] is passed) person may work like this:
void person_work(){
   int qi = W, ki = 0;
   while(1){
      int r = center.request(qi, ki);
      if(r > 0){ // seated in round r
         // do something when seated
         --qi; ki = r;
      }else{ // not seated
         // do something when not seated
      }
   }
}
If all persons are lawful, the method Center::request(int qi, int ki) can be implemented like this:
// Note that qi = p, p-1, or p-2 in our assumption
int Center::request(int qi, int ki){
   acquire mutex lock
   if(r>R // we've finished all R rounds
      || qi==p-2 // low priority
      || qi==p-1 && (v<=c // low priority
         || ki==r // person i has occupied a chair in this round
   )){
      release mutex lock
      return 0;
   }
   if(qi == p){
      --v; --c;
   }else{ // qi=p-1, v>c, and person i doesn't occupy any chair
      --v;
   }
   int res = r;
   if(v == 0){ // the last chair is occupied, go to the next round
      if(c == 0){ // update the maximum of Q[1..N]
         --p; c = N-r*M%N;
      }
      ++r;
   }
   release mutex lock
   return res;
}",0,"We solve the problem with the following monitor.
monitor CenterRequest{
   int r[N], c;
   condition x, y, z;
   void request(int i){
      ++r[i];
      x.wait();
      if(++c == n){
         z.wait(r[i]*N+i);
         y.signal();
      }
      x.signal();
      y.wait(r[i]*N+i);
      y.signal();
      x.wait();
      if(--c == 0){
         y.wait(r[i]*N+i);
         z.signal();
      }
      x.signal();
      z.wait(r[i]*N+i);
      z.signal();
   }
   void initialization_code(){
      for(int i=1; i<=N; ++i){
         r[i] = 0;
      }
      c = 0;
      x = Semaphore(1);
      y = Semaphore(0);
      z = Semaphore(1);
   }
}
To avoid starvation, an additional argument is passed to wait(). Note that an array of length N is stored in this case, which is unfavorable from the aspect of scalability.",0,"I'm going to investigate several kinds of scheduling algorithms on distributed systems in my term project. As there are still some unsolved scheduling problems in grid computing environments, my term project may enhance the performance in distributed computing.",0,5,0,0
I,I09,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 13:03:29 UTC,2,"1. No. Creating threads requires allocating resousrces, if the tasks very simple, than using multiple threads for them may be less efficient. Also, Under multithread scenario, some synchronization mechanisms, such as lock or barrier, might be requried, which might lead to performance drop.

2. TRUE.

3. TRUE.

4. No. Executing on RAM would be significantly faster.

5. TRUE.

6. TRUE.

7. No. It's usually done in software by OS. A single context switch is usually very fast and almost requires no time, so it won't make much sense to support it in hardware.

8. TRUE.

9. TRUE.

10. No. Two-pahse commit protocol indeed guarantees serializability, but it doesn't prevent deadlock.",0,"For the real-time scheduler in uC/OS-II, each task must be assigned a unique priority, which can be achieved through taking task ID as task priority. This is also how uC/OS-II assigns priority to tasks. This design may be acceptable for a simple real-time OS like uC/OS-II, but it is unsuitable for a general-purpose OS kernel like Linux.

Assigning unique priority for each task means that it is impossible to have tasks of the same priority, and since Linux schedules on threads, it also means that all threads need to have unique priority, this might not be desired for some multi-threaded programs. 

Furthermore, task priority in Linux ranges from 0 (high) to 139 (low), where 0~99 are reserved for real-time tasks, and 100~139 are for normal tasks. With each real-time task being assigned a unique priority, Linux can manage up to only 100 real-time tasks, which also significantly restricts the capability of Linux kernel.

Currently, Linux real-time policies include SCHED_FIFO and SCHED_RR, standing for first-in-first-out and round-robin, respectively, and since v3.14, it also added SCHED_DEADLINE, which is implemented using EDF (Early Deadline First) conjunction with CBS (Constant Bandwidth Server), for deadline scheduling. Those might be better solutions for a general purpose kernel.",0,"Our solution is presented below:

First of all, we check if [LaTeX: W \leq R] (/equation_images/W%2520%255Cleq%2520R?scale=1) and [LaTeX: R \times M \geq N \times W] (/equation_images/R%2520%255Ctimes%2520M%2520%255Cgeq%2520N%2520%255Ctimes%2520W?scale=1); otherwise, it is impossible to ensure each person win at least W times.

Since each person need to win at least W times, we first define the ""danger"" state: A person is in danger state if

[LaTeX: \left(W - the\ person's\ accumulated\ win\ times\right) = the\ number\ of\ remained\ rounds] (/equation_images/%255Cleft(W%2520-%2520the%255C%2520person's%255C%2520accumulated%255C%2520win%255C%2520times%255Cright)%2520%253D%2520the%255C%2520number%255C%2520of%255C%2520remained%255C%2520rounds?scale=1). A person is in danger state at the end of a round infers that the person must win in each of the following rounds.

Our strategy for competitions is, In each round, only the people that are in danger state can complete for the chairs at first, and then, only the people who haven't won W times and haven't acquired a chair in this round compete for the chairs. Finally, the people who haven't won a chair in this round compete for the chairs. At the end of each round, each person check if it is in danger state.

Our code is presented here: [Musical Chairs Problem] (https://hackmd.io/@VJqhl4IzToG8p2J9WXnPog/Bk2pyKQ01e)",0,"Synchronization is usually needed to guarantee some properties in multitasking, such as data consistency, serializability, or a stronger condition, linearizability. This usually involves mutual exclusion on some operations, that is, some tasks might need to wait another task to complete some operations, and a task cannot progress while being waiting. These waits can be very long and cause performance penalty. If we want to guarantee stronger properties between tasks, then the need of these waits might increase and the degree of concurrency might decrease, which leads to poor performance. All these problems can cause more significant impact when the system scales up. 

It is hard to eliminate those waits while synchronizing between tasks, as we may create a bug that is hard to detect and may not be able to guarantee some properties otherwise. Although the issues about using wait and signal operation correctly can be solved by using a monitor, the waits may still very long but necessary, making it hard to optimize performance for programs.",0,,0,5,0,0
J,J10,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 10:22:02 UTC,1,"* No. Performance depends on workload optimization; too few or too many threads can degrade performance.

* Yes.

* Yes.

* No. RAM generally provides faster access times than ROM, which is why systems often copy firmware from ROM to RAM for execution.

* No. The working set model is a memory management algorithm implemented at the OS level, not a hardware feature requiring MMU support.

* Yes.

* No. While hardware support exists in some processors, mainstream operating systems primarily implement context switching in software for better control and efficiency.

* No. With session semantics, once a file is closed, changes are flushed to the server and become visible to all clients that subsequently open the file.

* Yes.

* No. The two-phase commit protocol ensures atomicity of distributed transactions but doesn't inherently guarantee serializability or prevent deadlocks.",0,Linux 並未使用 uC/OS-II 的即時排程器，因兩者在系統設計與使用情境上的根本差異。uC/OS-II 的設計重點在為資源受限的嵌入式系統提供確定性的硬即時支援，其排程策略使用靜態優先級與固定任務集，強調最壞情況下的可預測性與極小的排程延遲，適合微控制器等單一功能設備。相對地，Linux 作為一個通用作業系統，其排程器必須處理桌面應用、伺服器工作與多媒體等混合負載，因此更重視公平性與動態調整能力。Linux 使用如 CFS 與 EEVDF 等演算法，利用虛擬運行時間與紅黑樹結構實現長期公平與延遲敏感任務的平衡，並配合多核架構與虛擬化技術進行排程域與負載追蹤管理，這些設計在 uC/OS-II 的簡化模型中較難實現。此外，Linux 透過 PREEMPT_RT 等機制雖可支援軟即時應用，但其上下文切換與系統呼叫的成本還是無法滿足硬即時應用的要求。在實務上，uC/OS-II 適合靜態、可預測的小型系統，而 Linux 則針對動態變化與高並行環境進行優化，兩者的排程器設計目標本質上無法兼容。,0,"首先會檢查 M 與 N 的比例是否滿足 M/N ≥ W/R，不滿足則立即回報 NO_WAY。接著每個參與者會透過一個長度為 R 的滑動窗口來記錄自己的勝場，並根據最近 R 輪的勝場總和來計算出自己還需要多少勝場，W 減去目前勝場數。此差值與速度因子結合後，形成一個動態優先級，讓需要彌補勝場較多的人在競爭中有更高的優先權。實作上利用全域變數來儲存剩餘椅子數和目前的輪次，並透過一個 semaphore mutex 保護共享資源，同時使用一個屏障 round_barrier 來同步所有人進入競爭區。在每輪開始時，所有人先透過 barrier_wait 同步，接著依據自己的 current_wins 與計算出的 required 值進行椅子分配：若需要補足的勝場數大於零，則在鎖住 mutex 下檢查是否有足夠的椅子可供扣除，並根據該值更新勝場紀錄；反之以單個椅子的方式競爭剩餘資源。當輪次結束後，再透過另一個屏障等待所有人完成後，更新滑動窗口（扣除掉最舊的勝場）並重置椅子數，準備進入新一輪。此外為降低全局鎖爭用，使用原子操作來更新每人的勝場數組，同時對連續 R 輪未達標的情況設定飢餓預防機制，將該參與者的優先級調高至無限大以確保其能獲得資源。在大規模處理時，可透過分組競爭或批次處理的方式降低記憶體壓力和全局鎖的粒度。

從時間複雜度來看，每一輪的操作都近似 O(1)，而每人維護的歷史記錄為 O(R)，總體空間複雜度為 O(NR)，但若使用位元映射則可降低至 O(N)。例如，假設 N 為 10^6、M 為 10^4、R 為 5 且 W 為 2，初始輪次多由速度較快者主導，隨著系統逐步調整，低速者的優先級也會不斷提升，最終實現每人五輪內至少獲得兩勝，達到近似比例公平的資源分配。",0,"在分散式即時作業系統中結合區塊鏈的信任機制，使用同步監視器來優化效能時，區塊鏈會要求所有節點對操作順序達成共識，linearizability 需求與 Sharding 技術的目標（提升並行度）之間會產生衝突。Sharding 系統透過將交易分散到不同的區塊來增加吞吐量，但在跨分片的資產轉移中，為了確保原子性操作（Atomicity），需要使用同步監視器進行鎖定和序列化，會讓交易處理變得串行，減少並行性，導致吞吐量下降。如以太坊 2.0 的分片設計中（Beacon Chain），跨分片交易的延遲比單一分片高出 3 倍，吞吐量也因此從 10k TPS 降至 3k TPS。

而當不同分片之間的智能合約相互依賴形成環狀結構時，若按照固定順序獲取鎖，可能會導致 deadlock。如分片 X 需要等待分片 Y 釋放資源，而分片 Y 又需要分片 Z 的資源，這樣的循環會阻礙系統正常運行。雖然可以透過超時機制來減少這種情況的發生，但這也會增加系統延遲，並影響即時性，這在即時性要求高的區塊鏈系統中是一個大問題。曾有數據顯示當分片數量達到 32 時，deadlock 檢測就消耗了 15% 的 CPU 資源，成為吞吐量的瓶頸。

在區塊鏈系統中，集中式鎖管理器如 PBFT 協議，會隨著節點數量的增加，協調成本呈指數級增長，讓系統無法有效達到 scalability。雖然分散式協調機制（如Raft協議）可以減少集中式瓶頸，但仍然會造成額外的延遲和網絡成本，導致每次操作的延遲大約增加 100 毫秒，對需要即時處理的區塊鏈系統來說，仍然無法滿足需求。",0,"去中心化期中考互評系統
設計一套以模擬作業系統邏輯為核心，結合簡易區塊鏈結構與 AI 模型模擬批改的匿名評分平台。

主要利用「評分共識模組與動態加權演算法」優化，整合學生彼此的評分，結合時間衰減與偏差偵測公式，自動調整每個評分者的影響力，進而得出更公平且具穩定性的最終成績。這樣能有效減少中心化評分的問題，提升批改期中考卷流程的自動化與可信度，整體系統整合了 CSV 成績上傳、隨機分派、AI 模擬批改、共識評分與區塊鏈記錄，並包含學生可發起申訴的中斷處理流程。
可應用於期中考的匿名互評，對於學生、教授皆具價值。
已將提案上傳https://hackmd.io/26akitTqR_-9MVGtqDTMCQ。",0,5,0,0
K,K11,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-12 09:19:45 UTC,1,"* 如果處理器數量少，較少的執行緒可以因執行較少次的 context switch 而提升性能

* Yes

* 如果執行緒在適當的 time quantum 條件下運行不會增加 average turnaround time

* Yes

* 也可以用記憶體來紀錄 working set，只是效能較差

* Yes

* 軟體上也需要作業系統做工作排程與資源調配

* 只要 session 結束，改變對所有未關閉或新建的 sessions 皆為  visible 

* Yes [real-time priorities 應包含 execution time, deadline 或自訂優先級等資訊]

* Yes [(1) 不具備 hold and wait (2) 可以設計規則打破 no preemption]",0,因為 uC/OS-II 使用 bitmap 達成時間複雜度 O(1) 的排程效能，但也因此讓任務優先權需要一開始就確定且無法動態調整，使得難以滿足公平性和即時性等需求。而 Linux 等大型作業系統，需要接收多變的程式運行請求和可調整的 deadline，所以需要有動態調整優先權的能力，以讓每個程式能較公平地獲取執行資源。,0,"* 建立以下 struct 紀錄玩家搶到的遊戲狀態
struct Player {
           int id; // 玩家編號
           int win_count; // 搶到椅子的次數
           int priority; // 可動態調整的優先權, 可為 W – win_count
           time_t join_time; // 加入本輪遊戲的時間
};

* 使用 max size 為 M 的 Min Heap 紀錄搶到本輪椅子的玩家, 當 N 名玩家都嘗試搶過椅子或遊戲時間結束, 結束本輪遊戲，並清空 Min Heap 和更新玩家遊戲狀態

* 比較是否搶到椅子的標準如下(即看優先權是否比 Min Heap 的 top 還大):
(1) 當有玩家 win_count < W:
     先比 player.priority(大的優先) 再比 player.join_time(小的優先)
(2) 當所有玩家 win_count >= W:
     直接比 join_time(小的優先)
※ 使用 bitmap 紀錄玩家的 win_count 是(1)否(0) < W

* 若使用併發的執行緒模擬玩家來實作遊戲，則可用 Monitor 做每輪遊戲時間跟椅子數的宏觀調控",0,"Synchronization monitor 處理併行執行緒間同步問題的方式如下:

* 使用互斥鎖 (mutex) 確保同一時間只有一個執行緒可以訪問共享資源

* 使用條件變量 (condition variable) 管理有 dependency 的其他執行緒

這樣的管理方式形成一種執行的序列性 (serializability)，但這也強制併行且具相關性的執行緒按照次序執行，從而限制了擴展性 (scalability)，因為程式實作時使用了資源鎖護的動作，所以需要仔細處理避免引發死鎖 (deadlock) 問題 。

以問題 3 的大風吹為例，此時的共享資源為 Min Heap，玩家需要有次序地加入才能正確決定每輪成功搶到椅子的人，但這也降低了併發效能和限制擴展性。因為需要搶佔的資源只有一個，用完即釋放無 Hold and Wait 問題，所以不會產生死鎖。",0,"專題嘗試利用事件集合的統計結果防範 OurChain 中偷跑挖礦的行為和決定區塊鏈的記帳權。

因為 OurChain 鼓勵節點以適當的算力來競爭記帳權，所以系統可以透過收取之事件 (資料) 的統計結果檢查出離群事件，除了可以此判定節點是否有偷跑挖礦的嫌疑外，也能提取出最大叢集(平均值 ± k*標準差)和決定其中最高優先權者取得記帳權。",0,5,0,0
L,L12,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-11 22:00:20 UTC,3,"* No. If there are lots of critical sections, more threading may lead to lower performance due to the synchronization.

* No. If the address sequence is 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 and FIFO algorithm is applied, when frame sizes are set to 3 and 4, the page faults become 9 (the last 1, 2, 5 are hit) and 10 (only the second 1 and 2 are hit), respectively.

* No. In the case where both process A and process B require 4 time units, setting quantum times to 2 and 3 gives the average turnaround times (6+8)/2=7 (AABBAABB) and (7+8)/2=7.5 (AAABBBAB), respectively.

* No. A firmware is copied to and run in RAM to gain a better performance.

* Yes.

* Yes.

* No. As hardware-supported context switches lack flexibility, software-supported ones are used in Windows and Linux.

* No. A growing file can be read with some commands (e.g., tail -f) without changing file descriptors.

* Yes.

* Yes.",0,"Unlike completely fair schedulers (CFS) and EEVDF schedulers, an O(1) scheduler doesn't maintain virtual run time and has to determine the priority in a stricter way. Linux, as a general-purpose OS, allows many interactive processes to be executed on it. For an interactive process P awaiting user's input, if the priority of P is set to be high, chances are the other background processes obtain few resources; if the priority of P is set to be low, the user may experience lagging.",0,"Assume N > M and NW <= MR. At first glance the problem seems to be solved by simply initializing Q[1..N] to be all W and then decrement Q[i] by 1 when person i gets seated, and person i is allowed to get her chair if Q[i]>0 or Q[j]=0 for all j = 1, 2, ..., N. But it doesn't work when 3 persons compete for 2 chairs in 3 rounds and each person has to be seated in at least 2 rounds, since person 1 and person 2 are allowed to occupy the 2 chairs in round 1 and round 2. But the algorithm can be slightly modified to work if we consider Q[i] the priority of person i getting seated in the next round. This is guaranteed to work since the maximum of |Q[i]-Q[j]| doesn't exceed 1 during the procedure.",0,,0,,0,5,0,0
M,M13,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-11 19:49:45 UTC,2,"* No. If there are lots of critical sections, more threading may lead to lower performance due to the synchronization.

* No. If the address sequence is 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 and FIFO algorithm is applied, when frame sizes are set to 3 and 4, the page faults become 9 (the last 1, 2, 5 are hit) and 10 (only the second 1 and 2 are hit), respectively.

* No. In the case where both process A and process B require 4 time units, setting quantum times to 2 and 3 gives the average turnaround times (6+8)/2=7 (AABBAABB) and (7+8)/2=7.5 (AAABBBAB), respectively.

* No. A firmware is copied to and run in RAM to gain a better performance.

* Yes.

* Yes.

* No. As hardware-supported context switches lack flexibility, software-supported ones are used in Windows and Linux.

* No. A growing file can be read with some commands (e.g., tail -f) without changing file descriptors.

* Yes.

* Yes.",0,"Unlike completely fair schedulers (CFS) and EEVDF schedulers, an O(1) scheduler doesn't maintain virtual run time and has to determine the priority in a stricter way. Linux, as a general-purpose OS, allows many interactive processes to be executed on it. For an interactive process P awaiting user's input, if the priority of P is set to be high, chances are the other background processes obtain few resources. On the other hand, if the priority of P is set to be low, the user may experience lagging.",0,,0,,0,,0,5,0,0
N,N14,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-11 14:00:06 UTC,1,"* No。 執行緒少不一定代表效能差，實際效能取決於工作負載與系統是否能有效平行處理。

* Yes。頁框（frame）越少，越容易造成分頁錯誤（page fault），因為活躍頁面不易保留在記憶體中。

* Yes。 時間區段（time quantum）越小，系統切換行程越頻繁，增加了額外的切換成本，導致平均周轉時間變長。

* No。 韌體（Firmware）儲存在ROM中通常比RAM慢，因此執行速度在ROM中通常較慢。

* No。 Working set model不一定需要由MMU（記憶體管理單元）硬體支援，也可以透過作業系統以軟體方式實作。

* Yes。 處於安全狀態的系統表示資源配置是安全的，不會發生死結（deadlock）。

* No。 因為上下文切換主要依賴軟體調度和保存/還原運算狀態，硬體僅提供輔助功能，其性能提升有限。

* Yes。 在「工作階段語意」（session semantics）下，檔案的變更在關閉前對其他工作階段不可見，關閉後才對新的工作階段可見。

* Yes。 即時排程器會依照任務的即時性優先順序來安排執行。

* No。 兩段式提交協定（two-phase commit protocol）只能保證分散式交易的原子性，無法保證可序列化（serializability）也無法避免死結。",0,"Linux 採用動態排程器（如 CFS、EEVDF），是因為其設計目標與 uC/OS-II 不同。uC/OS-II 面向的是嵌入式系統，任務少、需求單純，使用 O(1) 固定優先權排程器有效且簡單。而 Linux 是通用多任務作業系統，面對的是大量使用者、應用與資源競爭，以下是關鍵原因：

 

* 相同優先權任務太多，O(1) 反而沒效率

若很多任務優先權相同，固定優先權排程器會持續遍歷這些任務，導致排程開銷大、可能失去即時性。Linux 需要能區分這些任務誰該先跑。

 

* 排程器要能一直穩定跑下去

uC/OS-II 假設任務結束就釋放資源，但在 Linux 裡任務可能長期存在、動態加入或切換。固定模型會隨任務增加變得難以維護，容易造成死鎖或 starvation（飢餓）。

 

* 公平性與多核心支援

Linux 強調公平性（如互動程式與背景程式都該有資源），而且常在多核心環境下運作。動態排程（如 EEVDF）可根據任務行為與負載動態調整，更能有效利用多核心、避免任務被長期忽略。

 

結論

uC/OS-II 的 O(1) 排程雖然效率高，但無法處理現代作業系統面臨的高併發、多樣化與公平性需求。Linux 採用 CFS 或 EEVDF，能在確保效能的同時兼顧穩定與可擴展性，這是它不使用 uC/OS-II 類型排程器的根本原因。",0,"題目要求設計一個公平與有效率的機制，讓在多輪資源競爭中，每個人都有合理機會達到目標（至少 W 次），且系統可以擴展到非常大的 N 與 M。

N 人，M 張椅子，每回合搶椅子。

-- 每人必須在 R 回合內至少贏 W 次（不是每一回合都要贏）。

-- 椅子會在每回合更新（重置）。

-- 人的速度不同（搶到椅子的可能性可能不均等）。

-- 不必等所有人跑完一輪才開始下一輪（非同步）。

 

因此在設計部份可以有以下幾的面向:

* 模型設計：

--每個人是一個獨立的 process/thread。

--每輪有 M 個共享資源（椅子），每個椅子是個可搶佔的 slot（用 semaphore機制）。

--每輪後椅子會被 reset（可重新搶）。

--每個人持有自己的 win_count，目標是 win_count >= W within R rounds。

 

* 如何“搶椅子”：

--每張椅子代表一個 semaphore(1)，表示只允許一個人坐。

--每個人進入新一輪時，隨機或按某種策略嘗試 wait() 一張椅子。

--成功的就「坐下來」，win_count++。

 

* 回合結束邏輯（Round control）：

-- 每個 process 都記錄目前第幾輪，跑滿 R 輪就結束。

-- 回合間沒有全域同步，誰快誰先搶，模擬現實中的跑步速度不同。

-- 椅子在每輪結束後「重設」，讓下一輪重新競爭。

 

* 公平性與補償機制：

--若某人連續多輪搶不到椅子，可以考慮讓他下一輪有更高權重（如更快搶資源，或嘗試多個椅子），來模擬「補償公平」。

--可設一個簡單權重策略（例如：搶不到的話就優先選下一輪未搶過的椅子）。

 

Pseudo Code:

```

# shared

semaphores chairs[M] = [init(1) for _ in range(M)]

 

# per person

person(id):

    win_count = 0

    for round in range(R):

        # try to sit

        random.shuffle(chairs)  # 嘗試隨機搶椅子

        for chair in chairs:

            if wait(chair, non_blocking=True):

                win_count += 1

                break  # 成功搶到就進下一輪

        sleep(random_time())  # 模擬每人速度不同

 

    if win_count >= W:

        print(f""Person {id} succeeded with {win_count} wins"")

    else:

        print(f""Person {id} failed with only {win_count} wins"")

```

 ",0,"在區塊鏈這類分散式、強調信任的系統中，如果要處理類似「大風吹」（多個節點競爭資源，需保證每個節點公平取得資源）的情境，可以用同步監視器（synchronization monitor）來保證安全性與正確性。但要優化效能，會遇到不少挑戰。

 

* 序列化 (Serializability) 與效能限制

--說明：Monitor 會讓進入 critical section 的執行緒「一次只允許一個」，這樣雖然能保證資料一致性，但等於是把並行操作序列化了。

--困難點：當很多節點同時想預約資源時，只能一個一個進入，導致瓶頸效應。

--範例：假設 100 個節點同時要搶 10 張「區塊產權椅子」，但 monitor 只讓一次一個節點進來挑選椅子，造成後面的節點排隊、效率低下。

 

* 線性化 (Linearizability) 限制彈性

--說明：Monitor 預設操作是線性化的，也就是每個操作看起來像是發生在某一個確切的時間點上（即便是分散式系統）。

--困難點：在真正分散式的系統中（像 sharding 的區塊鏈），這種假設會讓系統無法靈活地平行處理彼此資料無關的任務。

--範例：即便兩個節點要存取的是不同 shard 的資源，monitor 還是會讓他們排隊執行，造成不必要的等待與資源浪費。

 

* 死結 (Deadlock) 風險上升

--說明：如果系統內部有多個 monitor（例如多個資源需要保護），節點若同時嘗試鎖定多個資源，就有可能出現「你等我、我等你」的死結情況。

--困難點：要用監視器時還得小心設計資源存取順序、加入 timeout 或 deadlock detection，增加複雜度。

--範例：節點 A 先鎖資源 X，再鎖 Y；節點 B 先鎖 Y 再鎖 X，就會卡住。

 

* 擴展性 (Scalability) 受限

--說明：monitor 本質是一種集中式鎖定機制，當參與者數量快速成長（例如數千個節點），競爭會急遽上升，效能反而更差。

--困難點：在分散式系統中，希望越多節點越快，但 monitor 會產生反效果。

--範例：當 1000 個節點搶同一個區塊的寫入權，若透過一個監視器來控管，每個節點都要等超久，無法做到近乎即時的反應。

 

結論:
使用 synchronization monitor 雖然設計上直觀、安全性高，但在區塊鏈這類追求高效能、高並行度、跨節點信任的系統中，會面臨以下效能上的困難：

* Monitor 將操作序列化，無法善用並行性（serializability 問題）

* 所有操作需線性化，不利 sharding 與資料獨立性（linearizability 問題）

* 多資源爭用易發生死結（deadlock 問題）

* 當節點數量擴大，效能反而下降（scalability 問題）

因此，要用 monitor 實作資源同步，必須特別設計機制來解這些效能瓶頸，例如：細粒度鎖、多 monitor 設計、shard-aware synchronization、非阻塞式演算法等。",0,"*

*

「去中心化交通違規舉發處理系統」，藉由整合區塊鏈、IPFS、AI 影像辨識（含去個資化處理）以及會員與獎懲系統等模組，動態監控並調度各個元件的資源。

 

【系統軟體/元件設計】

該系統整合區塊鏈、人工智慧與分散式運算等技術，重點在於：

 

--提高審理公正性與透明度：

取消單一員警審理機制，導入多元民眾參與的審查方式（類似國民法官制度），並利用區塊鏈上鏈存證，確保每筆判定資料不可竄改，讓決策過程全程公開透明。

 

--優化申訴與爭議處理流程：

引入智能合約與自動化工序，在民眾對結果有異議時，迅速啟動自動化的爭議處理機制，縮短申訴處理周期，並使每個爭議結果都有明確規範及審核流程。

 

--設計獎懲機制以促進有效參與：

根據民眾歷史判定記錄與可信度計算分數，利用獎懲系統進行積分、獎金發放或適當限制，避免濫用與惡意操作，同時鼓勵民眾以正確且負責的態度參與審理。

 

此外，系統軟體還將整合以下資源利用優化機制：

 

--硬體資源： 透過分散式伺服器及邊緣運算節點處理大量影像及數據，提高處理效能並降低延遲。

--軟體資源： 應用區塊鏈存證與智能合約自動執行機制，確保各模組間數據同步與一致性，同時利用 AI 協助去個資化與初步判定，減輕人工審核負擔。

 

【系統創造的價值】

 

為誰：

--政府/警政單位： 減輕警力負擔，提高案件審查的一致性與公信力。

--民眾： 增進審理流程的透明性與參與權，讓每一位參與者都能以公平方式表達意見，強化社會信任。

 

何時與何地：

--時機： 日常作業中作為優化申訴流程的輔助平台。",0,5,0,0
O,O15,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-11 12:27:15 UTC,1,"* (No) The less threading, the less performance.
        有時候少用執行緒反而更快，因為太多執行緒會造成切換和同步的成本開銷，甚至競爭資源。

* (Yes) The less frames, the more page fault rates.

* (Yes) The smaller time quantum for scheduling, the longer average turnaround time.

* (No) Firmware can be executed faster in ROM than in RAM.
        RAM 的存取速度比 ROM 或 Flash 快，韌體常被複製到 RAM 中執行以提高速度。

* (No) Working set model has to be supported by MMU.
        Working set model 是作業系統的策略，MMU 提供的是實現虛擬記憶體的硬體基礎，而非直接支援此模型。

* (Yes) A safe state will not go to a deadlocked state.

* (No) Context switch is usually supported in hardware instruction for performance.
        Context switch 主要靠作業系統軟體完成，硬體只提供部分輔助，並無單一的完整硬體指令。

* (Yes) Session Semantics: Once the file is closed, the changes are visible only to new sessions.

* (Yes) A real-time scheduler schedules tasks according to their real-time priorities.

* (No) The two-phase commit protocol guarantees serializability and prevents deadlock.
        兩階段提交確保 Atomicity 而非 Serializability，且可能因協調者故障而導致 Blocking，不能保證避免死結。",0,"RM 與 EDF 是 optimal 的即時排程演算法。RM：週期越短，優先權越高，而 EDF：截止期限越早，優先權越高，若優先權相同，ID 較小者優先。產生完整的排程表，直到超過一個 hyperperiod（最小公倍數）或指定次數。這是理論上理想的即時排程演算法，但不是 O(1) 的複雜度，因為每次都要掃過所有任務，找出最高優先權的。

uC/OS-II 用的是靜態優先權 + bitmap，可以做到O(1) 找最高優先權任務，但只適合任務數少、優先權固定的情境，適合小型即時系統，

Linux 不用這種純即時排程，因為它要支援大量任務與公平性，而且有很多同優先權的任務，如果用 bitmap 或掃描，會變得沒效率，甚至卡死。而是用CFS、EEVDF這種兼顧效率與公平的排程策略。",0,"* 是否能達成的推理

* 每輪最多只有 M 個人能贏（搶到椅子）。

* R 輪下來，總共會有 M × R 次贏的機會。

* N 個人都要至少贏 W 次，總共需要 N × W 次贏的機會。

* 如果 M × R < N × W，代表椅子不夠多，怎麼分都不可能讓每個人都達標。

* 如果 M × R ≥ N × W，理論上有機會讓每個人都至少贏 W 次。

* 基本公平與現實的平衡

* 公平：每個人搶到椅子的機會應該差不多，不能有人一直搶不到。

* 現實：有些人跑得快，搶椅子的機率比較高。

* 每輪椅子都會重置，大家每輪都有機會。

* 參與者可以自行決定是否參加本輪。

*

* 進階公平與現實的設計

* 人的速度不同，現實中可能有些人一直搶不到椅子。

* 但如果要讓嬴得次數公平，可以設計優先權規則
（例如：讓一直搶不到或搶到次數落後太多的人有更高的機會在這一輪中搶到等等）。

基本版 Pseudo code：[No3_Pseudo code.txt] (/users/180643/files/7407001?wrap=1&verifier=bV9e9oXGwzn8Udz7kIgl5s6tXstaJzp1B8uRG2Ks)",0,"在分散式區塊鏈平台上，若用同步監視器（synchronization monitor）來做資源預約（像大風吹搶椅子），雖然能確保正確性（如序列化、線性化），但會遇到以下效能優化上的困難：

* 效能瓶頸：同步監視器會讓所有節點排隊存取資源，無法同時處理多個請求，導致系統無法發揮分片（sharding）和並行（concurrency）的優勢。

* 死結風險：多個節點同時搶多個資源時，容易互相等待，產生死結（deadlock）。

* 擴展性差：節點數量一多，監視器成為系統瓶頸，無法隨節點數增加而提升效能。

* 公平性與效率衝突：為了確保公平與信任，監視器會強制序列化，但這會降低效率。

【提出方案】 為了提升效能與擴展性，可以考慮以下改進方案：

*

細粒度鎖（Fine-grained Locking）
將一個大監視器拆成多個小監視器，每個資源（椅子）各自有一把鎖，讓不同節點可以同時搶不同的椅子，提升並行度。

*

無鎖演算法（Lock-free Algorithm）
採用無鎖資料結構或原子操作（如 compare-and-swap），讓節點能在不互相等待的情況下搶資源，減少死結與等待。

*

分散式協議（如分散式鎖、共識演算法）
利用分散式鎖或區塊鏈的共識機制，讓多個節點協調搶資源，既能確保正確性，也能提升效率。例如：Raft、Paxos、或區塊鏈的 sharding 協議。

*

資源分片（Sharding）
將資源分成多個分片，每個分片由不同節點管理，節點只需和自己分片內的其他節點協調，大幅提升系統可擴展性。

假設有 100 個節點、10 張椅子。若每張椅子各自有一把鎖，最多可同時有 10 個節點搶椅子，效率比一個大監視器高很多。若再配合分片，每個分片只需管理自己的椅子，系統可隨節點數量線性擴展。

同步監視器雖然簡單，但在分散式高效能系統中會成為效能瓶頸。可以試著採用細粒度鎖、無鎖演算法、分散式協議與分片等技術，兼顧正確性、公平性與高效能。",0,"我計畫開發一套結合 AI 與 Visualization 用於 Digital Twin 中的工人與環境互動的自動化排程平台 。計畫利用大型語言模型（LLM）、MCP 協議與 iClone 8.7 來完成系統，並提出工人行動與排程的模擬。​系統將透過自然語言輸入，快速生成工廠佈局與工人排程腳本，並提供視覺化模擬，協助工廠主進行決策、訓練與展示。

完整的專案目標與說明請老師參考 hackmd 連結：

[Digital Twin 工人排程模擬系統] (https://hackmd.io/@dXq0bo-lTle1sSXOG8jsYg/rko5SUfA1x)",0,5,0,0
P,P16,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-11 10:34:24 UTC,1,"1、錯誤，執行緒過少可能降低並行處理能力，但過多執行緒也會因資源競爭和上下文切換開銷導致效能下降。
2、Yes
3、Yes
4、錯誤，RAM 的存取速度比 ROM 更快，因此從 RAM 執行韌體更快。
5、錯誤，工作集模型是作業系統層面的策略，無需MMU硬體直接支 持，但MMU可輔助實現。
6、Yes
7、錯誤，上下文切換由作業系統軟體實現，硬體指令僅輔助效能優化，但非直接支援完整切換。
8、Yes
9、Yes
10、錯誤，兩階段提交協定僅保證事務原子性，不涉及並發事務調度，因此不保證可串行化，也無法避免死鎖。",0,"Linux未直接採用類似uC/OS-II的即時調度器，而轉向完全公平調度器和EEVDF，主要是因為Linux作為通用作業系統的設計目標與嵌入式即時系統存在本質差異。

嵌入式即時系統的主要需求是要有確定性強的響應機制，可能只有單一 的任務擁有最高的優先級，和其他低優先級的功能。而linux系統則較為複雜，需要處理的動態負載，無法接受這種僵化的優先分配。 Linux需兼顧即時性、公平性和系統吞吐量，而CFS透過虛擬運行時間動態調整任務權重，即使優先權相同也能按比例分配CPU時間，EEVDF進一步引入虛擬截止時間以平衡公平性與低延遲，這種動態機制更適應多核心架構和大規模任務佇列的擴展性需求。

你同時開著遊戲、視訊聊天和下載文件，如果像uC/OS-II一樣只按固定優先權處理，一旦某個高優先級任務霸占CPU，其他任務可能會直接停止運作。而Linux的CFS就像個時間管理大師，給每個任務分配公平的CPU時間片，即使優先順序相同，也不會讓某個任務餓死。 EEVDF更進一步，還能動態調整任務的“緊急程度”，讓需要快速反應的任務更快處理。

Linux要面對成千上萬種任務，例如突然彈個網頁或後台更新系統，這些任務的行為根本無法事先預測。如果像嵌入式系統一樣用死板的優先列表，系統很容易忙不過來或卡死。所以Linux的調度器更像智慧交通系統，能根據即時路況動態調整，而不是全靠紅綠燈固定切換。",0,"1. 動態優先級佇列

每位玩家的初始優先級為 0。
每一輪計算「需求值」：
需求值 = max(0, W - 最近 R 輪的勝利次數)
優先級 = 需求值 + 累計未被滿足的次數（避免長期飢餓）。
將玩家分配至不同層級的佇列（例如：高優先級佇列、一般佇列）。
需求值越高的玩家，進入越高優先級的佇列，優先參與資源競爭（搶椅子）。
2. 時間片輪轉
每一輪即視為一個時間片，椅子（資源）在時間片開始時重新整理。
玩家於時間片內可同時非同步競爭椅子，不需等待其他人（並行搶奪）。
3. 滑動視窗資源計數
透過循環佇列（或固定長度的佇列）來記錄每位玩家最近 R 輪的輸贏狀態（1 為勝、0 為負）。
每輪結束後，淘汰最舊的紀錄，並加入本輪最新的結果。
4. 搶占與公平性
若某位玩家最近 R 輪的勝利次數 ≥ W，則在本輪中禁止參與競爭（類似釋放資源）。
直到其過去的勝利記錄被滑動視窗淘汰後，方可再次參與競爭。
若某位玩家的累計未被滿足需求值超過某個閾值（如 2W），則強制提升其優先級至最高，確保其至少獲得一次椅子
偽代碼如下：
對每位玩家 p：
p.優先級 ← 0
p.未滿足次數 ← 0
p.最近結果記錄 ← 長度為 R 的佇列（初始為 0）
對每輪時間片：
// 更新需求值與優先級
對每位玩家 p：
勝利次數 ← p.最近結果記錄 中 1 的數量
需求值 ← max(0, W - 勝利次數)
若 勝利次數 ≥ W：
p.本輪是否參與 ← 否
否則：

p.本輪是否參與 ← 是
p.優先級 ← 需求值 + p.未滿足次數
// 飢餓救助
對每位玩家 p：
若 p.未滿足次數 ≥ 2W：
p.優先級 ← 最大值
p.本輪是否參與 ← 是
// 分級佇列（如：高優先級、普通）
建立優先級佇列 ← 根據 p.優先級 由高到低排序所有有資格參與的玩家
// 椅子分配（並行搶椅子）
可用椅子 ← K
對優先級佇列中的每位玩家 p：
若 可用椅子 > 0 且 p.本輪是否參與：
分配椅子給 p
p.最近結果記錄.enqueue(1)
p.未滿足次數 ← 0
可用椅子 ← 可用椅子 - 1
否則：
p.最近結果記錄.enqueue(0)
若 p.本輪是否參與：
p.未滿足次數 ← p.未滿足次數 + 1
若 p.最近結果記錄 超過長度 R：
p.最近結果記錄.dequeue()",0,"在區塊鏈平台上以同步監視器管理資源時，可能會遇到效能優化的難題。

第一個問題是區塊鏈系統需要保證所有操作的順序是明確且不可篡改的，就像排隊買票時每個人都必須嚴格地按順序來，否則會亂套。但同步監視器如果要求所有操作像排隊一樣嚴格依序執行，就會導致系統無法同時處理多個任務，例如明明有多個視窗可以賣票，卻硬要所有人擠在一個視窗排隊。這種死板排隊雖然保證了公平，但效率會變得很低，尤其是在分片技術的場景下，反而讓系統變慢了。

第二個問題是死鎖。例如兩個人同時需要對方的資源才能完成任務。在區塊鏈中，智慧合約可能會因為爭奪資源而陷入這種僵局，而同步監視器需要不斷檢查和處理這些情況。但區塊鏈的節點分佈在全球，網路延遲會讓檢查變得更慢，甚至可能誤判，導致系統頻繁回滾操作或等待逾時，白白浪費時間和資源。

第三個難題是擴展性。同步監視器如果只有一個中心化的指揮中心，所有決策都要經過它，就像全校學生都擠在校長室門口等蓋章，校長根本忙不過來。但如果是分散式的多個監視器，雖然能分擔壓力，這些監視器之間又需要不斷溝通協調，例如跨分片的操作需要多個監視器達成一 致，反而讓溝通成本變得更高。這就像把校長換成十個班主任，但每個班級要辦事都得十個班主任開會討論，效率反而可能更低。

第四個問題是信任和速度的平衡。區塊鏈的信任來自多個節點共同驗證結果，就像全班一起核對作業答案。但同步監視器為了快速分配資源，可能會跳過部分驗證步驟直接做決定。如果其他節點來不及檢查這個決定，就可能出現分歧，例如有人覺得資源分配不公平，導致系統需要重新協調甚至撤銷操作。但如果每一步都等所有節點確認，速度又會很慢，完全發揮不出分片技術帶來的並發優勢。",0,"我設計了一套台灣醫院模擬叫號系統和一套評估模式。
在叫號系統中，我模擬了目前基於預約號的排隊策略、完全FIFO的策略、與我自己根據實際情況優化的策略
在評估方法上，我通過模擬生成不同類型的病人，並通過以下幾個維度的資料去評估一個叫號策略的優劣：
所有病人平均等待时间、各类型病人等待时间、医生总工作时间、 空闲等待时间、医生利用率、过号就诊患者數量、过号率
通過模擬和比較不同的排隊策略，嘗試去結合實際情況設計出一種更符合目前醫院患者到達情況的叫號系統，讓各種不同類型的患者減少排隊等待的時間，讓醫生有較高的效率去完成一日的就診任務。
本專案參考的設計思路主要來自作業系統中不同種的任務排程模式，優先級策略等，特別是動態優先級策略知識在系統中應用廣泛。",0,5,0,0
Q,Q17,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-10 09:15:28 UTC,1,"1、錯誤，執行緒過少可能降低並行處理能力，但過多執行緒也會因資源競爭和上下文切換開銷導致效能下降。

2、Yes

3、Yes

4、錯誤，RAM 的存取速度比 ROM 更快，因此從 RAM 執行韌體更快。

5、錯誤，工作集模型是作業系統層面的策略，無需MMU硬體直接支持，但MMU可輔助實現。

6、Yes

7、錯誤，上下文切換由作業系統軟體實現，硬體指令僅輔助效能優化，但非直接支援完整切換。

8、Yes

9、Yes

10、錯誤，兩階段提交協定僅保證事務原子性，不涉及並發事務調度，因此不保證可串行化，也無法避免死鎖。",0,"Linux未直接採用類似uC/OS-II的即時調度器，而轉向完全公平調度器和EEVDF，主要是因為Linux作為通用作業系統的設計目標與嵌入式即時系統存在本質差異。

嵌入式即時系統的主要需求是要有確定性強的響應機制，可能只有單一的任務擁有最高的優先級，和其他低優先級的功能。而linux系統則較為複雜，需要處理的動態負載，無法接受這種僵化的優先分配。 Linux需兼顧即時性、公平性和系統吞吐量，而CFS透過虛擬運行時間動態調整任務權重，即使優先權相同也能按比例分配CPU時間，EEVDF進一步引入虛擬截止時間以平衡公平性與低延遲，這種動態機制更適應多核心架構和大規模任務佇列的擴展性需求。

你同時開著遊戲、視訊聊天和下載文件，如果像uC/OS-II一樣只按固定優先權處理，一旦某個高優先級任務霸占CPU，其他任務可能會直接停止運作。而Linux的CFS就像個時間管理大師，給每個任務分配公平的CPU時間片，即使優先順序相同，也不會讓某個任務餓死。 EEVDF更進一步，還能動態調整任務的“緊急程度”，讓需要快速反應的任務更快處理。

Linux要面對成千上萬種任務，例如突然彈個網頁或後台更新系統，這些任務的行為根本無法事先預測。如果像嵌入式系統一樣用死板的優先列表，系統很容易忙不過來或卡死。所以Linux的調度器更像智慧交通系統，能根據即時路況動態調整，而不是全靠紅綠燈固定切換。",0,"我設計了以下幾項額外的遊戲規則，來保證搶椅子遊戲的公平，盡量讓每個跑的不一樣快的人，都能在R論中搶到W把椅子

1. 動態優先級佇列
----------

*

每位玩家的初始優先級為 0。

*

每一輪計算「需求值」：
需求值 = max(0, W - 最近 R 輪的勝利次數)

*

優先級 = 需求值 + 累計未被滿足的次數（避免長期飢餓）。

*

將玩家分配至不同層級的佇列（例如：高優先級佇列、一般佇列）。

*

需求值越高的玩家，進入越高優先級的佇列，優先參與資源競爭（搶椅子）。

2. 時間片輪轉

*

每一輪即視為一個時間片，椅子（資源）在時間片開始時重新整理。

*

玩家於時間片內可同時非同步競爭椅子，不需等待其他人（並行搶奪）。

3. 滑動視窗資源計數

*

透過循環佇列（或固定長度的佇列）來記錄每位玩家最近 R 輪的輸贏狀態（1 為勝、0 為負）。

*

每輪結束後，淘汰最舊的紀錄，並加入本輪最新的結果。

4. 搶占與公平性

*

若某位玩家最近 R 輪的勝利次數 ≥ W，則在本輪中禁止參與競爭（類似釋放資源）。

*

直到其過去的勝利記錄被滑動視窗淘汰後，方可再次參與競爭。

*

若某位玩家的累計未被滿足需求值超過某個閾值（如 2W），則強制提升其優先級至最高，確保其至少獲得一次椅子

偽代碼如下：

對每位玩家 p：
    p.優先級 ← 0
    p.未滿足次數 ← 0
    p.最近結果記錄 ← 長度為 R 的佇列（初始為 0）

對每輪時間片：
    // 更新需求值與優先級
    對每位玩家 p：
        勝利次數 ← p.最近結果記錄 中 1 的數量
        需求值 ← max(0, W - 勝利次數)
        若 勝利次數 ≥ W：
            p.本輪是否參與 ← 否
        否則：
            p.本輪是否參與 ← 是
        p.優先級 ← 需求值 + p.未滿足次數

    // 飢餓救助
    對每位玩家 p：
        若 p.未滿足次數 ≥ 2W：
            p.優先級 ← 最大值
            p.本輪是否參與 ← 是

    // 分級佇列（如：高優先級、普通）
    建立優先級佇列 ← 根據 p.優先級 由高到低排序所有有資格參與的玩家

    // 椅子分配（並行搶椅子）
    可用椅子 ← K
    對優先級佇列中的每位玩家 p：
        若 可用椅子 > 0 且 p.本輪是否參與：
            分配椅子給 p
            p.最近結果記錄.enqueue(1)
            p.未滿足次數 ← 0
            可用椅子 ← 可用椅子 - 1
        否則：
            p.最近結果記錄.enqueue(0)
            若 p.本輪是否參與：
                p.未滿足次數 ← p.未滿足次數 + 1

        若 p.最近結果記錄 超過長度 R：
            p.最近結果記錄.dequeue()",0,"在區塊鏈平台上以同步監視器管理資源時，可能會遇到效能優化的難題。

第一個問題是區塊鏈系統需要保證所有操作的順序是明確且不可篡改的，就像排隊買票時每個人都必須嚴格地按順序來，否則會亂套。但同步監視器如果要求所有操作像排隊一樣嚴格依序執行，就會導致系統無法同時處理多個任務，例如明明有多個視窗可以賣票，卻硬要所有人擠在一個視窗排隊。這種死板排隊雖然保證了公平，但效率會變得很低，尤其是在分片技術的場景下，反而讓系統變慢了。

第二個問題是死鎖。例如兩個人同時需要對方的資源才能完成任務。在區塊鏈中，智慧合約可能會因為爭奪資源而陷入這種僵局，而同步監視器需要不斷檢查和處理這些情況。但區塊鏈的節點分佈在全球，網路延遲會讓檢查變得更慢，甚至可能誤判，導致系統頻繁回滾操作或等待逾時，白白浪費時間和資源。

第三個難題是擴展性。同步監視器如果只有一個中心化的指揮中心，所有決策都要經過它，就像全校學生都擠在校長室門口等蓋章，校長根本忙不過來。但如果是分散式的多個監視器，雖然能分擔壓力，這些監視器之間又需要不斷溝通協調，例如跨分片的操作需要多個監視器達成一致，反而讓溝通成本變得更高。這就像把校長換成十個班主任，但每個班級要辦事都得十個班主任開會討論，效率反而可能更低。

第四個問題是信任和速度的平衡。區塊鏈的信任來自多個節點共同驗證結果，就像全班一起核對作業答案。但同步監視器為了快速分配資源，可能會跳過部分驗證步驟直接做決定。如果其他節點來不及檢查這個決定，就可能出現分歧，例如有人覺得資源分配不公平，導致系統需要重新協調甚至撤銷操作。但如果每一步都等所有節點確認，速度又會很慢，完全發揮不出分片技術帶來的並發優勢。",0,"我設計了一套台灣醫院模擬叫號系統和一套評估模式。

在叫號系統中，我模擬了目前基於預約號的排隊策略、完全FIFO的策略、與我自己根據實際情況優化的策略

在評估方法上，我通過模擬生成不同類型的病人，並通過以下幾個維度的資料去評估一個叫號策略的優劣：
所有病人平均等待时间、各类型病人等待时间、医生总工作时间、 空闲等待时间、医生利用率、过号就诊患者數量、过号率

通過模擬和比較不同的排隊策略，嘗試去結合實際情況設計出一種更符合目前醫院患者到達情況的叫號系統，讓各種不同類型的患者減少排隊等待的時間，讓醫生有較高的效率去完成一日的就診任務。

本專案參考的設計思路主要來自作業系統中不同種的任務排程模式，優先級策略等，特別是動態優先級策略知識在系統中應用廣泛。",0,5,0,0
R,R18,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-09 20:27:48 UTC,1,"* No. If there are lots of critical sections, more threading may lead to lower performance due to the synchronization.

* No. If the address sequence is 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 and FIFO algorithm is applied, when frame sizes are set to 3 and 4, the page faults become 9 (the last 1, 2, 5 are hit) and 10 (only the second 1 and 2 are hit), respectively.

* No. In the case where both process A and process B require 4 time units, setting quantum times to 2 and 3 gives the average turnaround times (6+8)/2=7 (AABBAABB) and (7+8)/2=7.5 (AAABBBAB), respectively.

* No. A firmware is copied to and run in RAM to gain a better performance.

* Yes.

* Yes.

* No. As hardware-supported context switches lack flexibility, software-supported ones are used in Windows and Linux.

* No. A growing file can be read with some commands, e.g., tail -f.

* Yes.

* Yes.",0,,0,,0,,0,,0,5,0,0
S,S19,,高等作業系統 (CSIE7010),46009,113-2:922_M1360,2025-04-09 06:03:57 UTC,1,"1. No. Creating threads requires allocating resousrces, if the tasks very simple, than using multiple threads for them may be less efficient. Also, Under multithread scenario, some synchronization mechanisms, such as lock or barrier, might be requried, which can also lead to performance drop.

2. TRUE.

3. TRUE.

4. No. Executing on RAM would be significantly faster.

5.

6. TRUE.

7. No. It's usually done in software by OS. A single context switch is usually very fast and almost requires no time, so it won't make much sense to support it in hardware.

8. TRUE.

9. TRUE.

10. No. Two-pahse commit protocol did guarantee serializability, but it doesn't prevent deadlock.",0,,0,"Our solution is presented below:

First of all, we check if [LaTeX: W \leq R] (/equation_images/W%2520%255Cleq%2520R?scale=1) and [LaTeX: R \times M \geq N \times W] (/equation_images/R%2520%255Ctimes%2520M%2520%255Cgeq%2520N%2520%255Ctimes%2520W?scale=1); otherwise, it is impossible to ensure each person win at least W times.

Since each person need to win at least W times, we first define the ""danger"" state: A person is in danger state if

[LaTeX: \left(W - the\ person's\ accumulated\ win\ times\right) = the\ number\ of\ remained\ rounds] (/equation_images/%255Cleft(W%2520-%2520the%255C%2520person's%255C%2520accumulated%255C%2520win%255C%2520times%255Cright)%2520%253D%2520the%255C%2520number%255C%2520of%255C%2520remained%255C%2520rounds?scale=1). A person is in danger state at the end of a round infers that the person must win in each of the following rounds.

Our strategy for competitions is, In each round, only the people that are in danger state can complete for the chairs at first, and then, only the people who haven't won W times and haven't acquired a chair in this round compete for the chairs. Finally, the people who haven't won a chair in this round compete for the chairs. At the end of each round, each person check if it is in danger state.

Our code is presented here: [Musical Chairs Problem] (https://hackmd.io/@VJqhl4IzToG8p2J9WXnPog/Bk2pyKQ01e)",0,,0,,0,5,0,0
T,T20,,,,,2025-04-12 13:25:34 UTC,1,"* No。threading的數量和performance的關係不一定是正相關的，需要取決於任務的類型和資源競爭。Threading過多可能導致context的頻繁切換，從而降低performance；但是過少可能導致無法充分利用多核心處理器，也會降低performance。

* Yes。

* Yes。

* No。RAM比ROM更快，所以firmware在RAM上執行更快。

* No。雖然MMU可以處理頁面訪問和追蹤，但是在沒有MMU支援的情況下，作業系統可以通過time clock和軟體模擬的方式（例如强制出發page fault）來估計哪些page是「常被使用的」。

* Yes。

* No。主要是由作業系統的軟體層面管理。

* No。Session Semantics規定對一個打開的共享的document所進行的修改只對session可見，被關閉之後的修改結果對其他sessions可見。

* Yes。

* two-phase commit protocol可以保證atomicity，但不能保證serializability，因爲serializability還需要locks 或timestamps等別的機制；2PC本身可能導致阻塞，但不會直接防止deadlock。",0,"使用uC/OS-II的通常適用於Hard Real-Time system這種嚴格保證任務的截止時間、優先級固定且通常任務數量較少的系統。而對於Linux這種需要處理更多任務的作業系統，需要有更複雜的處理，所以使用Completely Fair Scheduler（CFS）和EEVDF：

首先，uC/OS-II的O(1)是基於固定優先級，沒辦法處理大量同優先權的任務的公平性問題。uC/OS-II每個優先級對應一個任務列表，這樣在schedule時可以快速找到最高優先級的任務；但當有很多任務有相同優先級或者時間限制時，可能會造成starving，導致一個任務一直搶先執行。CFS進行公平分時，能夠動態調整任務權重，適應不同的loading；而EEVDF引入虛擬截止期限，更加兼顧公平性和即時性。

此外，在處理相同優先級任務的效率上，uC/OS-II也有限制。如果使用多個任務共享相同優先級，需額外管理「同優先級隊列」（例如輪詢或 FIFO），增加調度開銷；而使用CFS，可以用紅黑樹可以動態追蹤任務的vruntime，保證公平性，高效分配 CPU 時間。",0,"我們要模擬幾個人（N）、每輪幾張椅子（M）、總共幾輪比賽（R），以及每個人最少要贏幾次（W）才能過關。我們可以用一個隨機值模擬每個人搶椅子的速度，需要記錄每個人已經贏了幾次，如果提前滿足W次就不用繼續參加，每個人能可以決定是否參加下一輪，不用等其他人，每輪結束馬上開始下一輪，比賽的輪數R一直纍計到所有人都達到W次。

Pseudo Code如下：

Input: N 人, M 椅子, R 輪, W 次勝利

for i = 1 to N:

    wins[i] = 0

    speed[i] = random base speed  //每個人的速度不同

for round = 1 to R:

    participants = [i for i in 1..N if wins[i] < W]  //如果某個人已經達到 W 次勝利，就不用再比

    if participants.empty(): break //所有人都達標了，那比賽就可以提前結束了

    run_times = {}

    for i in participants:

        run_times[i] = random() / speed[i]   // random()模擬當場的臨場反應，speed[i]模擬搶椅子的時間（越小越快）

    winners = get_top_M_fastest(run_times, M) //只有前面搶到的人才有椅子

    for winner in winners:

        wins[winner] += 1

Output: 本輪結果（第幾輪，通過人數，是否全部通過）",0,"當我們在類似區塊鏈這種需要信任與高併發的分散式即時系統中使用monitor來處理像 Homework 3「大風吹」搶椅子那樣的資源分配問題時，會面臨多種效能上的挑戰。

首先是Serializability的問題。當每個node（像區塊鏈中的處理器）都必須透過 monitor 依序進入關鍵區操作共享資源時，就算彼此之間沒有資料衝突，仍會被迫排隊等待。例如：1000 個人同時要搶 10 張椅子，但是每次只能讓一人進來操作，導致本來可以平行完成的搶椅動作變成序列執行，整體效率大幅下降。

其次是線性化（Linearizability）帶來的效能壓力。為了確保系統看起來像是「每次只有一個node成功操作」，系統會強制讓所有操作表現得像是在某條時間線上依序進行，就像搶到椅子後還要確認這個椅子被搶到這個結果被其他人同步認可，其他人才能繼續行動，這會大幅增加等待時間，無法真正發揮系統的平行處理能力。

再來是Deadlock的風險。當多個nodes同時等待彼此釋放資源時，就可能出現資源互等的情況。例如： A搶到了椅子1，但想坐椅子2、B 等到椅子2，想坐椅子1；彼此都持有對方想要的資源，就會造成大家都卡住，誰也無法完成動作，整個系統陷入停滯。

最後是系統可擴展性的問題。monitor 本身通常設計來處理少量程序或threading的同步問題，但當nodes數量暴增（像在大規模區塊鏈系統中），所有nodes都集中透過單一monitor來同步資源，就會造成瓶頸。就好像如果要給游戲設置一個裁判，每個人要進行游戲的時候都需要到裁判那裏登記，一開始10個人還可以良好運行，但是如果土壤增加到1000個人參加比賽，就會變得效率很低。

總而言之，雖然synchronization monitor在邏輯上簡單好用，能幫助我們實現同步與一致性，但在高併發、需要平行處理的分散式系統中，會因為序列化限制、線性化等待、死結風險和可擴展性不足，而難以發揮最佳效能。因此若想要支援公平又高效的「大風吹」式資源競爭，可能需要更進階的並行技術來取代單純的 monitor 機制。",0,"我想要實作的是智慧公車調度系統，這種系統設計的核心是讓每輛公車能夠自主決策並根據需求調整運行路線，而非依賴客運公司的人爲調度。透過類似排程演算法（如 SJF、Priority、Round Robin）、佇列與資源分配、進程同步（如互斥與 IPC）等技術，公車可自主決定接送乘客的順序與路線，同時透過任務佇列與動態負載平衡提升資源利用率。此外，系統也可運用即時排程與預測機制來減少乘客等待時間與避免死結（同時多輛車接一個乘客），並即時通知乘客當前與預計路線。

該系統主要針對的人群是鄉村地區的公車乘客，例如，在中國浙江的溫州，城鄉公車采用的是固定路綫巡車，乘客即招即停的策略，雖然這種方式一定程度上能提高效率，但是固定的路綫有時候會有資源調配不均的情況，比如學校放學期間需要更多車次資源，這時候就需要增派車次，如果可以根據乘客的需求動態調整車次，就可以解決這些問題。這個系統同樣適用於其他交通需求較為分散且人員少的地區，如台灣的蘭嶼、金門。目的是讓公車系統可以實現最大化資源利用，提高居民出行效率。",0,5,0,0