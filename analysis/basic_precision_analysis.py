#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BASIC_PRECISION åƒæ•¸æœ€ä½³åŒ–åˆ†æå·¥å…·

åˆ†æä¸åŒ BASIC_PRECISION å€¼å°å­¸ç”Ÿè€ƒè©¦äº’è©•ç³»çµ±çš„å½±éŸ¿
é‡é»é—œæ³¨æ¬Šé‡è¨ˆç®—ã€è²è­½åˆ†æ•¸è¨ˆç®—å’Œç³»çµ±ç©©å®šæ€§
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import logging
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from core.vancouver import Graph

class LoggerPrint:
    """è‡ªå®šç¾©æ—¥å¿—è¨˜éŒ„å™¨ï¼ŒåŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    
    def __init__(self, log_file=None):
        if log_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = f"basic_precision_analysis_{timestamp}.log"
        
        self.log_file = log_file
        
        # è¨­ç½®æ—¥å¿—è¨˜éŒ„å™¨
        self.logger = logging.getLogger('BasicPrecisionAnalyzer')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„è™•ç†å™¨
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # æ–‡ä»¶è™•ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°è™•ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # è¨­ç½®æ ¼å¼
        formatter = logging.Formatter('%(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def print(self, message):
        """åŒæ™‚è¨˜éŒ„åˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
        self.logger.info(message)

class BasicPrecisionAnalyzer:
    """BASIC_PRECISION åƒæ•¸åˆ†æå™¨"""
    
    def __init__(self, logger_print=None):
        if logger_print is None:
            self.logger = LoggerPrint()
        else:
            self.logger = logger_print
        
        # åˆ†æç¯„åœï¼šå¾ 1e-6 åˆ° 1e-1
        self.precision_values = np.logspace(-6, -1, 25)
        
        # å­˜å„²åˆ†æçµæœ
        self.results = {}
        
        # å­¸ç”Ÿé…ç½®ï¼ˆå›ºå®š15å€‹å­¸ç”Ÿï¼‰
        self.student_names = [
            'Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 
            'Henry', 'Iris', 'Jack', 'Kate', 'Leo', 'Maya', 'Nick', 'Olivia'
        ]
        
        # å­¸ç”ŸçœŸå¯¦èƒ½åŠ›åˆ†ä½ˆï¼ˆå›ºå®šï¼‰
        np.random.seed(42)  # ç¢ºä¿å¯é‡ç¾æ€§
        self.true_abilities = np.random.normal(75, 15, 15)
        self.true_abilities = np.clip(self.true_abilities, 0, 100)
        
        # å­¸ç”Ÿè©•åˆ†è®Šç•°åº¦ï¼ˆå›ºå®šï¼‰
        self.student_variances = np.random.uniform(0.5, 3.0, 15)
    
    def create_enhanced_graph(self, basic_precision):
        """å‰µå»ºå¢å¼·åœ–å½¢å°è±¡ï¼Œä½¿ç”¨æŒ‡å®šçš„ basic_precision"""
        # ç›´æ¥å‰µå»º Graph å°è±¡ï¼Œä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸
        graph = Graph(basic_precision=basic_precision, use_all_data=True)
        
        # æ‰‹å‹•æ·»åŠ å­¸ç”Ÿå’Œä½œæ¥­çš„è©•åˆ†æ•¸æ“š
        # ä½¿ç”¨å›ºå®šçš„éš¨æ©Ÿç¨®å­ç¢ºä¿ä¸€è‡´æ€§
        np.random.seed(42)
        
        # æ¨¡æ“¬å­¸ç”Ÿå°ä½œæ¥­çš„è©•åˆ†
        assignments = ['HW1', 'HW2', 'HW3', 'HW4', 'HW5']
        
        for i, assignment in enumerate(assignments):
            # æ¯å€‹ä½œæ¥­çš„çœŸå¯¦åˆ†æ•¸ï¼ˆåŸºæ–¼å›ºå®šç¨®å­ï¼‰
            true_grade = 70 + i * 5 + np.random.normal(0, 10)
            true_grade = np.clip(true_grade, 0, 100)
            
            # æ¯å€‹å­¸ç”Ÿå°é€™å€‹ä½œæ¥­çš„è©•åˆ†
            for j, student_name in enumerate(self.student_names):
                # åŸºæ–¼å­¸ç”Ÿçš„èƒ½åŠ›å’Œè®Šç•°åº¦ç”Ÿæˆè©•åˆ†
                student_bias = (self.true_abilities[j] - 75) * 0.1  # èƒ½åŠ›åå·®
                noise = np.random.normal(0, self.student_variances[j])
                grade = true_grade + student_bias + noise
                grade = np.clip(grade, 0, 100)
                
                # åªæœ‰éƒ¨åˆ†å­¸ç”Ÿè©•åˆ†æ¯å€‹ä½œæ¥­ï¼ˆæ¨¡æ“¬äº’è©•å ´æ™¯ï¼‰
                if np.random.random() < 0.6:  # 60% çš„å­¸ç”Ÿåƒèˆ‡è©•åˆ†
                    graph.add_review(student_name, assignment, grade)
        
        return graph
    
    def analyze_system_metrics(self, graph):
        """åˆ†æç³»çµ±æŒ‡æ¨™"""
        
        # é‹è¡Œ Vancouver ç®—æ³•
        graph.evaluate_items(n_iterations=20)
        graph.evaluate_users()
        
        # è¨ˆç®—è²è­½åˆ†æ•¸
        reputation_scores = []
        for student_name in self.student_names:
            user = graph.get_user(student_name)
            if user and hasattr(user, 'quality') and user.quality is not None:
                reputation_scores.append(user.quality)
            else:
                reputation_scores.append(0.0)
        
        reputation_scores = np.array(reputation_scores)
        
        # è¨ˆç®—ä½œæ¥­æˆç¸¾
        assignment_grades = []
        assignments = ['HW1', 'HW2', 'HW3', 'HW4', 'HW5']
        for assignment in assignments:
            item = graph.get_item(assignment)
            if item and hasattr(item, 'grade') and item.grade is not None:
                assignment_grades.append(item.grade)
            else:
                assignment_grades.append(50.0)
        
        assignment_grades = np.array(assignment_grades)
        
        # è¨ˆç®—ç”¨æˆ¶åå·®å’Œè®Šç•°æ•¸
        user_biases = []
        user_variances = []
        for student_name in self.student_names:
            user = graph.get_user(student_name)
            if user:
                if hasattr(user, 'bias') and user.bias is not None:
                    user_biases.append(abs(user.bias))
                else:
                    user_biases.append(0.0)
                    
                if hasattr(user, 'variance') and user.variance is not None:
                    user_variances.append(user.variance)
                else:
                    user_variances.append(1.0)
            else:
                user_biases.append(0.0)
                user_variances.append(1.0)
        
        user_biases = np.array(user_biases)
        user_variances = np.array(user_variances)
        
        # ç³»çµ±æŒ‡æ¨™
        metrics = {
            'reputation_mean': np.mean(reputation_scores),
            'reputation_std': np.std(reputation_scores),
            'reputation_range': np.max(reputation_scores) - np.min(reputation_scores),
            'reputation_effective_ratio': np.sum(reputation_scores > 0) / len(reputation_scores),
            
            'grade_mean': np.mean(assignment_grades),
            'grade_std': np.std(assignment_grades),
            'grade_range': np.max(assignment_grades) - np.min(assignment_grades),
            
            'bias_mean': np.mean(user_biases),
            'bias_std': np.std(user_biases),
            'variance_mean': np.mean(user_variances),
            'variance_std': np.std(user_variances),
            
            'system_stability': 1.0 / (1.0 + np.std(user_variances)),
            'discrimination_power': np.std(reputation_scores) / (np.mean(reputation_scores) + 1e-6),
        }
        
        return metrics
    
    def workload_analysis(self, graph):
        """å·¥ä½œè² è¼‰åˆ†æ - è©•åˆ†åˆ†ä½ˆçµ±è¨ˆ"""
        grading_counts = {}
        for student_name in self.student_names:
            grading_counts[student_name] = 0
            
        # çµ±è¨ˆæ¯å€‹å­¸ç”Ÿçš„è©•åˆ†æ¬¡æ•¸
        for student_name in self.student_names:
            user = graph.get_user(student_name)
            if user:
                grading_counts[student_name] = len(user.items)
        
        counts = list(grading_counts.values())
        
        if len(counts) == 0 or max(counts) == 0:
            return {
                'min_workload': 0,
                'max_workload': 0,
                'mean_workload': 0.0,
                'std_workload': 0.0,
                'workload_balance': 1.0
            }
        
        return {
            'min_workload': min(counts),
            'max_workload': max(counts),
            'mean_workload': np.mean(counts),
            'std_workload': np.std(counts),
            'workload_balance': 1.0 - (np.std(counts) / np.mean(counts)) if np.mean(counts) > 0 else 1.0
        }
    
    def run_analysis(self):
        """é‹è¡Œå®Œæ•´çš„ BASIC_PRECISION åˆ†æ"""
        
        self.logger.print("=" * 80)
        self.logger.print("é–‹å§‹ BASIC_PRECISION åƒæ•¸æœ€ä½³åŒ–åˆ†æ")
        self.logger.print("=" * 80)
        self.logger.print(f"åˆ†æç¯„åœ: {self.precision_values[0]:.2e} åˆ° {self.precision_values[-1]:.2e}")
        self.logger.print(f"æ¸¬è©¦é»æ•¸é‡: {len(self.precision_values)}")
        self.logger.print("")
        
        for i, precision in enumerate(self.precision_values):
            self.logger.print(f"åˆ†æé€²åº¦: {i+1}/{len(self.precision_values)} - BASIC_PRECISION = {precision:.2e}")
            
            try:
                # å‰µå»ºå¢å¼·åœ–å½¢
                graph = self.create_enhanced_graph(precision)
                
                # åˆ†æç³»çµ±æŒ‡æ¨™
                metrics = self.analyze_system_metrics(graph)
                
                # å·¥ä½œè² è¼‰åˆ†æ
                workload = self.workload_analysis(graph)
                
                # åˆä½µçµæœ
                result = {**metrics, **workload}
                
                # è¨ˆç®—ç¶œåˆå¾—åˆ†
                result['composite_score'] = self.calculate_composite_score(result)
                
                # å­˜å„²çµæœ
                self.results[precision] = result
                
                self.logger.print(f"  è²è­½æœ‰æ•ˆæ¯”ç‡: {result['reputation_effective_ratio']:.3f}")
                self.logger.print(f"  ç³»çµ±ç©©å®šæ€§: {result['system_stability']:.3f}")
                self.logger.print(f"  ç¶œåˆå¾—åˆ†: {result['composite_score']:.3f}")
                self.logger.print("")
                
            except Exception as e:
                self.logger.print(f"éŒ¯èª¤ï¼šåˆ†æ BASIC_PRECISION = {precision:.2e} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        # æ‰¾åˆ°æœ€ä½³åƒæ•¸
        best_precision = self.find_optimal_precision()
        self.logger.print("=" * 80)
        self.logger.print("åˆ†æå®Œæˆï¼")
        self.logger.print("=" * 80)
        self.logger.print(f"æœ€ä½³ BASIC_PRECISION å€¼: {best_precision:.2e}")
        self.logger.print(f"æœ€ä½³ç¶œåˆå¾—åˆ†: {self.results[best_precision]['composite_score']:.3f}")
        self.logger.print("")
        
        return self.results
    
    def calculate_composite_score(self, result):
        """è¨ˆç®—ç¶œåˆå¾—åˆ†"""
        
        # æ¬Šé‡åˆ†é…
        weights = {
            'reputation_effective_ratio': 0.25,  # è²è­½æœ‰æ•ˆæ€§
            'system_stability': 0.20,            # ç³»çµ±ç©©å®šæ€§
            'discrimination_power': 0.15,        # å€åˆ†èƒ½åŠ›
            'workload_balance': 0.15,            # å·¥ä½œè² è¼‰å¹³è¡¡
            'grade_range': 0.10,                 # æˆç¸¾å€åˆ†åº¦
            'bias_control': 0.15                 # åå·®æ§åˆ¶
        }
        
        # æ¨™æº–åŒ–æŒ‡æ¨™ï¼ˆ0-1ç¯„åœï¼‰
        normalized_metrics = {
            'reputation_effective_ratio': result['reputation_effective_ratio'],
            'system_stability': min(result['system_stability'], 1.0),
            'discrimination_power': min(result['discrimination_power'] / 2.0, 1.0),
            'workload_balance': result['workload_balance'],
            'grade_range': min(result['grade_range'] / 50.0, 1.0),
            'bias_control': max(0, 1.0 - result['bias_mean'] / 10.0)
        }
        
        # è¨ˆç®—åŠ æ¬Šç¶œåˆå¾—åˆ†
        composite_score = sum(
            weights[key] * normalized_metrics[key]
            for key in weights.keys()
        )
        
        return composite_score
    
    def find_optimal_precision(self):
        """æ‰¾åˆ°æœ€ä½³çš„ BASIC_PRECISION å€¼"""
        best_score = -1
        best_precision = None
        
        for precision, result in self.results.items():
            if result['composite_score'] > best_score:
                best_score = result['composite_score']
                best_precision = precision
        
        return best_precision
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        
        if not self.results:
            return "æ²’æœ‰åˆ†æçµæœå¯ä¾›å ±å‘Šã€‚"
        
        best_precision = self.find_optimal_precision()
        best_result = self.results[best_precision]
        
        report = []
        report.append("=" * 80)
        report.append("BASIC_PRECISION åƒæ•¸æœ€ä½³åŒ–åˆ†æå ±å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # æœ€ä½³åƒæ•¸
        report.append("ğŸ¯ æœ€ä½³åƒæ•¸ï¼š")
        report.append(f"  BASIC_PRECISION: {best_precision:.2e}")
        report.append(f"  ç¶œåˆå¾—åˆ†: {best_result['composite_score']:.3f}")
        report.append("")
        
        # é—œéµæŒ‡æ¨™
        report.append("ğŸ“Š é—œéµæ€§èƒ½æŒ‡æ¨™ï¼š")
        report.append(f"  è²è­½æœ‰æ•ˆæ¯”ç‡: {best_result['reputation_effective_ratio']:.1%}")
        report.append(f"  ç³»çµ±ç©©å®šæ€§: {best_result['system_stability']:.3f}")
        report.append(f"  å€åˆ†èƒ½åŠ›: {best_result['discrimination_power']:.3f}")
        report.append(f"  å·¥ä½œè² è¼‰å¹³è¡¡: {best_result['workload_balance']:.3f}")
        report.append("")
        
        # è²è­½ç³»çµ±
        report.append("ğŸ† è²è­½ç³»çµ±è¡¨ç¾ï¼š")
        report.append(f"  å¹³å‡è²è­½åˆ†æ•¸: {best_result['reputation_mean']:.3f}")
        report.append(f"  è²è­½åˆ†æ•¸æ¨™æº–å·®: {best_result['reputation_std']:.3f}")
        report.append(f"  è²è­½åˆ†æ•¸ç¯„åœ: {best_result['reputation_range']:.3f}")
        report.append("")
        
        # è©•åˆ†ç³»çµ±
        report.append("ğŸ“ è©•åˆ†ç³»çµ±è¡¨ç¾ï¼š")
        report.append(f"  å¹³å‡æˆç¸¾: {best_result['grade_mean']:.1f}")
        report.append(f"  æˆç¸¾æ¨™æº–å·®: {best_result['grade_std']:.3f}")
        report.append(f"  æˆç¸¾ç¯„åœ: {best_result['grade_range']:.1f}")
        report.append("")
        
        # åå·®æ§åˆ¶
        report.append("âš–ï¸ åå·®æ§åˆ¶ï¼š")
        report.append(f"  å¹³å‡åå·®: {best_result['bias_mean']:.3f}")
        report.append(f"  åå·®æ¨™æº–å·®: {best_result['bias_std']:.3f}")
        report.append(f"  å¹³å‡è®Šç•°æ•¸: {best_result['variance_mean']:.3f}")
        report.append("")
        
        # å»ºè­°
        report.append("ğŸ’¡ å»ºè­°ï¼š")
        if best_precision < 1e-5:
            report.append("  - ä½¿ç”¨éå¸¸å°çš„ BASIC_PRECISION å€¼å¯èƒ½æœƒå°è‡´æ•¸å€¼ä¸ç©©å®š")
            report.append("  - å»ºè­°ç›£æ§ç³»çµ±çš„æ•¸å€¼ç©©å®šæ€§")
        elif best_precision > 1e-2:
            report.append("  - è¼ƒå¤§çš„ BASIC_PRECISION å€¼å¯èƒ½æœƒé™ä½ç³»çµ±æ•æ„Ÿåº¦")
            report.append("  - è€ƒæ…®æ˜¯å¦éœ€è¦æ›´ç²¾ç´°çš„æ¬Šé‡è¨ˆç®—")
        else:
            report.append("  - ç•¶å‰åƒæ•¸è¨­ç½®åœ¨åˆç†ç¯„åœå…§")
            report.append("  - ç³»çµ±è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ä¿æŒæ­¤è¨­ç½®")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

# å¯è¦–åŒ–å‡½æ•¸
def plot_analysis_results(results, save_path=None):
    """ç¹ªè£½å®Œæ•´çš„åˆ†æçµæœåœ–è¡¨ï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
    
    # æå–æ•¸æ“š
    precisions = list(results.keys())
    
    # å‰µå»º 2x3 å­åœ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('BASIC_PRECISION Parameter Analysis Results', fontsize=16, fontweight='bold')
    
    # å­åœ–1: ç¶œåˆæ€§èƒ½
    ax1 = axes[0, 0]
    composite_scores = [results[p]['composite_score'] for p in precisions]
    reputation_ratios = [results[p]['reputation_effective_ratio'] for p in precisions]
    
    ax1.semilogx(precisions, composite_scores, 'b-o', linewidth=2, markersize=6, label='Composite Score')
    ax1.semilogx(precisions, reputation_ratios, 'r--s', linewidth=2, markersize=5, label='Effective Reputation Ratio')
    ax1.set_xlabel('BASIC_PRECISION')
    ax1.set_ylabel('Score / Ratio')
    ax1.set_title('Overall System Performance')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1.1)
    
    # å­åœ–2: è²è­½åˆ†ä½ˆçµ±è¨ˆ
    ax2 = axes[0, 1]
    rep_means = [results[p]['reputation_mean'] for p in precisions]
    rep_stds = [results[p]['reputation_std'] for p in precisions]
    
    ax2.semilogx(precisions, rep_means, 'g-o', linewidth=2, markersize=6, label='Mean Reputation')
    ax2_twin = ax2.twinx()
    ax2_twin.semilogx(precisions, rep_stds, 'orange', linestyle='--', marker='s', 
                      linewidth=2, markersize=5, label='Reputation Std Dev')
    
    ax2.set_xlabel('BASIC_PRECISION')
    ax2.set_ylabel('Mean Reputation', color='g')
    ax2_twin.set_ylabel('Standard Deviation', color='orange')
    ax2.set_title('Reputation Distribution Statistics')
    ax2.grid(True, alpha=0.3)
    
    # åˆä½µåœ–ä¾‹
    lines1, labels1 = ax2.get_legend_handles_labels()
    lines2, labels2 = ax2_twin.get_legend_handles_labels()
    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    
    # å­åœ–3: ç³»çµ±ç©©å®šæ€§
    ax3 = axes[0, 2]
    stabilities = [results[p]['system_stability'] for p in precisions]
    workload_balances = [results[p]['workload_balance'] for p in precisions]
    
    ax3.semilogx(precisions, stabilities, 'purple', linestyle='-', marker='o', 
                 linewidth=2, markersize=6, label='System Stability')
    ax3.semilogx(precisions, workload_balances, 'brown', linestyle='--', marker='s', 
                 linewidth=2, markersize=5, label='Workload Balance')
    
    ax3.set_xlabel('BASIC_PRECISION')
    ax3.set_ylabel('Stability Score')
    ax3.set_title('System Stability Metrics')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, 1.1)
    
    # å­åœ–4: åå·®æ§åˆ¶
    ax4 = axes[1, 0]
    bias_means = [results[p]['bias_mean'] for p in precisions]
    variance_means = [results[p]['variance_mean'] for p in precisions]
    
    ax4.semilogx(precisions, bias_means, 'red', linestyle='-', marker='o', 
                 linewidth=2, markersize=6, label='Mean Bias')
    ax4_twin = ax4.twinx()
    ax4_twin.semilogx(precisions, variance_means, 'blue', linestyle='--', marker='s', 
                      linewidth=2, markersize=5, label='Mean Variance')
    
    ax4.set_xlabel('BASIC_PRECISION')
    ax4.set_ylabel('Mean Bias', color='red')
    ax4_twin.set_ylabel('Mean Variance', color='blue')
    ax4.set_title('Bias and Variance Control')
    ax4.grid(True, alpha=0.3)
    
    # åˆä½µåœ–ä¾‹
    lines1, labels1 = ax4.get_legend_handles_labels()
    lines2, labels2 = ax4_twin.get_legend_handles_labels()
    ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    
    # å­åœ–5: å€åˆ†èƒ½åŠ›
    ax5 = axes[1, 1]
    discrimination_powers = [results[p]['discrimination_power'] for p in precisions]
    grade_ranges = [results[p]['grade_range'] for p in precisions]
    
    ax5.semilogx(precisions, discrimination_powers, 'teal', linestyle='-', marker='o', 
                 linewidth=2, markersize=6, label='Discrimination Power')
    ax5_twin = ax5.twinx()
    ax5_twin.semilogx(precisions, grade_ranges, 'navy', linestyle='--', marker='s', 
                      linewidth=2, markersize=5, label='Grade Range')
    
    ax5.set_xlabel('BASIC_PRECISION')
    ax5.set_ylabel('Discrimination Power', color='teal')
    ax5_twin.set_ylabel('Grade Range', color='navy')
    ax5.set_title('Discrimination Ability')
    ax5.grid(True, alpha=0.3)
    
    # åˆä½µåœ–ä¾‹
    lines1, labels1 = ax5.get_legend_handles_labels()
    lines2, labels2 = ax5_twin.get_legend_handles_labels()
    ax5.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    
    # å­åœ–6: è²è­½åˆ†æ•¸ç¯„åœ
    ax6 = axes[1, 2]
    rep_ranges = [results[p]['reputation_range'] for p in precisions]
    grade_stds = [results[p]['grade_std'] for p in precisions]
    
    ax6.semilogx(precisions, rep_ranges, 'darkgreen', linestyle='-', marker='o', 
                 linewidth=2, markersize=6, label='Reputation Range')
    ax6_twin = ax6.twinx()
    ax6_twin.semilogx(precisions, grade_stds, 'maroon', linestyle='--', marker='s', 
                      linewidth=2, markersize=5, label='Grade Std Dev')
    
    ax6.set_xlabel('BASIC_PRECISION')
    ax6.set_ylabel('Reputation Range', color='darkgreen')
    ax6_twin.set_ylabel('Grade Std Dev', color='maroon')
    ax6.set_title('Score Distribution Range')
    ax6.grid(True, alpha=0.3)
    
    # åˆä½µåœ–ä¾‹
    lines1, labels1 = ax6.get_legend_handles_labels()
    lines2, labels2 = ax6_twin.get_legend_handles_labels()
    ax6.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"åœ–è¡¨å·²ä¿å­˜è‡³: {save_path}")
    
    plt.show()

def plot_detailed_comparison(results, save_path=None):
    """ç¹ªè£½è©³ç´°å°æ¯”åœ–è¡¨"""
    
    precisions = list(results.keys())
    
    # å‰µå»º 2x2 å­åœ–é€²è¡Œè©³ç´°å°æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('BASIC_PRECISION Detailed Performance Comparison', fontsize=16, fontweight='bold')
    
    # å­åœ–1: æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™å°æ¯”
    ax1 = axes[0, 0]
    composite_scores = [results[p]['composite_score'] for p in precisions]
    reputation_ratios = [results[p]['reputation_effective_ratio'] for p in precisions]
    stabilities = [results[p]['system_stability'] for p in precisions]
    
    ax1.semilogx(precisions, composite_scores, 'b-o', linewidth=2, markersize=6, label='Composite Score')
    ax1.semilogx(precisions, reputation_ratios, 'r--s', linewidth=2, markersize=5, label='Effective Reputation')
    ax1.semilogx(precisions, stabilities, 'g-.^', linewidth=2, markersize=5, label='System Stability')
    
    ax1.set_xlabel('BASIC_PRECISION')
    ax1.set_ylabel('Performance Score')
    ax1.set_title('Core Performance Metrics')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1.1)
    
    # å­åœ–2: æ•¸å€¼ç©©å®šæ€§åˆ†æ
    ax2 = axes[0, 1]
    variance_means = [results[p]['variance_mean'] for p in precisions]
    bias_means = [results[p]['bias_mean'] for p in precisions]
    
    ax2.loglog(precisions, variance_means, 'purple', linestyle='-', marker='o', 
               linewidth=2, markersize=6, label='Mean Variance')
    ax2.loglog(precisions, np.array(bias_means) + 1e-6, 'orange', linestyle='--', marker='s', 
               linewidth=2, markersize=5, label='Mean Bias')
    
    ax2.set_xlabel('BASIC_PRECISION')
    ax2.set_ylabel('Numerical Values')
    ax2.set_title('Numerical Stability Analysis')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å­åœ–3: ç³»çµ±éŸ¿æ‡‰æ•æ„Ÿåº¦
    ax3 = axes[1, 0]
    discrimination_powers = [results[p]['discrimination_power'] for p in precisions]
    rep_stds = [results[p]['reputation_std'] for p in precisions]
    
    ax3.semilogx(precisions, discrimination_powers, 'teal', linestyle='-', marker='o', 
                 linewidth=2, markersize=6, label='Discrimination Power')
    ax3.semilogx(precisions, rep_stds, 'navy', linestyle='--', marker='s', 
                 linewidth=2, markersize=5, label='Reputation Std Dev')
    
    ax3.set_xlabel('BASIC_PRECISION')
    ax3.set_ylabel('Sensitivity Measure')
    ax3.set_title('System Response Sensitivity')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # å­åœ–4: å·¥ä½œè² è¼‰èˆ‡å¹³è¡¡æ€§
    ax4 = axes[1, 1]
    workload_balances = [results[p]['workload_balance'] for p in precisions]
    grade_ranges = [results[p]['grade_range'] / 50.0 for p in precisions]  # æ¨™æº–åŒ–
    
    ax4.semilogx(precisions, workload_balances, 'brown', linestyle='-', marker='o', 
                 linewidth=2, markersize=6, label='Workload Balance')
    ax4.semilogx(precisions, grade_ranges, 'darkred', linestyle='--', marker='s', 
                 linewidth=2, markersize=5, label='Grade Range (norm.)')
    
    ax4.set_xlabel('BASIC_PRECISION')
    ax4.set_ylabel('Balance Score')
    ax4.set_title('Load Balance & Distribution')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim(0, 1.1)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"è©³ç´°å°æ¯”åœ–è¡¨å·²ä¿å­˜è‡³: {save_path}")
    
    plt.show()

def generate_comprehensive_analysis(with_plots=True):
    """ç”Ÿæˆå®Œæ•´çš„ BASIC_PRECISION åˆ†æ"""
    
    # å‰µå»ºåˆ†æå™¨
    logger = LoggerPrint()
    analyzer = BasicPrecisionAnalyzer(logger)
    
    # é‹è¡Œåˆ†æ
    results = analyzer.run_analysis()
    
    # ç”Ÿæˆå ±å‘Š
    report = analyzer.generate_summary_report()
    logger.print(report)
    
    # å¯è¦–åŒ–çµæœ
    if with_plots and results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç¶œåˆåˆ†æåœ–è¡¨
        plot_save_path = f"basic_precision_analysis_visualization_{timestamp}.png"
        plot_analysis_results(results, plot_save_path)
        
        # è©³ç´°å°æ¯”åœ–è¡¨
        detailed_save_path = f"basic_precision_detailed_comparison_{timestamp}.png"
        plot_detailed_comparison(results, detailed_save_path)
    
    return results, analyzer

def main():
    """ä¸»å‡½æ•¸"""
    print("é–‹å§‹ BASIC_PRECISION åƒæ•¸æœ€ä½³åŒ–åˆ†æ...")
    results, analyzer = generate_comprehensive_analysis(with_plots=True)
    
    if results:
        print("\nåˆ†æå®Œæˆï¼è«‹æŸ¥çœ‹ç”Ÿæˆçš„åœ–è¡¨å’Œæ—¥èªŒæ–‡ä»¶ã€‚")
    else:
        print("\nåˆ†æå¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯ã€‚")

if __name__ == "__main__":
    main()
