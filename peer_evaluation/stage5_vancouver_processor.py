#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vancouver ç®—æ³•è™•ç†å™¨ - åŒå„•è©•åˆ†ç³»çµ±
ä½¿ç”¨æ”¶é›†åˆ°çš„è©•åˆ†æ•¸æ“šåŸ·è¡ŒVancouverç®—æ³•è¨ˆç®—æœ€çµ‚æˆç¸¾
"""

import json
import sys
import os
import re
import numpy as np
import pandas as pd
import argparse
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from stage0_config_unified import PeerEvaluationConfig

# å°å…¥æ ¸å¿ƒVancouverç®—æ³•
try:
    from core.vancouver import Graph
    VANCOUVER_CORE_AVAILABLE = True
except ImportError:
    VANCOUVER_CORE_AVAILABLE = False
    print("è­¦å‘Š: ç„¡æ³•å°å…¥æ ¸å¿ƒVancouverç®—æ³•ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–å¯¦ç¾")

class User:
    """ç”¨æˆ¶é¡"""
    def __init__(self, name):
        self.name = name
        self.items = set()
        self.grade = {}
        self.variance = None
        self.reputation = None
        self.incentive_weight = None
        
    def add_item(self, item, grade):
        self.items = self.items | set([item])
        self.grade[item] = grade

class Item:
    """é …ç›®é¡"""
    def __init__(self, id):
        self.id = id
        self.users = set()
        self.grade = None
        self.variance = None
    
    def add_user(self, user):
        self.users = self.users | set([user])

class EnhancedGraph:
    """å®Œæ•´çš„Vancouverç®—æ³•å¯¦ç¾ï¼ŒåŒ…å«è²è­½ç³»çµ±å’Œæ¿€å‹µæ©Ÿåˆ¶"""
    
    def __init__(self, R_max=1.0, v_G=8.0, alpha=0.1, N=4, basic_precision=0.0001):
        self.R_max = R_max        # æœ€å¤§è²è­½å€¼
        self.v_G = v_G           # èª¤å·®å®¹å¿ä¸Šé™
        self.alpha = alpha       # è©•å¯©æˆåˆ†ä½”æ¯”
        self.N = N               # ç³»çµ±æŒ‡å®šçš„æœ€å°‘è©•å¯©æ•¸é‡
        self.lambda_param = R_max / v_G  # æ‡²ç½°æ–œç‡
        self.basic_precision = basic_precision
        
        # æ•¸æ“šçµæ§‹
        self.items = set()
        self.users = set()
        self.user_dict = {}
        self.item_dict = {}
        
        print(f"âœ… ä½¿ç”¨å®Œæ•´Vancouverç®—æ³•å¯¦ç¾")
        print(f"   - è²è­½ç³»çµ±: R_max={R_max}, v_G={v_G}, Î»={self.lambda_param:.3f}")
        print(f"   - æ¿€å‹µæ©Ÿåˆ¶: Î±={alpha}, N={N}")
        
    def add_review(self, username, item_id, grade):
        """æ·»åŠ è©•åˆ†"""
        # ç²å–æˆ–å‰µå»ºç”¨æˆ¶
        if username in self.user_dict:
            u = self.user_dict[username]
        else:
            u = User(username)
            self.user_dict[username] = u
            self.users = self.users | set([u])
        
        # ç²å–æˆ–å‰µå»ºé …ç›®
        if item_id in self.item_dict:
            it = self.item_dict[item_id]
        else:
            it = Item(item_id)
            self.item_dict[item_id] = it
            self.items = self.items | set([it])
        
        # æ·»åŠ é€£æ¥
        u.add_item(it, grade)
        it.add_user(u)
    
    def evaluate_items(self, n_iterations=25):
        """è©•ä¼°é …ç›®ï¼ˆå…±è­˜åˆ†æ•¸è¨ˆç®—ï¼‰"""
        # ç°¡åŒ–çš„å…±è­˜åˆ†æ•¸è¨ˆç®—ï¼šä½¿ç”¨åŠ æ¬Šå¹³å‡
        for item in self.items:
            if item.users:
                # æ”¶é›†æ‰€æœ‰è©•åˆ†
                grades = []
                weights = []
                for user in item.users:
                    if item in user.grade:
                        grade = user.grade[item]
                        # ä½¿ç”¨ç”¨æˆ¶è²è­½ä½œç‚ºæ¬Šé‡ï¼ˆå¦‚æœå·²è¨ˆç®—ï¼‰
                        weight = getattr(user, 'reputation', 1.0)
                        grades.append(grade)
                        weights.append(weight)
                
                if grades:
                    # è¨ˆç®—åŠ æ¬Šå¹³å‡ä½œç‚ºå…±è­˜åˆ†æ•¸
                    weighted_sum = sum(g * w for g, w in zip(grades, weights))
                    total_weight = sum(weights)
                    item.grade = weighted_sum / total_weight if total_weight > 0 else np.mean(grades)
                    
                    # è¨ˆç®—è®Šç•°æ•¸
                    if len(grades) > 1:
                        mean_grade = item.grade
                        item.variance = np.var(grades)
                    else:
                        item.variance = 0.0
                else:
                    item.grade = 0.0
                    item.variance = 0.0
    
    def evaluate_users(self):
        """è©•ä¼°ç”¨æˆ¶ï¼ˆè®Šç•°æ•¸è¨ˆç®—ï¼‰"""
        for user in self.users:
            if user.grade:
                # æ”¶é›†ç”¨æˆ¶çµ¦å‡ºçš„æ‰€æœ‰è©•åˆ†
                user_grades = list(user.grade.values())
                
                # è¨ˆç®—ç”¨æˆ¶è©•åˆ†èˆ‡å…±è­˜çš„å·®ç•°
                differences = []
                for item in user.items:
                    if item.grade is not None and item in user.grade:
                        user_score = user.grade[item]
                        consensus_score = item.grade
                        diff = abs(user_score - consensus_score)
                        differences.append(diff)
                
                # è¨ˆç®—ç”¨æˆ¶è®Šç•°æ•¸ï¼ˆè©•åˆ†ä¸€è‡´æ€§ï¼‰
                if differences:
                    user.variance = np.mean(differences) ** 2
                else:
                    user.variance = 0.0
            else:
                user.variance = float('inf')  # æ²’æœ‰è©•åˆ†çš„ç”¨æˆ¶
    
    def calculate_reputation_scores(self):
        """è¨ˆç®—è²è­½åˆ†æ•¸ R_j = max(0, R_max - Î» * âˆšvÌ‚_j)"""
        for user in self.users:
            if user.variance is not None and user.variance != float('inf'):
                # è¨ˆç®—è²è­½åˆ†æ•¸
                variance_sqrt = np.sqrt(max(0, user.variance))
                user.reputation = max(0.0, self.R_max - self.lambda_param * variance_sqrt)
            else:
                user.reputation = 0.0  # æ²’æœ‰è©•åˆ†æˆ–è®Šç•°æ•¸ç„¡é™å¤§
    
    def calculate_incentive_weights(self):
        """è¨ˆç®—æ¿€å‹µæ¬Šé‡ Î¸_j = min(m_j, N)/N * R_j"""
        for user in self.users:
            # è¨ˆç®—è©•å¯©æ•¸é‡
            m_j = len(user.items)
            # è¨ˆç®—æ¿€å‹µæ¬Šé‡
            participation_factor = min(m_j, self.N) / self.N
            reputation_factor = getattr(user, 'reputation', 0.0)
            user.incentive_weight = participation_factor * reputation_factor
    
    def calculate_final_grades(self):
        """è¨ˆç®—æœ€çµ‚æˆç¸¾ FinalGrade_j = (1-Î±)*qÌ‚_j + Î±*Î¸_j*100"""
        final_grades = {}
        
        for user in self.users:
            # æ‰¾åˆ°è©²ç”¨æˆ¶è¢«è©•åˆ†çš„é …ç›®ï¼ˆå‡è¨­ç”¨æˆ¶IDèˆ‡é …ç›®IDå°æ‡‰ï¼‰
            user_item = None
            for item in self.items:
                if str(item.id) == str(user.name) or item.id == user.name:
                    user_item = item
                    break
            
            if user_item is not None and user_item.grade is not None:
                # è€ƒå·å…±è­˜åˆ†æ•¸
                q_hat = user_item.grade
                # æ¿€å‹µæ¬Šé‡
                theta = getattr(user, 'incentive_weight', 0.0)
                
                # è¨ˆç®—åŸå§‹åŠ æ¬Šæˆç¸¾
                weighted_grade = (1 - self.alpha) * q_hat + self.alpha * theta * 100
                
                # ä¿è­·æ©Ÿåˆ¶ï¼šå¦‚æœåŠ æ¬Šå¾Œåˆ†æ•¸é™ä½ï¼Œæ¡ç”¨è¼ƒé«˜åˆ†æ•¸
                final_grade = max(q_hat, weighted_grade)
                
                # è¨˜éŒ„æ˜¯å¦ä½¿ç”¨äº†ä¿è­·æ©Ÿåˆ¶
                protection_used = final_grade == q_hat and weighted_grade < q_hat
                
                final_grades[user.name] = {
                    'consensus_score': float(q_hat),
                    'incentive_weight': float(theta),
                    'final_grade': float(final_grade),
                    'weighted_grade': float(weighted_grade),
                    'protection_used': bool(protection_used),
                    'reputation': float(getattr(user, 'reputation', 0.0)),
                    'variance': float(getattr(user, 'variance', 0.0))
                }
        
        return final_grades
    
    def evaluate_with_reputation(self, n_iterations=25):
        """å®Œæ•´çš„è©•ä¼°æµç¨‹ï¼ŒåŒ…å«è²è­½ç³»çµ±"""
        print("  ğŸ”„ åŸ·è¡ŒåŸºæœ¬Vancouverç®—æ³•è©•ä¼°...")
        
        # 1. åˆå§‹è²è­½è¨­å®šï¼ˆæ‰€æœ‰ç”¨æˆ¶é–‹å§‹æ™‚è²è­½ç›¸ç­‰ï¼‰
        for user in self.users:
            user.reputation = self.R_max
        
        # 2. è¿­ä»£è©•ä¼°éç¨‹
        for iteration in range(n_iterations):
            # è©•ä¼°é …ç›®ï¼ˆè¨ˆç®—å…±è­˜åˆ†æ•¸ï¼‰
            self.evaluate_items(1)
            
            # è©•ä¼°ç”¨æˆ¶ï¼ˆè¨ˆç®—è®Šç•°æ•¸ï¼‰
            self.evaluate_users()
            
            # æ›´æ–°è²è­½åˆ†æ•¸
            self.calculate_reputation_scores()
            
            # æ¯5æ¬¡è¿­ä»£è¼¸å‡ºé€²åº¦
            if (iteration + 1) % 5 == 0:
                avg_reputation = np.mean([u.reputation for u in self.users])
                print(f"    è¿­ä»£ {iteration + 1}/{n_iterations}: å¹³å‡è²è­½ = {avg_reputation:.3f}")
        
        print("  âœ… åŸºæœ¬ç®—æ³•è©•ä¼°å®Œæˆ")
        
        # 3. è¨ˆç®—æ¿€å‹µæ¬Šé‡
        print("  ğŸ¯ è¨ˆç®—æ¿€å‹µæ¬Šé‡...")
        self.calculate_incentive_weights()
        
        # 4. è¨ˆç®—æœ€çµ‚æˆç¸¾
        print("  ğŸ“Š è¨ˆç®—æœ€çµ‚æˆç¸¾...")
        final_grades = self.calculate_final_grades()
        
        # 5. è¼¸å‡ºçµ±è¨ˆä¿¡æ¯
        if final_grades:
            avg_reputation = np.mean([g['reputation'] for g in final_grades.values()])
            avg_incentive = np.mean([g['incentive_weight'] for g in final_grades.values()])
            protection_count = sum(1 for g in final_grades.values() if g['protection_used'])
            
            print(f"  ğŸ“ˆ ç®—æ³•çµ±è¨ˆ:")
            print(f"    - å¹³å‡è²è­½åˆ†æ•¸: {avg_reputation:.3f}")
            print(f"    - å¹³å‡æ¿€å‹µæ¬Šé‡: {avg_incentive:.3f}")
            print(f"    - ä¿è­·æ©Ÿåˆ¶ä½¿ç”¨æ¬¡æ•¸: {protection_count}")
        
        return final_grades

# å˜—è©¦å°å…¥çœŸå¯¦å¯¦ç¾ï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨å‚™ç”¨å¯¦ç¾
# EnhancedGraph é¡å·²åœ¨æœ¬æ–‡ä»¶ä¸­é‡æ–°å¯¦ç¾
print("ä½¿ç”¨æœ¬åœ°å®Œæ•´Vancouverç®—æ³•å¯¦ç¾")

class VancouverProcessor:
    """Vancouverç®—æ³•è™•ç†å™¨"""
    
    def __init__(self, evaluation_data_path=None, preset_name: str = None):
        """
        åˆå§‹åŒ–è™•ç†å™¨
        
        Args:
            evaluation_data_path: è©•åˆ†æ•¸æ“šJSONæ–‡ä»¶è·¯å¾‘
            preset_name: é è¨­é…ç½®åç¨±
        """
        self.config = PeerEvaluationConfig()
        self.preset_name = preset_name
        self.evaluation_data_path = evaluation_data_path
        self.raw_data = None
        self.students = []
        self.submissions = []
        self.graph = None
        self.output_dir = None  # æ–°å¢è¼¸å‡ºç›®éŒ„å±¬æ€§
        
    def load_evaluation_data(self):
        """è¼‰å…¥è©•åˆ†æ•¸æ“š"""
        print("è¼‰å…¥è©•åˆ†æ•¸æ“š...")
        
        with open(self.evaluation_data_path, 'r', encoding='utf-8') as f:
            self.raw_data = json.load(f)
        
        # æ¨™æº–åŒ–è³‡æ–™éµåï¼šæ”¯æ´ evaluations æˆ– evaluation_data
        if 'evaluations' in self.raw_data and 'evaluation_data' not in self.raw_data:
            self.raw_data['evaluation_data'] = self.raw_data['evaluations']
        
        # æª¢æŸ¥ summary_stats æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œå˜—è©¦ä½¿ç”¨ summary
        if 'summary_stats' not in self.raw_data and 'summary' in self.raw_data:
            self.raw_data['summary_stats'] = self.raw_data['summary']
            
        # æ·»åŠ ç¼ºå¤±çš„çµ±è¨ˆæ•¸æ“šï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'summary_stats' not in self.raw_data:
            print("è­¦å‘Š: æ‰¾ä¸åˆ°æ‘˜è¦çµ±è¨ˆè³‡æ–™ï¼Œå°‡è‡ªå‹•ç”ŸæˆåŸºæœ¬çµ±è¨ˆ")
            self._generate_basic_summary_stats()
            
        # è£œå……å¯èƒ½ç¼ºå¤±çš„çµ±è¨ˆè³‡æ–™
        stats = self.raw_data['summary_stats']
        if 'total_evaluators' not in stats:
            evaluators = {eval_data['evaluator_id'] for eval_data in self.raw_data['evaluation_data']}
            stats['total_evaluators'] = len(evaluators)
            
        if 'total_evaluations' not in stats:
            stats['total_evaluations'] = len(self.raw_data['evaluation_data'])
            
        if 'overall_stats' not in stats:
            stats['overall_stats'] = {'mean': stats.get('average_score', 0)}
        
        print(f"æˆåŠŸè¼‰å…¥æ•¸æ“š:")
        print(f"   - è©•åˆ†è€…æ•¸é‡: {stats.get('total_evaluators', 0)}")
        print(f"   - è©•åˆ†ç¸½æ•¸: {stats.get('total_evaluations', 0)}")
        print(f"   - åˆ†æ•¸ç¸½æ•¸: {stats.get('total_scores', 0)}")
        print(f"   - æ•´é«”å¹³å‡åˆ†: {stats.get('overall_stats', {}).get('mean', 0):.2f}")
        
    def extract_students_and_submissions(self):
        """æå–å­¸ç”Ÿå’Œä½œæ¥­æ¸…å–®"""
        print("\nğŸ“‹ æå–å­¸ç”Ÿå’Œä½œæ¥­æ¸…å–®...")
        
        # å¾è©•åˆ†æ•¸æ“šä¸­æå–æ‰€æœ‰å­¸ç”ŸID
        evaluators = set()
        evaluatees = set()
        
        for evaluation in self.raw_data.get('evaluation_data', []):
            if 'evaluator_id' in evaluation:
                evaluators.add(evaluation['evaluator_id'])
                
            # æª¢æŸ¥ evaluations æ ¼å¼
            evaluations = evaluation.get('evaluations', {})
            if isinstance(evaluations, dict):
                for evaluatee_id in evaluations.keys():
                    evaluatees.add(evaluatee_id)
                    
            # å˜—è©¦è™•ç†å¯èƒ½çš„æ›¿ä»£æ ¼å¼
            elif 'evaluatee_id' in evaluation:
                evaluatees.add(evaluation['evaluatee_id'])
        
        # åˆä½µæ‰€æœ‰å­¸ç”ŸIDï¼ˆè©•åˆ†è€…å’Œè¢«è©•åˆ†è€…æ‡‰è©²æ˜¯åŒä¸€ç¾¤å­¸ç”Ÿï¼‰
        all_students = evaluators.union(evaluatees)
        self.students = sorted(list(all_students))
        
        # æ¯å€‹å­¸ç”Ÿçš„ä½œæ¥­å°±æ˜¯å…¶IDï¼ˆå‡è¨­æ¯å€‹å­¸ç”Ÿéƒ½æäº¤äº†ä¸€ä»½ä½œæ¥­ï¼‰
        self.submissions = self.students.copy()
        
        print(f"æå–å®Œæˆ:")
        print(f"   - å­¸ç”Ÿæ•¸é‡: {len(self.students)}")
        print(f"   - ä½œæ¥­æ•¸é‡: {len(self.submissions)}")
        print(f"   - å­¸ç”Ÿæ¸…å–®: {', '.join(self.students)}")
        
    def create_vancouver_graph(self):
        """å‰µå»ºVancouverç®—æ³•åœ–çµæ§‹"""
        print("\nğŸ”— å‰µå»ºVancouverç®—æ³•åœ–çµæ§‹...")
        
        # åˆå§‹åŒ–åœ–ï¼Œä½¿ç”¨åˆé©çš„åƒæ•¸
        self.graph = EnhancedGraph(
            R_max=1.0,      # æœ€å¤§è²è­½å€¼
            v_G=8.0,        # èª¤å·®å®¹å¿ä¸Šé™  
            alpha=0.1,      # è©•å¯©æˆåˆ†ä½”æ¯”
            N=4             # ç³»çµ±æŒ‡å®šçš„æœ€å°‘è©•å¯©æ•¸é‡
        )
        
        print(f"åœ–çµæ§‹åˆå§‹åŒ–å®Œæˆ")
        
    def add_evaluations_to_graph(self):
        """å°‡è©•åˆ†æ•¸æ“šæ·»åŠ åˆ°åœ–ä¸­ - ä½¿ç”¨Q1~Q5çš„åŠ ç¸½åˆ†æ•¸"""
        print("\nğŸ“Š æ·»åŠ è©•åˆ†æ•¸æ“šåˆ°åœ–ä¸­ï¼ˆä½¿ç”¨Q1~Q5åŠ ç¸½åˆ†æ•¸ï¼‰...")
        
        total_evaluations = 0
        
        for evaluation in self.raw_data['evaluation_data']:
            evaluator_id = evaluation.get('evaluator_id')
            if not evaluator_id:
                continue
                
            # è™•ç†æ–°çš„ç°¡åŒ–æ ¼å¼ï¼ševaluations ç›´æ¥æ˜¯ {target_id: total_score}
            if 'evaluations' in evaluation and isinstance(evaluation['evaluations'], dict):
                for evaluatee_id, score_value in evaluation['evaluations'].items():
                    # å¦‚æœæ˜¯æ•¸å­—ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆé€™æ˜¯ç¸½åˆ†ï¼‰
                    if isinstance(score_value, (int, float)):
                        self.graph.add_review(evaluator_id, evaluatee_id, score_value)
                        total_evaluations += 1
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå˜—è©¦å¾ details è¨ˆç®—ï¼ˆèˆŠæ ¼å¼ï¼‰
                    elif isinstance(score_value, dict) and 'details' in score_value:
                        # å¾detailsè¨ˆç®—Q1~Q5åˆ†æ•¸çš„åŠ ç¸½
                        q_scores = []
                        details = score_value['details']
                        for q_num in range(1, 6):
                            score_key = f"{evaluatee_id}_Q{q_num}_score"
                            if score_key in details:
                                q_scores.append(details[score_key])
                        
                        if len(q_scores) == 5:  # ç¢ºä¿æœ‰å®Œæ•´çš„Q1~Q5åˆ†æ•¸
                            total_score = sum(q_scores)
                            self.graph.add_review(evaluator_id, evaluatee_id, total_score)
                            total_evaluations += 1
                            
                            # é©—è­‰ï¼šæ¯”è¼ƒè¨ˆç®—å‡ºçš„åŠ ç¸½èˆ‡åŸå§‹å¹³å‡åˆ†æ•¸
                            original_score = score_value.get('score', 0)
                            expected_average = total_score / 5
                            if abs(expected_average - original_score) > 0.01:
                                print(f"âš ï¸  {evaluator_id}è©•{evaluatee_id}: å¹³å‡åˆ†é©—è­‰å¤±æ•— (è¨ˆç®—å¹³å‡:{expected_average:.2f} vs åŸå§‹:{original_score})")
                        else:
                            print(f"âš ï¸  {evaluator_id}è©•{evaluatee_id}: Q1~Q5åˆ†æ•¸ä¸å®Œæ•´ ({len(q_scores)}/5)")
            
            # å‚™ç”¨è™•ç†ï¼šå¦‚æœæ²’æœ‰evaluationsæ¬„ä½ï¼Œå¾scoresç›´æ¥è¨ˆç®—
            elif 'scores' in evaluation and isinstance(evaluation['scores'], dict):
                # æŒ‰å­¸ç”ŸIDåˆ†çµ„è¨ˆç®—Q1~Q5åŠ ç¸½
                student_scores = {}
                for score_key, score_value in evaluation['scores'].items():
                    if '_Q' in score_key and score_key.endswith('_score'):
                        # æå–å­¸ç”ŸID (ä¾‹å¦‚å¾ "C03_Q1_score" æå– "C03")
                        parts = score_key.split('_Q')
                        if len(parts) == 2:
                            student_id = parts[0]
                            if student_id not in student_scores:
                                student_scores[student_id] = []
                            student_scores[student_id].append(score_value)
                
                # ç‚ºæ¯å€‹å­¸ç”Ÿæ·»åŠ åŠ ç¸½åˆ†æ•¸
                for student_id, scores in student_scores.items():
                    if len(scores) == 5:  # ç¢ºä¿æœ‰Q1~Q5å…¨éƒ¨åˆ†æ•¸
                        total_score = sum(scores)
                        self.graph.add_review(evaluator_id, student_id, total_score)
                        total_evaluations += 1
                    else:
                        print(f"âš ï¸  {evaluator_id}è©•{student_id}: å•é¡Œæ•¸é‡ä¸å®Œæ•´ ({len(scores)}/5)")
        
        print(f"è©•åˆ†æ•¸æ“šæ·»åŠ å®Œæˆ:")
        print(f"   - ç¸½è©•åˆ†æ•¸: {total_evaluations}")
        print(f"   - ç”¨æˆ¶æ•¸é‡: {len(self.graph.users)}")
        print(f"   - é …ç›®æ•¸é‡: {len(self.graph.items)}")
        
    def run_vancouver_algorithm(self):
        """åŸ·è¡ŒVancouverç®—æ³•"""
        print("\nğŸš€ åŸ·è¡ŒVancouverç®—æ³•...")
        
        # åŸ·è¡Œå®Œæ•´çš„è©•ä¼°æµç¨‹
        final_grades = self.graph.evaluate_with_reputation(n_iterations=25)
        
        print(f"Vancouverç®—æ³•åŸ·è¡Œå®Œæˆ")
        print(f"   - è¨ˆç®—äº† {len(final_grades)} å€‹æœ€çµ‚æˆç¸¾")
        
        return final_grades
        
    def generate_detailed_report(self, final_grades):
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        print("\nç”Ÿæˆè©³ç´°å ±å‘Š...")
        
        # å‰µå»ºå ±å‘Šæ•¸æ“š
        report_data = []
        
        for student_id in self.students:
            if student_id in final_grades:
                grade_info = final_grades[student_id]
                
                # ç²å–è©²å­¸ç”Ÿçš„åŸå§‹è©•åˆ†çµ±è¨ˆï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                student_stats = {'mean': 0, 'count': 0}  # é è¨­å€¼
                
                # å˜—è©¦å¾ä¸åŒå¯èƒ½çš„ä½ç½®ç²å–è©•åˆ†çµ±è¨ˆ
                if 'summary_stats' in self.raw_data:
                    summary = self.raw_data['summary_stats']
                    if 'score_by_evaluatee' in summary and student_id in summary['score_by_evaluatee']:
                        student_stats = summary['score_by_evaluatee'][student_id]
                    elif 'score_by_question' in summary and student_id in summary['score_by_question']:
                        # å˜—è©¦ä½¿ç”¨æ›¿ä»£æ ¼å¼
                        student_stats = summary['score_by_question'][student_id]
                
                # æº–å‚™å ±å‘Šæ•¸æ“šï¼Œç¢ºä¿æ‰€æœ‰éœ€è¦çš„éµéƒ½å­˜åœ¨ï¼Œä½¿ç”¨é è¨­å€¼è™•ç†ç¼ºå¤±çš„éµ
                report_entry = {
                    'student_id': student_id,
                    'original_avg_score': student_stats.get('mean', 0),
                    'consensus_score': grade_info.get('consensus_score', 0),
                    'final_grade': grade_info.get('final_grade', 0),  # ç¢ºä¿ final_grade å­˜åœ¨
                    'evaluation_count': student_stats.get('count', 0)
                }
                
                # æ·»åŠ å¯èƒ½å­˜åœ¨çš„é¡å¤–éµ
                if 'incentive_weight' in grade_info:
                    report_entry['incentive_weight'] = grade_info['incentive_weight']
                else:
                    report_entry['incentive_weight'] = 1.0  # é è¨­å€¼
                    
                if 'weighted_grade' in grade_info:
                    report_entry['weighted_grade'] = grade_info['weighted_grade']
                else:
                    report_entry['weighted_grade'] = report_entry['consensus_score']  # ä½¿ç”¨å…±è­˜åˆ†æ•¸ä½œç‚ºé è¨­å€¼
                    
                if 'protection_used' in grade_info:
                    report_entry['protection_used'] = grade_info['protection_used']
                else:
                    report_entry['protection_used'] = False  # é è¨­å€¼
                    
                if 'reputation' in grade_info:
                    report_entry['reputation'] = grade_info['reputation']
                else:
                    report_entry['reputation'] = 0.5  # é è¨­å€¼
                    
                if 'variance' in grade_info:
                    report_entry['variance'] = grade_info['variance']
                else:
                    report_entry['variance'] = 0  # é è¨­å€¼
                    
                report_data.append(report_entry)
        
        # è½‰æ›ç‚ºDataFrameä¾¿æ–¼åˆ†æ
        df = pd.DataFrame(report_data)
        
        # æ’åºï¼ˆæŒ‰æœ€çµ‚æˆç¸¾é™åºï¼‰- åªæœ‰ç•¶ 'final_grade' æ¬„ä½å­˜åœ¨æ™‚
        if not df.empty and 'final_grade' in df.columns:
            df = df.sort_values('final_grade', ascending=False)
        else:
            print("è­¦å‘Š: 'final_grade' æ¬„ä½ä¸å­˜åœ¨ï¼Œç„¡æ³•æ’åº")
        
        return df
        
    def save_results(self, final_grades, report_df):
        """ä¿å­˜çµæœåˆ°æ–‡ä»¶"""
        print("\nä¿å­˜çµæœ...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è½‰æ›final_gradesä¸­çš„numpyé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹
        serializable_grades = {}
        for student_id, grade_info in final_grades.items():
            # è™•ç†å¯èƒ½ç¼ºå¤±çš„æ¬„ä½ï¼Œæä¾›é è¨­å€¼
            serializable_grades[student_id] = {
                'consensus_score': float(grade_info.get('consensus_score', grade_info.get('final_grade', 0))) if hasattr(grade_info.get('consensus_score', grade_info.get('final_grade', 0)), 'item') else grade_info.get('consensus_score', grade_info.get('final_grade', 0)),
                'incentive_weight': float(grade_info.get('incentive_weight', 1.0)) if hasattr(grade_info.get('incentive_weight', 1.0), 'item') else grade_info.get('incentive_weight', 1.0),
                'final_grade': float(grade_info.get('final_grade', grade_info.get('consensus_score', 0))) if hasattr(grade_info.get('final_grade', grade_info.get('consensus_score', 0)), 'item') else grade_info.get('final_grade', grade_info.get('consensus_score', 0)),
                'weighted_grade': float(grade_info.get('weighted_grade', grade_info.get('final_grade', grade_info.get('consensus_score', 0)))) if hasattr(grade_info.get('weighted_grade', grade_info.get('final_grade', grade_info.get('consensus_score', 0))), 'item') else grade_info.get('weighted_grade', grade_info.get('final_grade', grade_info.get('consensus_score', 0))),
                'protection_used': bool(grade_info.get('protection_used', False)),
                'reputation': float(grade_info.get('reputation', 0.5)) if hasattr(grade_info.get('reputation', 0.5), 'item') else grade_info.get('reputation', 0.5),
                'variance': float(grade_info.get('variance', 0.0)) if hasattr(grade_info.get('variance', 0.0), 'item') else grade_info.get('variance', 0.0)
            }
        
        # ä¿å­˜è©³ç´°çµæœï¼ˆJSONæ ¼å¼ï¼‰
        results = {
            'processing_time': datetime.now().isoformat(),
            'algorithm_parameters': {
                'R_max': float(self.graph.R_max),
                'v_G': float(self.graph.v_G),
                'alpha': float(self.graph.alpha),
                'N': int(self.graph.N)
            },
            'final_grades': serializable_grades,
            'summary_statistics': {
                'total_students': len(final_grades),
                'avg_final_grade': float(np.mean([g['final_grade'] for g in final_grades.values()])),
                'std_final_grade': float(np.std([g['final_grade'] for g in final_grades.values()])),
                'avg_consensus_score': float(np.mean([g['consensus_score'] for g in final_grades.values()])),
                'avg_reputation': float(np.mean([g['reputation'] for g in final_grades.values()])),
                'protection_used_count': int(sum(1 for g in final_grades.values() if g['protection_used']))
            }
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_dir = self.config.ensure_output_dir('stage6_vancouver')
        
        # ä½¿ç”¨é…ç½®ç”Ÿæˆè¼¸å‡ºè·¯å¾‘
        json_path = self.config.get_path('stage6_vancouver', 'vancouver_results_json', timestamp=datetime.now())
        excel_path = self.config.get_path('stage6_vancouver', 'vancouver_results_xlsx', timestamp=datetime.now())
        
        # ä¿å­˜JSONçµæœ
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Excelå ±å‘Š
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # ä¸»è¦å ±å‘Š
            report_df.to_excel(writer, sheet_name='Final_Grades', index=False)
            
            # æ‘˜è¦çµ±è¨ˆ
            summary_data = [
                ['ç¸½å­¸ç”Ÿæ•¸', len(final_grades)],
                ['å¹³å‡æœ€çµ‚æˆç¸¾', float(np.mean([g['final_grade'] for g in final_grades.values()]))],
                ['æœ€çµ‚æˆç¸¾æ¨™æº–å·®', float(np.std([g['final_grade'] for g in final_grades.values()]))],
                ['å¹³å‡å…±è­˜åˆ†æ•¸', float(np.mean([g['consensus_score'] for g in final_grades.values()]))],
                ['å¹³å‡è²è­½åˆ†æ•¸', float(np.mean([g['reputation'] for g in final_grades.values()]))],
                ['ä½¿ç”¨ä¿è­·æ©Ÿåˆ¶æ•¸', int(sum(1 for g in final_grades.values() if g['protection_used']))]
            ]
            summary_df = pd.DataFrame(summary_data, columns=['æŒ‡æ¨™', 'æ•¸å€¼'])
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        print(f"çµæœä¿å­˜å®Œæˆ:")
        print(f"   - JSONçµæœ: {json_path}")
        print(f"   - Excelå ±å‘Š: {excel_path}")
        
        return json_path, excel_path
        
    def print_top_results(self, report_df, top_n=10):
        """æ‰“å°å‰Nåçµæœ"""
        print(f"\nğŸ† å‰ {top_n} åå­¸ç”Ÿæœ€çµ‚æˆç¸¾:")
        print("=" * 100)
        print(f"{'æ’å':<4} {'å­¸ç”ŸID':<8} {'åŸå§‹å‡åˆ†':<10} {'å…±è­˜åˆ†æ•¸':<10} {'è²è­½åˆ†æ•¸':<10} {'æœ€çµ‚æˆç¸¾':<10} {'ä¿è­·æ©Ÿåˆ¶':<8}")
        print("-" * 100)
        
        for i, row in report_df.head(top_n).iterrows():
            rank = list(report_df.index).index(i) + 1
            protection = "æ˜¯" if row['protection_used'] else "å¦"
            print(f"{rank:<4} {row['student_id']:<8} {row['original_avg_score']:<10.2f} "
                  f"{row['consensus_score']:<10.2f} {row['reputation']:<10.3f} "
                  f"{row['final_grade']:<10.2f} {protection:<8}")
        
    def process_complete_workflow(self):
        """åŸ·è¡Œå®Œæ•´çš„è™•ç†æµç¨‹"""
        print("Vancouver åŒå„•è©•åˆ†ç³»çµ± - å®Œæ•´è™•ç†æµç¨‹")
        print("=" * 60)
        
        # 1. è¼‰å…¥æ•¸æ“š
        self.load_evaluation_data()
        
        # 2. æå–å­¸ç”Ÿå’Œä½œæ¥­
        self.extract_students_and_submissions()
        
        # 3. å‰µå»ºåœ–çµæ§‹
        self.create_vancouver_graph()
        
        # 4. æ·»åŠ è©•åˆ†æ•¸æ“š
        self.add_evaluations_to_graph()
        
        # 5. åŸ·è¡Œç®—æ³•
        final_grades = self.run_vancouver_algorithm()
        
        # 6. ç”Ÿæˆå ±å‘Š
        report_df = self.generate_detailed_report(final_grades)
        
        # 7. ä¿å­˜çµæœ
        json_path, excel_path = self.save_results(final_grades, report_df)
        
        # 8. é¡¯ç¤ºå‰10åçµæœ
        self.print_top_results(report_df)
        
        print(f"\nè™•ç†å®Œæˆ! è©³ç´°çµæœå·²ä¿å­˜åˆ°:")
        print(f"   ğŸ“„ {json_path}")
        print(f"   ğŸ“Š {excel_path}")
        
        return final_grades, report_df, json_path, excel_path

    def _generate_basic_summary_stats(self):
        """ç”ŸæˆåŸºæœ¬çµ±è¨ˆæ‘˜è¦ï¼Œä»¥é˜²åŸå§‹æ•¸æ“šä¸­ç¼ºå°‘æ‘˜è¦çµ±è¨ˆ"""
        if not self.raw_data or 'evaluation_data' not in self.raw_data:
            self.raw_data['summary_stats'] = {}
            return
            
        # æ”¶é›†æ‰€æœ‰è©•åˆ†è€…å’Œè¢«è©•åˆ†è€…
        evaluators = set()
        evaluatees = set()
        all_scores = []
        
        for evaluation in self.raw_data['evaluation_data']:
            evaluator_id = evaluation.get('evaluator_id')
            if evaluator_id:
                evaluators.add(evaluator_id)
                
            for evaluatee_id, eval_data in evaluation.get('evaluations', {}).items():
                evaluatees.add(evaluatee_id)
                if isinstance(eval_data, dict) and 'score' in eval_data:
                    all_scores.append(eval_data['score'])
        
        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_score = sum(all_scores) / len(all_scores) if all_scores else 0
        
        # å»ºç«‹åŸºæœ¬çµ±è¨ˆ
        self.raw_data['summary_stats'] = {
            'total_evaluators': len(evaluators),
            'total_evaluations': len(self.raw_data['evaluation_data']),
            'total_scores': len(all_scores),
            'average_score': avg_score,
            'overall_stats': {
                'mean': avg_score,
                'min': min(all_scores) if all_scores else 0,
                'max': max(all_scores) if all_scores else 0
            }
        }

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description='Vancouverç®—æ³•è™•ç†å™¨')
    parser.add_argument('--input-file', '-i', type=str,
                       help='è©•åˆ†æ•¸æ“šJSONæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output-dir', '-o', type=str,
                       help='è¼¸å‡ºç›®éŒ„è·¯å¾‘')
    
    args = parser.parse_args()
    
    # è¼‰å…¥çµ±ä¸€é…ç½®
    from stage0_config_unified import PeerEvaluationConfig
    config = PeerEvaluationConfig()
    
    # è¨­å®šè©•åˆ†æ•¸æ“šè·¯å¾‘
    if args.input_file:
        evaluation_data_path = args.input_file
    else:
        # ä½¿ç”¨ vancouver_input.jsonï¼ˆç”± stage4 ç”Ÿæˆçš„ Vancouver æ ¼å¼ï¼‰
        # å¾é…ç½®ç²å–è·¯å¾‘
        try:
            from stage0_config_unified import PeerEvaluationConfig
        except ImportError:
            from .stage0_config_unified import PeerEvaluationConfig
        
        config = PeerEvaluationConfig()
        # ä¿®æ”¹ï¼šä½¿ç”¨ vancouver_input è€Œä¸æ˜¯ evaluation_results_json
        evaluation_data_path = config.get_path('stage5_results', 'vancouver_input')
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(evaluation_data_path):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°è©•åˆ†æ•¸æ“šæ–‡ä»¶ {evaluation_data_path}")
        return
    
    # å‰µå»ºè™•ç†å™¨
    processor = VancouverProcessor(evaluation_data_path)
    
    # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºç›®éŒ„ï¼Œä¿®æ”¹è™•ç†å™¨çš„è¼¸å‡ºè·¯å¾‘
    if args.output_dir:
        processor.output_dir = args.output_dir
        if not os.path.exists(args.output_dir):
            os.makedirs(args.output_dir, exist_ok=True)
    
    # åŸ·è¡Œå®Œæ•´æµç¨‹
    final_grades, report_df, json_path, excel_path = processor.process_complete_workflow()
    
    return processor, final_grades, report_df

if __name__ == "__main__":
    main()
