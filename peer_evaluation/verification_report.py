#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vancouverç®—æ³•çµæœé©—è­‰å™¨
é©—è­‰Q1~Q5åˆ†æ•¸åŠ ç¸½è¨ˆç®—çš„æ­£ç¢ºæ€§
"""

import json
import pandas as pd
import numpy as np
import os

def verify_score_calculation():
    """é©—è­‰åˆ†æ•¸è¨ˆç®—çš„æ­£ç¢ºæ€§"""
    # è¼‰å…¥çµ±ä¸€é…ç½®
    from config_unified import PeerEvaluationConfig
    config = PeerEvaluationConfig()
    
    print("ğŸ” é©—è­‰Vancouverç®—æ³•åˆ†æ•¸è¨ˆç®—...")
    print("=" * 60)
    
    # è¼‰å…¥åŸå§‹è©•åˆ†æ•¸æ“š
    # å¯¦éš›æ–‡ä»¶ä½ç½®åœ¨ workflow_results/4_result_evaluation/ ç›®éŒ„ä¸‹
    evaluation_results_path = config.get_unified_output_path("result_evaluation", "evaluation_results.json")
    with open(evaluation_results_path, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    # è¼‰å…¥Vancouverç®—æ³•çµæœï¼ˆå‹•æ…‹å°‹æ‰¾æœ€æ–°çµæœæ–‡ä»¶ï¼‰
    # å¯¦éš›æ–‡ä»¶ä½ç½®åœ¨ workflow_results/5_vancouver_processing/ ç›®éŒ„ä¸‹
    vancouver_results_dir = config.get_unified_output_path("vancouver_processing")
    vancouver_files = [f for f in os.listdir(vancouver_results_dir) if f.startswith("vancouver_results_") and f.endswith(".json")]
    if not vancouver_files:
        print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°Vancouverçµæœæ–‡ä»¶")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„çµæœæ–‡ä»¶
    latest_file = sorted(vancouver_files)[-1]
    vancouver_results_path = os.path.join(vancouver_results_dir, latest_file)
    print(f"ğŸ“‚ ä½¿ç”¨Vancouverçµæœæ–‡ä»¶: {latest_file}")
    
    with open(vancouver_results_path, 'r', encoding='utf-8') as f:
        vancouver_results = json.load(f)
    
    print("ğŸ“Š åŸå§‹æ•¸æ“šçµ±è¨ˆ:")
    print(f"   - è©•åˆ†è€…æ•¸é‡: {original_data['summary_stats']['total_evaluators']}")
    print(f"   - è©•åˆ†ç¸½æ•¸: {original_data['summary_stats']['total_evaluations']}")
    print(f"   - åŸå§‹å¹³å‡åˆ†: {original_data['summary']['overall_stats']['mean']:.2f} (å–®é¡Œå¹³å‡)")
    
    # é‡æ–°è¨ˆç®—åŠ ç¸½åˆ†æ•¸ä¾†é©—è­‰
    print("\nğŸ§® é‡æ–°è¨ˆç®—Q1~Q5åŠ ç¸½åˆ†æ•¸...")
    
    total_scores_by_evaluatee = {}
    evaluation_counts = {}
    
    for evaluation in original_data['evaluation_data']:
        evaluator_id = evaluation['evaluator_id']
        
        for evaluatee_id, eval_data in evaluation['evaluations'].items():
            # è¨ˆç®—Q1~Q5çš„ç¸½åˆ†ï¼Œä½¿ç”¨å¯¦éš›çš„detailsçµæ§‹
            total_score = 0
            for q_num in range(1, 6):
                score_key = f"{evaluatee_id}_Q{q_num}_score"
                if score_key in eval_data['details']:
                    score = eval_data['details'][score_key]
                    total_score += score
            
            # è¨˜éŒ„æ¯å€‹è¢«è©•åˆ†è€…çš„æ‰€æœ‰ç¸½åˆ†
            if evaluatee_id not in total_scores_by_evaluatee:
                total_scores_by_evaluatee[evaluatee_id] = []
                evaluation_counts[evaluatee_id] = 0
            
            total_scores_by_evaluatee[evaluatee_id].append(total_score)
            evaluation_counts[evaluatee_id] += 1
    
    # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
    print("âœ… åŠ ç¸½åˆ†æ•¸çµ±è¨ˆ:")
    all_total_scores = []
    for scores in total_scores_by_evaluatee.values():
        all_total_scores.extend(scores)
    
    print(f"   - åˆ†æ•¸ç¯„åœ: {min(all_total_scores)} ~ {max(all_total_scores)}")
    print(f"   - å¹³å‡ç¸½åˆ†: {np.mean(all_total_scores):.2f}")
    print(f"   - æ¨™æº–å·®: {np.std(all_total_scores):.2f}")
    
    # æ¯”è¼ƒVancouverç®—æ³•çµæœ
    print("\nğŸ¯ Vancouverç®—æ³•æœ€çµ‚çµæœ:")
    print(f"   - å¹³å‡æœ€çµ‚æˆç¸¾: {vancouver_results['summary_statistics']['avg_final_grade']:.2f}")
    print(f"   - å¹³å‡å…±è­˜åˆ†æ•¸: {vancouver_results['summary_statistics']['avg_consensus_score']:.2f}")
    print(f"   - å¹³å‡è²è­½åˆ†æ•¸: {vancouver_results['summary_statistics']['avg_reputation']:.3f}")
    print(f"   - ä½¿ç”¨ä¿è­·æ©Ÿåˆ¶: {vancouver_results['summary_statistics']['protection_used_count']} äºº")
    
    # å‰µå»ºè©³ç´°æ¯”è¼ƒå ±å‘Š
    print("\nğŸ“‹ è©³ç´°å­¸ç”Ÿæˆç¸¾æ¯”è¼ƒ:")
    print("=" * 120)
    print(f"{'å­¸ç”ŸID':<6} {'åŸå§‹Q1~Q5å‡åˆ†':<15} {'åŠ ç¸½åˆ†æ•¸å‡å€¼':<15} {'Vancouverå…±è­˜':<15} {'æœ€çµ‚æˆç¸¾':<12} {'è²è­½åˆ†æ•¸':<10} {'ä¿è­·æ©Ÿåˆ¶':<8}")
    print("-" * 120)
    
    comparison_data = []
    
    # è¨ˆç®—æ¯å€‹å­¸ç”Ÿçš„åŸå§‹å–®é¡Œå¹³å‡åˆ†
    student_question_scores = {}  # {student_id: {Q1: [scores], Q2: [scores], ...}}
    
    # æ”¶é›†æ‰€æœ‰å­¸ç”Ÿçš„å–®é¡Œåˆ†æ•¸
    for evaluation in original_data['evaluation_data']:
        for evaluatee_id, eval_data in evaluation['evaluations'].items():
            if evaluatee_id not in student_question_scores:
                student_question_scores[evaluatee_id] = {f'Q{i}': [] for i in range(1, 6)}
            
            for q_num in range(1, 6):
                score_key = f"{evaluatee_id}_Q{q_num}_score"
                if score_key in eval_data['details']:
                    score = eval_data['details'][score_key]
                    student_question_scores[evaluatee_id][f'Q{q_num}'].append(score)
    
    for student_id in sorted(total_scores_by_evaluatee.keys()):
        # è¨ˆç®—åŸå§‹å–®é¡Œå¹³å‡åˆ†
        all_question_scores = []
        for q_num in range(1, 6):
            q_key = f'Q{q_num}'
            if q_key in student_question_scores[student_id]:
                all_question_scores.extend(student_question_scores[student_id][q_key])
        
        original_avg = np.mean(all_question_scores) if all_question_scores else 0
        
        # åŠ ç¸½åˆ†æ•¸çš„å¹³å‡
        total_scores_avg = np.mean(total_scores_by_evaluatee[student_id])
        
        # Vancouverçµæœ
        vancouver_grade = vancouver_results['final_grades'][student_id]
        consensus_score = vancouver_grade['consensus_score']
        final_grade = vancouver_grade['final_grade']
        reputation = vancouver_grade['reputation']
        protection_used = "æ˜¯" if vancouver_grade['protection_used'] else "å¦"
        
        print(f"{student_id:<6} {original_avg:<15.2f} {total_scores_avg:<15.2f} {consensus_score:<15.2f} "
              f"{final_grade:<12.2f} {reputation:<10.3f} {protection_used:<8}")
        
        comparison_data.append({
            'student_id': student_id,
            'original_avg_per_question': original_avg,
            'total_score_avg': total_scores_avg,
            'expected_total': original_avg * 5,  # é æœŸçš„ç¸½åˆ†ï¼ˆå–®é¡Œå‡åˆ† Ã— 5ï¼‰
            'vancouver_consensus': consensus_score,
            'vancouver_final': final_grade,
            'reputation': reputation,
            'protection_used': vancouver_grade['protection_used']
        })
    
    # é©—è­‰è¨ˆç®—æ­£ç¢ºæ€§
    print("\nğŸ”¬ è¨ˆç®—æ­£ç¢ºæ€§é©—è­‰:")
    all_correct = True
    
    for data in comparison_data:
        expected = data['expected_total']
        actual = data['total_score_avg']
        diff = abs(expected - actual)
        
        if diff > 0.01:  # å…è¨±å¾®å°çš„æµ®é»èª¤å·®
            print(f"âš ï¸  {data['student_id']}: é æœŸç¸½åˆ† {expected:.2f}, å¯¦éš›ç¸½åˆ† {actual:.2f}, å·®ç•° {diff:.2f}")
            all_correct = False
    
    if all_correct:
        print("âœ… æ‰€æœ‰å­¸ç”Ÿçš„åˆ†æ•¸è¨ˆç®—éƒ½æ­£ç¢ºï¼")
    
    # åˆ†æVancouverç®—æ³•æ•ˆæœ
    print("\nğŸ“ˆ Vancouverç®—æ³•æ•ˆæœåˆ†æ:")
    
    # è¨ˆç®—åŸå§‹åˆ†æ•¸çš„æ¨™æº–å·®vs Vancouverå…±è­˜åˆ†æ•¸çš„æ¨™æº–å·®
    original_total_avgs = [d['total_score_avg'] for d in comparison_data]
    vancouver_consensus = [d['vancouver_consensus'] for d in comparison_data]
    vancouver_final = [d['vancouver_final'] for d in comparison_data]
    
    print(f"   - åŸå§‹ç¸½åˆ†æ¨™æº–å·®: {np.std(original_total_avgs):.2f}")
    print(f"   - Vancouverå…±è­˜åˆ†æ•¸æ¨™æº–å·®: {np.std(vancouver_consensus):.2f}")
    print(f"   - Vancouveræœ€çµ‚æˆç¸¾æ¨™æº–å·®: {np.std(vancouver_final):.2f}")
    
    # æ’åè®ŠåŒ–åˆ†æ
    original_ranking = sorted(comparison_data, key=lambda x: x['total_score_avg'], reverse=True)
    vancouver_ranking = sorted(comparison_data, key=lambda x: x['vancouver_final'], reverse=True)
    
    print(f"\nğŸ† æ’åè®ŠåŒ–åˆ†æ:")
    print(f"   å‰5å (åŸå§‹ç¸½åˆ†): {', '.join([d['student_id'] for d in original_ranking[:5]])}")
    print(f"   å‰5å (Vancouver): {', '.join([d['student_id'] for d in vancouver_ranking[:5]])}")
    
    # ä¿å­˜æ¯”è¼ƒå ±å‘Š
    df = pd.DataFrame(comparison_data)
    verification_report_path = config.get_unified_output_path("vancouver_processing", "vancouver_verification_report.xlsx")
    df.to_excel(verification_report_path, index=False)
    print(f"\nğŸ’¾ è©³ç´°æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜åˆ°: {verification_report_path}")

if __name__ == "__main__":
    verify_score_calculation()
